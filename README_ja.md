<div align="center">
    <img alt="OpenRLHF logo" src="./docs/logo.png" style="height: 140px;" />
</div>
<div align="center">
<p align="center">
      <a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/discussions">
        <img alt="Issues" src="https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff" />
      <a href="https://github.com/OpenRLHF/OpenRLHF/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf" />
      </a>
      <br>
      <em>ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ / åŒ…æ‹¬çš„ / è»½é‡ / ä½¿ã„ã‚„ã™ã„</em>
    </p>
</p>
</div>

<hr>

<span>[ <a href="README.md">English</a> | <a href="README_zh.md">ä¸­æ–‡</a> | æ—¥æœ¬èª ]</span>

OpenRLHFã¯ã€Rayã€DeepSpeedã€ãŠã‚ˆã³HF Transformersã‚’åŸºç›¤ã¨ã—ãŸé«˜æ€§èƒ½ãªRLHFãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ï¼š

- **ã‚·ãƒ³ãƒ—ãƒ«ã§ä½¿ã„ã‚„ã™ã„**: OpenRLHFã¯ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªé«˜æ€§èƒ½RLHFãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸€ã¤ã§ã‚ã‚Šã€Huggingfaceã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«äº’æ›æ€§ãŒã‚ã‚Šã¾ã™ã€‚
- **é«˜æ€§èƒ½**: RLHFãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®80ï¼…ã®æ™‚é–“ã¯ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆæ®µéšã«è²»ã‚„ã•ã‚Œã¾ã™ã€‚Rayã¨Packing SamplesãŠã‚ˆã³vLLMç”ŸæˆåŠ é€Ÿã®èƒ½åŠ›ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€OpenRLHFã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯Optimized DeepSpeedChat with Hybrid Engineã®3ã€œ4å€ä»¥ä¸Šã§ã™ã€‚
- **åˆ†æ•£RLHF**: OpenRLHFã¯ã€Actorã€Rewardã€Referenceã€ãŠã‚ˆã³Criticãƒ¢ãƒ‡ãƒ«ã‚’Rayã‚’ä½¿ç”¨ã—ã¦åˆ¥ã€…ã®GPUã«åˆ†æ•£ã—ã€Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’CPUã«é…ç½®ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¤‡æ•°ã®A100 80G GPUã¨vLLMã‚’ä½¿ç”¨ã—ã¦70B+ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã®å¾®èª¿æ•´ãŒå¯èƒ½ã«ãªã‚Šã€è¤‡æ•°ã®24GB RTX 4090 GPUã§7Bãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã§ãã¾ã™ã€‚
- **PPOå®Ÿè£…ã®æœ€é©åŒ–**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€PPOã®å®Ÿè£…ãƒˆãƒªãƒƒã‚¯ã‚’çµ±åˆã—ã¾ã—ãŸã€‚è©³ç´°ã¯[Zhihu](https://zhuanlan.zhihu.com/p/622134699)ãŠã‚ˆã³[Notionãƒ–ãƒ­ã‚°](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

è©³ç´°ã¯[ã‚¹ãƒ©ã‚¤ãƒ‰](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [æŠ€è¡“å ±å‘Š](https://arxiv.org/abs/2405.11143) | [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://openrlhf.readthedocs.io/)ã‚’ã”è¦§ãã ã•ã„ã€‚

## ãƒ‹ãƒ¥ãƒ¼ã‚¹
- [2024/12] ç§ãŸã¡ã¯ğŸ˜Š [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS)ã‚’ã€Œææ¡ˆã€ã—ã¾ã—ãŸã€‚
- [2024/12] [Notionãƒ–ãƒ­ã‚°](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05)ã§PPOã€REINFORCE++ã€GRPOã€ãŠã‚ˆã³RLOOã‚’åˆ†æã—ã¾ã—ãŸã€‚

## ç‰¹å¾´

- Rayã«åŸºã¥ãåˆ†æ•£[ PPO](./examples/scripts/train_ppo_llama_ray.sh)ãŠã‚ˆã³[REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray.sh)ã®å®Ÿè£…ã€‚
- [70å„„ä»¥ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«](./examples/scripts/train_ppo_llama_ray_70b.sh)ã®å®Œå…¨ãªRLHFå¾®èª¿æ•´ã®ã‚µãƒãƒ¼ãƒˆã€‚
- RLHFã‚¿ã‚¹ã‚¯ã§ã®ç”Ÿæˆã‚’åŠ é€Ÿã™ã‚‹ãŸã‚ã®vLLMã®çµ±åˆï¼ˆ`--vllm_num_engines`ï¼‰ã€‚
- è¤‡æ•°ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆ`--reward_pretrain model1,model2...`ï¼‰ãŠã‚ˆã³ãƒªãƒ¢ãƒ¼ãƒˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆ`--remote_rm_url`ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [DPOï¼ˆç›´æ¥é¸å¥½æœ€é©åŒ–ï¼‰/IPO/cDPO](./examples/scripts/train_dpo_llama.sh)ãŠã‚ˆã³[Kahneman-Tversky Optimizationï¼ˆKTOï¼‰](./examples/scripts/train_kto_llama.sh)ã®å®Ÿè£…ã€‚
- [åå¾©DPO](./examples/scripts/train_iterative_dpo_llama.sh)ï¼ˆ[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [æ‹’å¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](./examples/scripts/train_rejection_sampling_llama.sh)ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [æ¡ä»¶ä»˜ãSFT](./examples/scripts/train_conditional_llama.sh)ï¼ˆ[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)ï¼‰ã®å®Ÿè£…ã€‚
- [çŸ¥è­˜è’¸ç•™](./examples/scripts/train_knowledge_distillation.sh)ï¼ˆ[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [ãƒ—ãƒ­ã‚»ã‚¹å ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆPRMï¼‰](./examples/scripts/train_prm_mistral.sh)ã®çµ±åˆã€‚
- SFTã€DPOã€RMã€PRMã€ãŠã‚ˆã³PPOã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ‘ãƒƒã‚­ãƒ³ã‚°ï¼ˆ`--packing_samples`ï¼‰ã€‚
- [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)ã®å®Ÿè£…ï¼ˆ`--ring_attn_size`ã€`--ring_head_stride`ï¼‰ã€‚
- [å°‚é–€å®¶ã®æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆMoEï¼‰](./examples/test_scripts/train_sft_mixtral_lora.sh)ã®ã‚µãƒãƒ¼ãƒˆï¼ˆ`--aux_loss_coef`ï¼‰ã€‚
- FlashAttention2ã®çµ±åˆï¼ˆ`--flash_attn`ï¼‰ã€‚
- QLoRAï¼ˆ`--load_in_4bit`ï¼‰ãŠã‚ˆã³[LoRA](./examples/scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--lora_rank`ã€`--target_modules`ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- HuggingFaceã®`tokenizer.apply_chat_template`ã¨ã®äº’æ›æ€§ï¼ˆ`--apply_chat_template`ãŠã‚ˆã³`--input_key`ï¼‰ã€‚
- Wandbï¼ˆ`--use_wandb`ï¼‰ãŠã‚ˆã³TensorBoardï¼ˆ`--use_tensorboard`ï¼‰ã«ã‚ˆã‚‹ãƒ­ã‚°è¨˜éŒ²ã®ã‚µãƒãƒ¼ãƒˆã€‚
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å›å¾©æ©Ÿèƒ½ï¼ˆ`--load_checkpoint`ãŠã‚ˆã³`--save_steps`ï¼‰ã€‚
- [DPO](./examples/scripts/train_llama_slurm.sh)ãŠã‚ˆã³[Ray PPO](./examples/scripts/train_ppo_llama_ray_slurm.sh)ãªã©ã®ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã€‚

### PPOã‚µãƒãƒ¼ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ç‰¹å¾´ | OpenRLHF | DSChat | CAIChat | TRL |
| ------------- |:-------------:| :-------------:| :-------------:| :-------------:|
| 16 A100-80GBã§70B+ã®ãƒ•ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°      | âœ… | âŒ | âŒ | âŒ |
| 4 RTX4090ã§7Bã®ãƒ•ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | âœ…      |    âŒ | âŒ | âŒ |
| 8 A100-80GBã§34B DPOã®ãƒ•ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | âœ…      |    âŒ | âŒ | âŒ |  
| PPOã§ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âœ… | âŒ | âŒ |  
| PPOå®Ÿè£…ã®ãƒˆãƒªãƒƒã‚¯ | âœ…      |    âŒ | âŒ | âœ… |
| QLoRAã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âŒ | âŒ | âœ… | 
| Mixtral 8*7bã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âŒ | âŒ | âŒ |  
| æœªçµåˆã®Actor-Criticã®ã‚µãƒãƒ¼ãƒˆ | âœ…     |   âœ… | âœ… | âŒ | 
| è¤‡æ•°ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âŒ | âŒ | âŒ |   
| Huggingfaceãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âœ… | âœ… | âœ… | 
| ä½¿ã„ã‚„ã™ã• | âœ…      |   âŒ (HybridEngineã®ãƒã‚°) | âœ… | âœ… | 

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

OpenRLHFã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã¾ãšDockerã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•ã—ï¼ˆ**æ¨å¥¨**ï¼‰ã€Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§`pip install`ã‚’å®Ÿè¡Œã—ã¦openrlhfã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼š

```bash
# Dockerã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•
docker run --runtime=nvidia -it --rm --shm-size="10g" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash
sudo pip uninstall xgboost transformer_engine flash_attn -y

# pip install
pip install openrlhf

# vLLMåŠ é€Ÿã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆvLLM 0.7.2ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
pip install openrlhf[vllm]
# æœ€æ–°ã®vLLMã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™
pip install openrlhf[vllm_latest]

# æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’pip install
pip install git+https://github.com/OpenRLHF/OpenRLHF.git

# ã¾ãŸã¯git clone
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
```

> [!NOTE]
>vLLM 0.6.4ä»¥é™ã®ä½¿ç”¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ä»–ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆvLLM >= 0.4.2ï¼‰ã¯ã€Glooã‚’ä»‹ã—ã¦é‡ã¿ã®åŒæœŸãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™ï¼ˆ`--vllm_sync_backend gloo`ï¼‰ã€‚
>ã¾ãŸã€[vLLMç”¨ã®Dockerfile](./dockerfile/)ãŠã‚ˆã³[Nvidia-Dockerã®ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](./examples/scripts/nvidia_docker_install.sh)ã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
OpenRLHFã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹å†…ã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ–¹æ³•ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
ä¾‹ãˆã°ã€[Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6)ã§ã¯ï¼š

```python
def preprocess_data(data, input_template=None, input_key="input", apply_chat_template=None) -> str:
    if apply_chat_template:
        chat = data[input_key]
        if isinstance(chat, str):
            chat = [{"role": "user", "content": chat}]
        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    else:
        prompt = data[input_key]
        if input_template:
            prompt = input_template.format(prompt)
    return prompt
```

- `--input_key`ã‚’ä½¿ç”¨ã—ã¦ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®`JSON key name`ã‚’æŒ‡å®šã—ã€`--prompt_data {name or path}`ï¼ˆPPOï¼‰ã¾ãŸã¯`--dataset {name or path}`ã‚’ä½¿ç”¨ã—ã€`--apply_chat_template`ã‚’ä½¿ç”¨ã—ã¦[Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating)ã®`chat_template`ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚
- `--apply_chat_template`ã‚’ä½¿ç”¨ã—ãŸããªã„å ´åˆã¯ã€ä»£ã‚ã‚Šã«`--input_template`ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§å‰å‡¦ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- OpenRLHFã¯ã€`--prompt_data_probs 0.1,0.4,0.5`ï¼ˆPPOï¼‰ã¾ãŸã¯`--dataset_probs 0.1,0.4,0.5`ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ··åˆã™ã‚‹ã“ã¨ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

Chat Templatingã®å‹•ä½œæ–¹æ³•ï¼š

```python
dataset = [{"input_key": [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]}]

tokenizer.apply_chat_template(dataset[0]["input_key"], tokenize=False)

"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æŒ‡å®šæ–¹æ³•ï¼š

`data_type@data_dir`å½¢å¼ã‚’ä½¿ç”¨ã—ã¦æŒ‡å®šã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯`--dataset json@./data`ã¨ã—ã¦è¨­å®šã§ãã¾ã™ã€‚

```
data
â”œâ”€â”€ test.jsonl
â””â”€â”€ train.jsonl
```

> [!NOTE]
> ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€`train`ãŠã‚ˆã³`test`ã‚’ä½¿ç”¨ã—ã¦Huggingfaceã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åŒºåˆ¥ã—ã¾ã™ã€‚
> `JSON key`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ä¾å­˜ã—ã¾ã™ã€‚è©³ç´°ã¯[Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10)ãŠã‚ˆã³[SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### æ•™å¸«ã‚ã‚Šå¾®èª¿æ•´

OpenRLHFã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯HuggingFaceãƒ¢ãƒ‡ãƒ«ã¨å®Œå…¨ã«äº’æ›æ€§ãŒã‚ã‚Šã¾ã™ã€‚`--pretrain  {name or path}`ã€`--reward_pretrain  {name or path}`ã€ãŠã‚ˆã³`--critic_pretrain  {name or path}`ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã„ãã¤ã‹ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’[HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF)ã§æä¾›ã—ã¦ã„ã¾ã™ã€‚

æ¬¡ã«ã€[examples/scripts](./examples/scripts/)ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æä¾›ã•ã‚Œã¦ã„ã‚‹èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã§ãã¾ã™ã€‚

```bash 
deepspeed --module openrlhf.cli.train_sft \
   --max_len 4096 \
   --dataset Open-Orca/OpenOrca \
   --input_key question \
   --output_key response \
   --input_template $'User: {}\nAssistant: ' \
   --train_batch_size 256 \
   --micro_train_batch_size 2 \
   --max_samples 500000 \
   --pretrain meta-llama/Meta-Llama-3-8B \
   --save_path ./checkpoint/llama3-8b-sft \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --zero_stage 2 \
   --max_epochs 1 \
   --packing_samples \
   --bf16 \
   --flash_attn \
   --learning_rate 5e-6 \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

# HF tokenizer.apply_chat_templateã®ã‚µãƒãƒ¼ãƒˆ
# --apply_chat_template 
# --tokenizer_chat_template {HF Chat Template}

# RingAttentionã®ã‚µãƒãƒ¼ãƒˆ
# pip install ring_flash_attn
#   --ring_attn_size 2 \
#   --ring_head_stride 2 \

# ç¶™ç¶šçš„ãªäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚‚ä½¿ç”¨ã§ãã¾ã™
# --pretrain_mode
```

> [!NOTE]
> OpenRLHF SFT/DPO/RewardModel/PPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã¯`--packing_samples`ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ [`--flash_attn`ã«åŸºã¥ã](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)

### å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
deepspeed --module openrlhf.cli.train_rm \
   --save_path ./checkpoint/llama3-8b-rm \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --train_batch_size 256 \
   --micro_train_batch_size 1 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --bf16 \
   --max_epochs 1 \
   --max_len 8192 \
   --zero_stage 3 \
   --learning_rate 9e-6 \
   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
   --apply_chat_template \
   --chosen_key chosen \
   --rejected_key rejected \
   --flash_attn \
   --packing_samples \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

```

å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®`--value_prefix_head`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’`score`ã«è¨­å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€`AutoModelForSequenceClassification`ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š

```python
reward_model = AutoModelForSequenceClassification.from_pretrained(
              reward_model_path,
              num_labels=1,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              use_cache=False,
          )
inputs = xxxx (Left Padding Input Tokens)
reward = reward_model.model(*inputs).last_hidden_state
reward = reward_model.score(reward)[:, -1]
```

### Rayã‚’ä½¿ç”¨ã—ãªã„PPO

```bash
deepspeed --module openrlhf.cli.train_ppo \
  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \
  --save_path ./checkpoint/llama-3-8b-rlhf \
  --save_steps -1 \
  --logging_steps 1 \
  --eval_steps -1 \
  --micro_train_batch_size 2 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 4 \
  --rollout_batch_size 1024 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --generate_max_len 1024 \
  --zero_stage 2 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data OpenRLHF/prompt-collection-v0.1 \
  --input_key context_messages \
  --apply_chat_template \
  --max_samples 100000 \
  --normalize_reward \
  --adam_offload \
  --flash_attn \
  --gradient_checkpointing \
  --use_wandb {wandb_token}

# ãƒªãƒ¢ãƒ¼ãƒˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆï¼ˆHTTPï¼‰
# --remote_rm_url http://localhost:5000/get_reward
```

### Rayã¨vLLMã‚’ä½¿ç”¨ã—ãŸPPO/REINFORCE++

RLHFãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã‹ã€70Bãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«ã€Rayã¨vLLMåŠ é€Ÿã‚’ä½¿ç”¨ã—ãŸPPOã‚’ä½¿ç”¨ã§ãã¾ã™

```bash
# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§Rayã®ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‚’èµ·å‹•
ray start --head --node-ip-address 0.0.0.0 --num-gpus 8

# ã•ã‚‰ã«å¤šãã®ãƒãƒ¼ãƒ‰ã§Rayã‚’èµ·å‹•ã™ã‚‹å ´åˆã¯
ray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8

ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  --ref_num_nodes 1 \
  --ref_num_gpus_per_node 2 \
  --reward_num_nodes 1 \
  --reward_num_gpus_per_node 2 \
  --critic_num_nodes 1 \
  --critic_num_gpus_per_node 2 \
  --actor_num_nodes 1 \
  --actor_num_gpus_per_node 2 \
  --vllm_num_engines 2 \
  --vllm_tensor_parallel_size 2 \
  --colocate_critic_reward \
  --colocate_actor_ref \
  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \
  --save_path /openrlhf/examples/checkpoint/llama3-8b-rlhf \
  --micro_train_batch_size 8 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 16 \
  --rollout_batch_size 1024 \
  --max_samples 100000 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --generate_max_len 1024 \
  --zero_stage 3 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data OpenRLHF/prompt-collection-v0.1 \
  --input_key context_messages \
  --apply_chat_template \
  --normalize_reward \
  --packing_samples \
  --adam_offload \
  --flash_attn \
  --gradient_checkpointing \
  --use_wandb {wandb_token}

# REINFORCE++ | RLOOã®ã‚µãƒãƒ¼ãƒˆ
# --advantage_estimator reinforce | rloo

# ãƒªãƒ¢ãƒ¼ãƒˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆï¼ˆHTTPï¼‰
# --remote_rm_url http://localhost:5000/get_reward


# Nã‚µãƒ³ãƒ—ãƒ«ã®ã‚µãƒãƒ¼ãƒˆ
# --n_samples_per_prompt 4
```
> [!NOTE]
> `--vllm_num_engines`ã‚’è¨­å®šã—ãªã„å ´åˆã¯ã€vLLMã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
> `setup_commands`ã‚’ä½¿ç”¨ã—ã¦RayãŒè‡ªå‹•çš„ã«ç’°å¢ƒã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€`--runtime-env-json='{"setup_commands": ["pip install openrlhf[vllm]"]}'`ã€‚

[!NOTE]
OPENRLHFã®RLOOã¯ã€REINFORCE++ã‚’åŸºã«æ”¹è‰¯ã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Šã€ã‚ªãƒªã‚¸ãƒŠãƒ«ç‰ˆã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚

> [!NOTE]
> deepspeedãŒGPUãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®šã™ã‚‹éš›ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒç¯„å›²å¤–ã«é–¢é€£ã™ã‚‹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€ç’°å¢ƒå¤‰æ•°[`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py)ã‚’è¨­å®šã—ã¦å›é¿ç­–ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚
>   ```bash
>   # NVIDIA GPUã®å ´åˆ:
>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1
>   ```

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯[example/scripts](./examples/scripts/)ãŠã‚ˆã³[Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)ã«ã‚ã‚Šã¾ã™ã€‚

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

Adamã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã®æœ‰åŠ¹åŒ–ã€å ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆRMï¼‰ãŠã‚ˆã³å‚ç…§ãƒ¢ãƒ‡ãƒ«ï¼ˆRefï¼‰ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ãªã©ã®æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€DSChatã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§é™ã«æœ€é©åŒ–ã—ã€æ¨è«–æ®µéšã§ã®ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã—ã€ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å•é¡Œã‚’å›é¿ã—ã¾ã—ãŸã€‚LLaMA2ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆHEï¼‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚ã«ã€DSChatã®ã„ãã¤ã‹ã®ãƒã‚°ã‚‚ä¿®æ­£ã—ã¾ã—ãŸã€‚Optimized DSChatã¨OpenRLHFã‚’ä½¿ç”¨ã—ã¦1024ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’1ã¤ã®PPOã‚¨ãƒãƒƒã‚¯ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã®ã«ã‹ã‹ã‚‹å¹³å‡æ™‚é–“ï¼ˆç§’ï¼‰ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ï¼š

| **ã‚µã‚¤ã‚º** | **NVIDIA A800-80GB GPU** | **Optimized DSChatï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³ä»˜ãï¼‰** | **OpenRLHF** | **ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—** |
| :---: | :---: | :---: | :---: | :---: |
| 7B | 16 | 855.09 | 471.11 | 1.82x |
| 13B | 32 | 1528.93 | 608.93 | 2.5x |
| 34B | 32 | 3634.98 | 1526.4 | 2.4x |
| 70B | 32 | 10407.0 | 4488.53 | 2.3x |

> [!NOTE]
> ãƒ‡ãƒ¼ã‚¿ã¯å¤ããªã£ã¦ã„ã¾ã™ã€‚å†ãƒ†ã‚¹ãƒˆã®ãŸã‚ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€vLLMã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚Šå¤šãã®ãƒãƒ¼ãƒ‰ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€32å€‹ã®A100 GPUã‚’æŒã¤70Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€16å€‹ã®A100 GPUã‚’vLLMã‚¨ãƒ³ã‚¸ãƒ³ã«å‰²ã‚Šå½“ã¦ã€8å€‹ã®GPUã‚’Actorãƒ¢ãƒ‡ãƒ«ã«ã€æ®‹ã‚Šã®8å€‹ã®GPUã‚’Criticãƒ¢ãƒ‡ãƒ«ã«å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€`--colocate_critic_reward`ã€`--colocate_actor_ref`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹ã«ã—ã¦ãƒãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸ã—ã¾ã™ã€‚æœ€å¾Œã«ã€`rollout_micro_batch_size`ï¼ˆãŠã‚ˆã³vLLMã‚¨ãƒ³ã‚¸ãƒ³ã®TPã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–ï¼‰ã‚’å¯èƒ½ãªé™ã‚Šå¢—ã‚„ã™ã¹ãã§ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯ã€ã‚ˆã‚Šå¤§ããª`--micro_train_batch_size`ãŒæœ›ã¾ã—ãã€`--packing_samples`ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚ååˆ†ãªGPUãŒã‚ã‚‹å ´åˆã€`--adam_offload`ã‚’ç„¡åŠ¹ã«ã—ã€`--overlap_comm`ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚ãƒãƒ«ãƒãƒãƒ¼ãƒ‰RLHFã®å ´åˆã€vLLM 0.6.4+ã§`--vllm_sync_backend nccl`ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

## OpenRLHFã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ä¼æ¥­ã¨çµ„ç¹”

- Google
- ByteDance
- Tencent
- Alibaba
- Baidu
- China Telecom
- Vivo
- Allen AI
- NexusFlow
- JÃ¼lich Supercomputing Centre (JSC)
- Berkeley Starling Team
- M-A-P
- ...

## å‚åŠ æ–¹æ³•

**å‚åŠ æ–¹æ³•**

1. janhu9527@gmail.comã«ãƒ¡ãƒ¼ãƒ«ã‚’é€ã‚‹ã‹ã€[GitHub Organization](https://github.com/OpenRLHF)ã«å‚åŠ ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®è©³ç´°ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
   - ã‚ãªãŸã®åå‰
   - ã‚ãªãŸã®GitHubãƒ¦ãƒ¼ã‚¶ãƒ¼å
   - ã‚ãªãŸã®èˆˆå‘³ã®ã‚ã‚‹åˆ†é‡
   - NLPãŠã‚ˆã³/ã¾ãŸã¯AIã«é–¢é€£ã™ã‚‹ã‚¹ã‚­ãƒ«ã¨çµŒé¨“
1. å…¬å¼GitHub[OpenRLHF â†—](https://github.com/OpenRLHF/OpenRLHF)ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒšãƒ¼ã‚¸ã‚’é€šã˜ã¦å‚åŠ ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚è²¢çŒ®ã—ãŸã„èˆˆå‘³ã«ã¤ã„ã¦ã®issueã‚’ä½œæˆã™ã‚‹ã ã‘ã§ã€ç§ãŸã¡ãŒé€£çµ¡ã—ã¾ã™ã€‚

**ä½•ãŒã§ãã‚‹ã‹**

1. ãƒãƒ¼ãƒ ã«å‚åŠ ã—ã€OpenRLHFãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é–‹ç™ºã«å‚åŠ ã—ã¾ã™ã€‚
1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«è²¢çŒ®ã™ã‚‹ãŸã‚ã«ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æå‡ºã—ã¾ã™ã€‚
1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ”¹å–„ã€ãƒã‚°ã®ä¿®æ­£ã€æ–°æ©Ÿèƒ½ã®ä½œæˆã‚’æ‰‹ä¼ã„ã¾ã™ã€‚
1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å…±æœ‰ã—ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æˆé•·ã‚’æ”¯æ´ã—ã¾ã™ã€‚

## ã‚¹ãƒãƒ³ã‚µãƒ¼

ã‚¹ãƒãƒ³ã‚µãƒ¼ã‚·ãƒƒãƒ—ã¯ã€OpenRLHFã®ç¶­æŒã¨æ”¹å–„ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå½¹ç«‹ã¤ã¨æ„Ÿã˜ãŸå ´åˆã¯ã€ã‚¹ãƒãƒ³ã‚µãƒ¼ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚[Open Collective â†—](https://opencollective.com/OpenRLHF)ã§ã‚¹ãƒãƒ³ã‚µãƒ¼ã«ãªã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## ã‚¹ã‚¿ãƒ¼å±¥æ­´

[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)

## è²¢çŒ®è€…

ã™ã¹ã¦ã®è²¢çŒ®è€…ã«æ„Ÿè¬ã—ã¾ã™ï¼è²¢çŒ®ã—ãŸã„å ´åˆã¯ã€ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹ã‹ã€issueã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

<a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF" />
</a>

## å‚è€ƒæ–‡çŒ®ã¨è¬è¾

AIãŠã‚ˆã³NLPåˆ†é‡ã¸ã®è²¢çŒ®ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŠã‚ˆã³çµ„ç¹”ã«æ„Ÿè¬ã—ã¾ã™ï¼š

- [Hugging Face Transformers â†—](https://github.com/huggingface/transformers)
- [OpenAI GPT â†—](https://github.com/openai/gpt-3)
- [LLaMA â†—](https://llama.meta.com/)
- [DeepSpeed â†—](https://github.com/microsoft/DeepSpeed)
- [Ray â†—](https://github.com/ray-project/ray)

ç§ãŸã¡ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€[ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)ãŠã‚ˆã³[DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)ã«ã‚‚æ„Ÿè¬ã—ã¾ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸæ®µéšã§ã€å½¼ã‚‰ã®ã‚³ãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚

(2024/7) ç§ãŸã¡ã®GitHubçµ„ç¹”ã¯OpenLLMAIã‹ã‚‰OpenRLHFã«å¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚

## å¼•ç”¨
```
@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}
```

______________________________________________________________________

*OpenRLHF Â© 2025 OpenRLHF. All Rights Reserved.*
