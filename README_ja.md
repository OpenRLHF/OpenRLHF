<div align="center">
    <img alt="OpenRLHF logo" src="./docs/logo.png" style="height: 140px;" />
</div>
<div align="center">
<p align="center">
      <a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/discussions">
        <img alt="Issues" src="https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff" />
      <a href="https://github.com/OpenRLHF/OpenRLHF/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf" />
      </a>
      <br>
      <em>ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ / åŒ…æ‹¬çš„ / è»½é‡ / ä½¿ã„ã‚„ã™ã„</em>
    </p>
</p>
</div>

<hr>

<span>[ <a href="README.md">English</a> | <a href="README_zh.md">ä¸­æ–‡</a> | æ—¥æœ¬èª ]</span>

OpenRLHFã¯ã€Rayã€DeepSpeedã€ãŠã‚ˆã³HF Transformersã‚’åŸºç›¤ã¨ã—ãŸé«˜æ€§èƒ½ãªRLHFãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ï¼š

- **ã‚·ãƒ³ãƒ—ãƒ«ã§ä½¿ã„ã‚„ã™ã„**: OpenRLHFã¯ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªé«˜æ€§èƒ½RLHFãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸€ã¤ã§ã‚ã‚Šã€Huggingfaceã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«äº’æ›æ€§ãŒã‚ã‚Šã¾ã™ã€‚
- **é«˜æ€§èƒ½**: RLHFãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®80ï¼…ã®æ™‚é–“ã¯ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆæ®µéšã«è²»ã‚„ã•ã‚Œã¾ã™ã€‚Rayã¨Packing SamplesãŠã‚ˆã³vLLMç”ŸæˆåŠ é€Ÿã®èƒ½åŠ›ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€OpenRLHFã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯Optimized DeepSpeedChat with Hybrid Engineã®3ã€œ4å€ä»¥ä¸Šã§ã™ã€‚
- **åˆ†æ•£RLHF**: OpenRLHFã¯ã€Actorã€Rewardã€Referenceã€ãŠã‚ˆã³Criticãƒ¢ãƒ‡ãƒ«ã‚’Rayã‚’ä½¿ç”¨ã—ã¦åˆ¥ã€…ã®GPUã«åˆ†æ•£ã—ã€Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’CPUã«é…ç½®ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¤‡æ•°ã®A100 80G GPUã¨vLLMã‚’ä½¿ç”¨ã—ã¦70B+ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã®å¾®èª¿æ•´ãŒå¯èƒ½ã«ãªã‚Šã€è¤‡æ•°ã®24GB RTX 4090 GPUã§7Bãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã§ãã¾ã™ã€‚
- **PPOå®Ÿè£…ã®æœ€é©åŒ–**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€PPOã®å®Ÿè£…ãƒˆãƒªãƒƒã‚¯ã‚’çµ±åˆã—ã¾ã—ãŸã€‚è©³ç´°ã¯[Zhihu](https://zhuanlan.zhihu.com/p/622134699)ãŠã‚ˆã³[Notionãƒ–ãƒ­ã‚°](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
- **Hybrid Engine**: OpenRLHFã¯Hybrid Engineã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€ã™ã¹ã¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ã¨æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãŒGPUã‚’å…±æœ‰ã—ã¦ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã‚’é˜²ãã¾ã™ã€‚

è©³ç´°ã¯[ã‚¹ãƒ©ã‚¤ãƒ‰](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [æŠ€è¡“å ±å‘Š](https://arxiv.org/abs/2405.11143) | [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://openrlhf.readthedocs.io/)ã‚’ã”è¦§ãã ã•ã„ã€‚

## ãƒ‹ãƒ¥ãƒ¼ã‚¹
- [2025/3] CMUã®[2025å¹´æ˜¥ã®é«˜åº¦è‡ªç„¶è¨€èªå‡¦ç†ã‚³ãƒ¼ã‚¹](https://cmu-l3.github.io/anlp-spring2025/)ãŒOpenRLHFã‚’RLHFãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ•™è‚²äº‹ä¾‹ã¨ã—ã¦æ¡ç”¨ã€‚
- [2025/2] [Logic-RL](https://arxiv.org/abs/2502.14768) ã¨ [PRIME](https://arxiv.org/abs/2502.01456) ã¯ã€REINFORCE++ ãŒè¨“ç·´ã®å®‰å®šæ€§ã«ãŠã„ã¦ GRPO ã‚ˆã‚Šå„ªã‚Œã€PPO ã‚ˆã‚Šé«˜é€Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚
- [2025/2] [LMM-R1](https://github.com/TideDra/lmm-r1) ã¯ OpenRLHF ã®ãƒ•ã‚©ãƒ¼ã‚¯ã§ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã§ã® DeepSeek-R1 ã®å†ç¾ã®ãŸã‚ã®é«˜æ€§èƒ½ RL ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚
- [2025/2] MIT & Microsoft ã¯ OpenRLHF ã‚’ä½¿ç”¨ã—ã¦ [On the Emergence of Thinking in LLMs I: Searching for the Right Intuition](https://arxiv.org/pdf/2502.06773) ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚
- [2025/1] HKUSTã¯ [OpenRLHF ã‚’ä½¿ç”¨ã—ã¦å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã® DeepSeek-R1-Zero ã¨ DeepSeek-R1 ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°](https://github.com/hkust-nlp/simpleRL-reason)ã‚’å†ç¾ã—ã¾ã—ãŸã€‚
- [2024/12] ç§ãŸã¡ã¯ğŸ˜Š [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS)ã‚’ã€Œææ¡ˆã€ã—ã¾ã—ãŸã€‚
- [2024/12] [Notionãƒ–ãƒ­ã‚°](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05)ã§PPOã€REINFORCE++ã€GRPOã€ãŠã‚ˆã³RLOOã‚’åˆ†æã—ã¾ã—ãŸã€‚

## ç‰¹å¾´

- Rayã«åŸºã¥ãåˆ†æ•£[ PPO](./examples/scripts/train_ppo_llama_ray.sh)ãŠã‚ˆã³[EINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray.sh)ã®å®Ÿè£…ã€‚
- [Ray-based Reinforced Finetuning](./examples/scripts/train_ppo_llama_with_reward_fn.sh)
- Rayã¨Hybrid Engineã«åŸºã¥ã[PPO](./examples/scripts/train_ppo_llama_ray_hybrid_engine.sh)ãŠã‚ˆã³[REINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh)ã®ã‚µãƒãƒ¼ãƒˆ (`--colocate_all_models`, `--vllm_enable_sleep` and `--vllm_gpu_memory_utilization 0.5`)
- [70å„„ä»¥ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«](./examples/scripts/train_ppo_llama_ray_70b.sh)ã®å®Œå…¨ãªRLHFå¾®èª¿æ•´ã®ã‚µãƒãƒ¼ãƒˆã€‚
- RLHFã‚¿ã‚¹ã‚¯ã§ã®ç”Ÿæˆã‚’åŠ é€Ÿã™ã‚‹ãŸã‚ã®vLLMã®çµ±åˆï¼ˆ`--vllm_num_engines`ï¼‰ã€‚
- è¤‡æ•°ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆ`--reward_pretrain model1,model2...`ï¼‰ãŠã‚ˆã³ãƒªãƒ¢ãƒ¼ãƒˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆ`--remote_rm_url`ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [DPOï¼ˆç›´æ¥é¸å¥½æœ€é©åŒ–ï¼‰/IPO/cDPO](./examples/scripts/train_dpo_llama.sh)ãŠã‚ˆã³[Kahneman-Tversky Optimizationï¼ˆKTOï¼‰](./examples/scripts/train_kto_llama.sh)ã®å®Ÿè£…ã€‚
- [åå¾©DPO](./examples/scripts/train_iterative_dpo_llama.sh)ï¼ˆ[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [æ‹’å¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](./examples/scripts/train_rejection_sampling_llama.sh)ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [æ¡ä»¶ä»˜ãSFT](./examples/scripts/train_conditional_llama.sh)ï¼ˆ[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)ï¼‰ã®å®Ÿè£…ã€‚
- [çŸ¥è­˜è’¸ç•™](./examples/scripts/train_knowledge_distillation.sh)ï¼ˆ[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [ãƒ—ãƒ­ã‚»ã‚¹å ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆPRMï¼‰](./examples/scripts/train_prm_mistral.sh)ã®çµ±åˆã€‚
- SFTã€DPOã€RMã€PRMã€ãŠã‚ˆã³PPOã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ‘ãƒƒã‚­ãƒ³ã‚°ï¼ˆ`--packing_samples`ï¼‰ã€‚
- [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)ã®å®Ÿè£…ï¼ˆ`--ring_attn_size`ã€`--ring_head_stride`ï¼‰ã€‚
- [å°‚é–€å®¶ã®æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆMoEï¼‰](./examples/test_scripts/train_sft_mixtral_lora.sh)ã®ã‚µãƒãƒ¼ãƒˆï¼ˆ`--aux_loss_coef`ï¼‰ã€‚
- FlashAttention2ã®çµ±åˆï¼ˆ`--flash_attn`ï¼‰ã€‚
- QLoRAï¼ˆ`--load_in_4bit`ï¼‰ãŠã‚ˆã³[LoRA](./examples/scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--lora_rank`ã€`--target_modules`ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- HuggingFaceã®`tokenizer.apply_chat_template`ã¨ã®äº’æ›æ€§ï¼ˆ`--apply_chat_template`ãŠã‚ˆã³`--input_key`ï¼‰ã€‚
- Wandbï¼ˆ`--use_wandb`ï¼‰ãŠã‚ˆã³TensorBoardï¼ˆ`--use_tensorboard`ï¼‰ã«ã‚ˆã‚‹ãƒ­ã‚°è¨˜éŒ²ã®ã‚µãƒãƒ¼ãƒˆã€‚
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å›å¾©æ©Ÿèƒ½ï¼ˆ`--load_checkpoint`ãŠã‚ˆã³`--save_steps`ï¼‰ã€‚
- [DPO](./examples/scripts/train_llama_slurm.sh)ãŠã‚ˆã³[Ray PPO](./examples/scripts/train_ppo_llama_ray_slurm.sh)ãªã©ã®ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã€‚

### PPOã‚µãƒãƒ¼ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ç‰¹å¾´ | OpenRLHF | DSChat | CAIChat | TRL |
| ------------- |:-------------:| :-------------:| :-------------:| :-------------:|
| 16 A100-80GBã§70B+ã®ãƒ•ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°      | âœ… | âŒ | âŒ | âŒ |
| 4 RTX4090ã§7Bã®ãƒ•ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | âœ…      |    âŒ | âŒ | âŒ |
| 8 A100-80GBã§34B DPOã®ãƒ•ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | âœ…      |    âŒ | âŒ | âŒ |  
| PPOã§ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âœ… | âŒ | âŒ |  
| PPOå®Ÿè£…ã®ãƒˆãƒªãƒƒã‚¯ | âœ…      |    âŒ | âŒ | âœ… |
| QLoRAã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âŒ | âŒ | âœ… | 
| Mixtral 8*7bã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âŒ | âŒ | âŒ |  
| æœªçµåˆã®Actor-Criticã®ã‚µãƒãƒ¼ãƒˆ | âœ…     |   âœ… | âœ… | âŒ | 
| è¤‡æ•°ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âŒ | âŒ | âŒ |   
| Huggingfaceãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆ | âœ…      |    âœ… | âœ… | âœ… | 
| ä½¿ã„ã‚„ã™ã• | âœ…      |   âŒ (HybridEngineã®ãƒã‚°) | âœ… | âœ… | 

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

OpenRLHFã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã¾ãšDockerã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•ã—ï¼ˆ**æ¨å¥¨**ï¼‰ã€Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§`pip install`ã‚’å®Ÿè¡Œã—ã¦openrlhfã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼š

```bash
# Dockerã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•
docker run --runtime=nvidia -it --rm --shm-size="10g" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash
sudo pip uninstall xgboost transformer_engine flash_attn -y

# pip install
pip install openrlhf

# vLLMåŠ é€Ÿã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆvLLM 0.8.3ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
pip install openrlhf[vllm]
# æœ€æ–°ã®vLLMã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™
pip install openrlhf[vllm_latest]

# æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’pip install
pip install git+https://github.com/OpenRLHF/OpenRLHF.git

# ã¾ãŸã¯git clone
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
```

> [!NOTE]
>vLLM 0.8.3ä»¥é™ã®ä½¿ç”¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚
>ã¾ãŸã€[vLLMç”¨ã®Dockerfile](./dockerfile/)ãŠã‚ˆã³[Nvidia-Dockerã®ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](./examples/scripts/nvidia_docker_install.sh)ã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
OpenRLHFã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹å†…ã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ–¹æ³•ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
ä¾‹ãˆã°ã€[Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6)ã§ã¯ï¼š

```python
def preprocess_data(data, input_template=None, input_key="input", apply_chat_template=None) -> str:
    if apply_chat_template:
        chat = data[input_key]
        if isinstance(chat, str):
            chat = [{"role": "user", "content": chat}]
        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    else:
        prompt = data[input_key]
        if input_template:
            prompt = input_template.format(prompt)
    return prompt
```

- `--input_key`ã‚’ä½¿ç”¨ã—ã¦ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®`JSON key name`ã‚’æŒ‡å®šã—ã€`--prompt_data {name or path}`ï¼ˆPPOï¼‰ã¾ãŸã¯`--dataset {name or path}`ã‚’ä½¿ç”¨ã—ã€`--apply_chat_template`ã‚’ä½¿ç”¨ã—ã¦[Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating)ã®`chat_template`ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚
- `--apply_chat_template`ã‚’ä½¿ç”¨ã—ãŸããªã„å ´åˆã¯ã€ä»£ã‚ã‚Šã«`--input_template`ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§å‰å‡¦ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- OpenRLHFã¯ã€`--prompt_data_probs 0.1,0.4,0.5`ï¼ˆPPOï¼‰ã¾ãŸã¯`--dataset_probs 0.1,0.4,0.5`ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ··åˆã™ã‚‹ã“ã¨ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

Chat Templatingã®å‹•ä½œæ–¹æ³•ï¼š

```python
dataset = [{"input_key": [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]}]

tokenizer.apply_chat_template(dataset[0]["input_key"], tokenize=False)

"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æŒ‡å®šæ–¹æ³•

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ã¯ ``--eval_dataset {name or path}`` ã‚’ä½¿ç”¨ã—ã¦è¨­å®šã—ã¦ãã ã•ã„ã€‚

> [!NOTE]
> ``JSON key`` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ä¾å­˜ã—ã¾ã™ã€‚è©³ç´°ã¯ [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) ãŠã‚ˆã³ [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### æ•™å¸«ã‚ã‚Šå¾®èª¿æ•´

OpenRLHFã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯HuggingFaceãƒ¢ãƒ‡ãƒ«ã¨å®Œå…¨ã«äº’æ›æ€§ãŒã‚ã‚Šã¾ã™ã€‚`--pretrain  {name or path}`ã€`--reward_pretrain  {name or path}`ã€ãŠã‚ˆã³`--critic_pretrain  {name or path}`ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã„ãã¤ã‹ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’[HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF)ã§æä¾›ã—ã¦ã„ã¾ã™ã€‚

æ¬¡ã«ã€[examples/scripts](./examples/scripts/)ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æä¾›ã•ã‚Œã¦ã„ã‚‹èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã§ãã¾ã™ã€‚

```bash 
deepspeed --module openrlhf.cli.train_sft \
   --max_len 4096 \
   --dataset Open-Orca/OpenOrca \
   --input_key question \
   --output_key response \
   --input_template $'User: {}\nAssistant: ' \
   --train_batch_size 256 \
   --micro_train_batch_size 2 \
   --max_samples 500000 \
   --pretrain meta-llama/Meta-Llama-3-8B \
   --save_path ./checkpoint/llama3-8b-sft \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --zero_stage 2 \
   --max_epochs 1 \
   --packing_samples \
   --bf16 \
   --flash_attn \
   --learning_rate 5e-6 \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

# HF tokenizer.apply_chat_templateã®ã‚µãƒãƒ¼ãƒˆ
# --apply_chat_template 
# --tokenizer_chat_template {HF Chat Template}

# RingAttentionã®ã‚µãƒãƒ¼ãƒˆ
# pip install ring_flash_attn
#   --ring_attn_size 2 \
#   --ring_head_stride 2 \

# ç¶™ç¶šçš„ãªäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚‚ä½¿ç”¨ã§ãã¾ã™
# --pretrain_mode
```

> [!NOTE]
> OpenRLHF SFT/DPO/RewardModel/PPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã¯`--packing_samples`ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ [`--flash_attn`ã«åŸºã¥ã](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)

### å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
deepspeed --module openrlhf.cli.train_rm \
   --save_path ./checkpoint/llama3-8b-rm \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --train_batch_size 256 \
   --micro_train_batch_size 1 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --bf16 \
   --max_epochs 1 \
   --max_len 8192 \
   --zero_stage 3 \
   --learning_rate 9e-6 \
   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
   --apply_chat_template \
   --chosen_key chosen \
   --rejected_key rejected \
   --flash_attn \
   --packing_samples \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

```

å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®`--value_prefix_head`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’`score`ã«è¨­å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€`AutoModelForSequenceClassification`ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š

```python
reward_model = AutoModelForSequenceClassification.from_pretrained(
              reward_model_path,
              num_labels=1,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              use_cache=False,
          )
inputs = xxxx (Left Padding Input Tokens)
reward = reward_model.model(*inputs).last_hidden_state
reward = reward_model.score(reward)[:, -1]
```

### Rayã¨vLLMã‚’ä½¿ç”¨ã—ãŸPPO/REINFORCE++

RLHFãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã‹ã€70Bãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«ã€Rayã¨vLLMåŠ é€Ÿã‚’ä½¿ç”¨ã—ãŸPPOã‚’ä½¿ç”¨ã§ãã¾ã™ (Hybrid Engine)

```bash
# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§Rayã®ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‚’èµ·å‹•
ray start --head --node-ip-address 0.0.0.0 --num-gpus 8

# ã•ã‚‰ã«å¤šãã®ãƒãƒ¼ãƒ‰ã§Rayã‚’èµ·å‹•ã™ã‚‹å ´åˆã¯
ray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8

ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  --ref_num_nodes 1 \
  --ref_num_gpus_per_node 2 \
  --reward_num_nodes 1 \
  --reward_num_gpus_per_node 2 \
  --critic_num_nodes 1 \
  --critic_num_gpus_per_node 2 \
  --actor_num_nodes 1 \
  --actor_num_gpus_per_node 2 \
  --vllm_num_engines 4 \
  --vllm_tensor_parallel_size 2 \
  --colocate_all_models \
  --vllm_gpu_memory_utilization 0.5 \
  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
  --reward_pretrain OpenRLHF/Llama-3-8b-rm-700k \
  --save_path /openrlhf/examples/test_scripts/final/llama3-8b-rlhf \
  --ckpt_path /openrlhf/examples/test_scripts/ckpt/llama3-8b-rlhf \
  --save_hf_ckpt \
  --micro_train_batch_size 8 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 16 \
  --rollout_batch_size 1024 \
  --n_samples_per_prompt 1 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --max_samples 100000 \
  --generate_max_len 1024 \
  --zero_stage 3 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data OpenRLHF/prompt-collection-v0.1 \
  --input_key context_messages \
  --apply_chat_template \
  --normalize_reward \
  --gradient_checkpointing \
  --packing_samples \
  --vllm_sync_backend nccl \
  --enforce_eager \
  --vllm_enable_sleep \
  --deepspeed_enable_sleep \
  --use_wandb {wandb_token}

# REINFORCE++  | RLOO | REINFORCE++-baseline | GRPO | Dr. GRPO ã‚’ã‚µãƒãƒ¼ãƒˆ
# --advantage_estimator reinforce | rloo | reinforce_baseline | group_norm | dr_grpo

# --init_kl_coef ã‚’ 0 ã«è¨­å®šã™ã‚‹ã¨å‚ç…§ãƒ¢ãƒ‡ãƒ«ãŒèµ·å‹•ã—ã¾ã›ã‚“

# ãƒªãƒ¢ãƒ¼ãƒˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆHTTPï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆ
# --remote_rm_url http://localhost:5000/get_reward

# Nå€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆ
# --n_samples_per_prompt 4
```
> [!NOTE]
> ã¾ãŸã€``setup_commands``ã‚’ä½¿ç”¨ã—ã¦Rayã«ç’°å¢ƒã‚’è‡ªå‹•çš„ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã›ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ä¾‹ï¼š`--runtime-env-json='{"setup_commands": ["pip install openrlhf[vllm]"]}'`

> [!NOTE]
> OpenRLHFã®RLOOã¨REINFORCE++-baselineã¯REINFORCE++ã«åŸºã¥ãä¿®æ­£ç‰ˆã§ã™ï¼š
> - REINFORCE++ã¯ã€PPOã®ä¸»è¦ãªæœ€é©åŒ–æŠ€è¡“ï¼ˆã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸æ­£è¦åŒ–ã‚„PPO-clipãƒ­ã‚¹ãªã©ï¼‰ã‚’çµ±åˆã—ã€criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¿…è¦æ€§ã‚’æ’é™¤ã—ã¾ã™ã€‚
> - REINFORCE++-baselineã¯ã€`åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸè¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡å ±é…¬`ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦å ±é…¬ã‚’å†å½¢æˆã—ã¾ã™ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒæ­£è¦åŒ– `/std` ã‚’ä½¿ç”¨ï¼‰ã€‚
> - OpenRLHFã®RLOOã¯ã€`ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®KLå ±é…¬`ã‚’å°å…¥ã—ã€`PPO-clipãƒ­ã‚¹`ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å…ƒã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¿®æ­£ã—ã¦ã„ã¾ã™ã€‚
> - Dr. GRPOã¯ã€GRPOã®ã‚°ãƒ«ãƒ¼ãƒ—æ­£è¦åŒ– `/std` ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

> [!NOTE]
> deepspeedãŒGPUãƒ‡ãƒã‚¤ã‚¹ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹éš›ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²å¤–ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ç’°å¢ƒå¤‰æ•° [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py) ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ä¸€æ™‚çš„ãªè§£æ±ºç­–ã¨ã—ã¦å¯¾å¿œã§ãã¾ã™ã€‚
>   ```bash
>   # NVIDIA GPUã®å ´åˆï¼š
>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1
>   ```

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯[example/scripts](./examples/scripts/)ãŠã‚ˆã³[Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)ã«ã‚ã‚Šã¾ã™ã€‚

### Reinforced Fine-tuning

OpenRLHFã¯ã€ä¾¿åˆ©ã§åŠ¹ç‡çš„ãªReinforced Fine-tuningã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã‚«ã‚¹ã‚¿ãƒ  `reward_func` é–¢æ•°ã‚’å«ã‚€[ãƒ•ã‚¡ã‚¤ãƒ«](./examples/scripts/reward_func.py)ã‚’å®Ÿè£…ã—ã€ãã®ãƒ‘ã‚¹ã‚’ `