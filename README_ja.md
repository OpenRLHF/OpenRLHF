<div align="center">
    <img alt="OpenRLHF logo" src="./docs/logo.png" style="height: 140px;" />
</div>
<div align="center">
<p align="center">
      <a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/discussions">
        <img alt="Issues" src="https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff" />
      <a href="https://github.com/OpenRLHF/OpenRLHF/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf" />
      </a>
      <br>
      <em>ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ / åŒ…æ‹¬çš„ / è»½é‡ / ä½¿ã„ã‚„ã™ã„</em>
    </p>
</p>
</div>

<hr>

<span>[ <a href="README.md">English</a> | <a href="README_zh.md">ä¸­æ–‡</a> | æ—¥æœ¬èª ]</span>

OpenRLHFã¯ã€Rayã€vLLMã€ZeRO-3ã€ãŠã‚ˆã³HuggingFace Transformersã‚’åŸºç›¤ã¨ã—ãŸæœ€åˆã®é«˜æ€§èƒ½RLHFãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ï¼š

- **Rayãƒ™ãƒ¼ã‚¹ã®åˆ†æ•£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**  
  OpenRLHFã¯[Ray](https://github.com/ray-project/ray)ã‚’æ´»ç”¨ã—ã¦åŠ¹ç‡çš„ãªåˆ†æ•£ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã—ã¾ã™ã€‚Actorã€Rewardã€Referenceã€ãŠã‚ˆã³Criticãƒ¢ãƒ‡ãƒ«ã‚’ç•°ãªã‚‹GPUã«åˆ†æ•£ã—ã€70Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¾ã§ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚  
  ã¾ãŸã€**Hybrid Engine**ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã¨vLLMã‚¨ãƒ³ã‚¸ãƒ³ãŒGPUãƒªã‚½ãƒ¼ã‚¹ã‚’å…±æœ‰ã—ã€ã‚¢ã‚¤ãƒ‰ãƒ«æ™‚é–“ã‚’æœ€å°é™ã«æŠ‘ãˆã€GPUåˆ©ç”¨ç‡ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚
- **vLLM æ¨è«–åŠ é€Ÿ + AutoTP**  
  RLHF ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã® 80% ã®æ™‚é–“ã¯ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆæ®µéšã«è²»ã‚„ã•ã‚Œã¾ã™ã€‚[vLLM](https://github.com/vllm-project/vllm) ã¨ Auto Tensor Parallelism (AutoTP) ã‚’æ´»ç”¨ã—ã€OpenRLHF ã¯é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆã‚’å®Ÿç¾ã—ã¾ã™ã€‚HuggingFace Transformers ã¨ã®ãƒã‚¤ãƒ†ã‚£ãƒ–çµ±åˆã«ã‚ˆã‚Šã€ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã§é«˜é€Ÿãªç”Ÿæˆã‚’ä¿è¨¼ã—ã€ç¾åœ¨æœ€ã‚‚é«˜é€Ÿãª RLHF ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ãªã£ã¦ã„ã¾ã™ã€‚
- **ZeRO-3ãƒ™ãƒ¼ã‚¹ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**  
  [DeepSpeed](https://github.com/deepspeedai/DeepSpeed)ã®ZeRO-3ã¨[deepcompile](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md)ã‚’åŸºç›¤ã¨ã—ã€OpenRLHFã¯é‡é‡ç´šãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãªã—ã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚HuggingFaceã¨ç›´æ¥é€£æºã—ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ç°¡å˜ãªãƒ­ãƒ¼ãƒ‰ã¨å¾®èª¿æ•´ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
- **æœ€é©åŒ–ã•ã‚ŒãŸPPOå®Ÿè£…**  
  å®Ÿè·µã‚¬ã‚¤ãƒ‰ã¨ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«åŸºã¥ã„ãŸé«˜åº¦ãªPPOãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’çµ±åˆã—ã€RLHFãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®‰å®šæ€§ã¨å ±é…¬å“è³ªã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚[Zhihu](https://zhuanlan.zhihu.com/p/622134699)ã¨[Advanced Tricks for Training Large Language Models with Proximal Policy Optimization](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361)ã‚’å‚ç…§ã€‚

è©³ç´°ã¯[ã‚¹ãƒ©ã‚¤ãƒ‰](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [æŠ€è¡“å ±å‘Š](https://arxiv.org/abs/2405.11143) | [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://openrlhf.readthedocs.io/)ã‚’ã”è¦§ãã ã•ã„ã€‚

## ãƒ‹ãƒ¥ãƒ¼ã‚¹
- [2025/6] [Magistral](https://mistral.ai/static/research/magistral.pdf) ã¯ REINFORCE++-baseline ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ã„ã¾ã™ã€‚
- [2025/5] [MARTI](https://github.com/TsinghuaC3I/MARTI) ãŒ OpenRLHF ã®ãƒ•ã‚©ãƒ¼ã‚¯ã¨ã—ã¦ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸã€‚é›†ä¸­å‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç›¸äº’ä½œç”¨ã¨åˆ†æ•£å‹ãƒãƒªã‚·ãƒ¼è¨“ç·´ã‚’çµ±åˆã—ã€RL ã‚’ä½¿ç”¨ã—ãŸ LLM ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®è¨“ç·´ã‚’ç›®çš„ã¨ã—ã¦è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
- [2025/5] OpenRLHF 0.8.0 ã¯ [Async Pipeline RLHF](./examples/scripts/train_reinforce_baseline_llama_ray_async.sh) (`--async_train`) ã¨ [Async Agent RLHF](./examples/scripts/train_reinforce_baseline_llama_ray_agent_async.sh)(`--agent_func_path`) ãŠã‚ˆã³å†è¨­è¨ˆã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆAPIã‚’ã‚µãƒãƒ¼ãƒˆ
- [2025/4] ãƒ–ãƒ­ã‚°è¨˜äº‹ [Accelerating RLHF with vLLM, Best Practice from OpenRLHF](https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html) ã‚’å…¬é–‹
- [2025/4] Clean OpenRLHF: ã‚·ãƒ³ã‚°ãƒ«ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã¨çµ±åˆãƒ‘ãƒƒã‚­ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã«åŸºã¥ãã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
- [2025/3] CMUã®[2025å¹´æ˜¥ã®é«˜åº¦è‡ªç„¶è¨€èªå‡¦ç†ã‚³ãƒ¼ã‚¹](https://cmu-l3.github.io/anlp-spring2025/)ãŒOpenRLHFã‚’RLHFãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ•™è‚²äº‹ä¾‹ã¨ã—ã¦æ¡ç”¨ã€‚
- [2025/2] [Logic-RL](https://arxiv.org/abs/2502.14768) ã¨ [PRIME](https://arxiv.org/abs/2502.01456) ã¯ã€REINFORCE++ ãŒè¨“ç·´ã®å®‰å®šæ€§ã«ãŠã„ã¦ GRPO ã‚ˆã‚Šå„ªã‚Œã€PPO ã‚ˆã‚Šé«˜é€Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚
- [2025/2] [LMM-R1](https://github.com/TideDra/lmm-r1) ã¯ OpenRLHF ã®ãƒ•ã‚©ãƒ¼ã‚¯ã§ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã§ã® DeepSeek-R1 ã®å†ç¾ã®ãŸã‚ã®é«˜æ€§èƒ½ RL ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚
- [2025/2] MIT & Microsoft ã¯ OpenRLHF ã‚’ä½¿ç”¨ã—ã¦ [On the Emergence of Thinking in LLMs I: Searching for the Right Intuition](https://arxiv.org/pdf/2502.06773) ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚
- [2025/1] HKUSTã¯ [OpenRLHF ã‚’ä½¿ç”¨ã—ã¦å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã® DeepSeek-R1-Zero ã¨ DeepSeek-R1 ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°](https://github.com/hkust-nlp/simpleRL-reason)ã‚’å†ç¾ã—ã¾ã—ãŸã€‚
- [2024/12] ç§ãŸã¡ã¯ğŸ˜Š [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://arxiv.org/abs/2501.03262)ã‚’ã€Œææ¡ˆã€ã—ã¾ã—ãŸã€‚
- [2024/12] [Notionãƒ–ãƒ­ã‚°](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05)ã§PPOã€REINFORCE++ã€GRPOã€ãŠã‚ˆã³RLOOã‚’åˆ†æã—ã¾ã—ãŸã€‚
- [2023/8] OpenRLHF ãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¾ã—ãŸã€‚

## ç‰¹å¾´

- Rayã«åŸºã¥ãåˆ†æ•£[ PPO](./examples/scripts/train_ppo_llama_ray.sh)ãŠã‚ˆã³[EINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray.sh)ã®å®Ÿè£…ã€‚
- [Ray-based Reinforced Finetuning](./examples/scripts/train_ppo_llama_with_reward_fn.sh)
- Rayã¨Hybrid Engineã«åŸºã¥ã[PPO](./examples/scripts/train_ppo_llama_ray_hybrid_engine.sh)ãŠã‚ˆã³[REINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh)ã®ã‚µãƒãƒ¼ãƒˆ (`--colocate_all_models`, `--vllm_enable_sleep` and `--vllm_gpu_memory_utilization 0.5`)
- DAPOã‹ã‚‰ã®RL Dynamic Samplingã®ã‚µãƒãƒ¼ãƒˆ(`--dynamic_filtering` and `--dynamic_filtering_reward_range`)
- [DeepSpeed AutoTP ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°](./examples/scripts/train_sft_llama_tensor_parallelism.sh)ã®ã‚µãƒãƒ¼ãƒˆ (`--ds_tensor_parallel_size`)
- [70å„„ä»¥ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«](./examples/scripts/train_ppo_llama_ray_70b.sh)ã®å®Œå…¨ãªRLHFå¾®èª¿æ•´ã®ã‚µãƒãƒ¼ãƒˆã€‚
- RLHFã‚¿ã‚¹ã‚¯ã§ã®ç”Ÿæˆã‚’åŠ é€Ÿã™ã‚‹ãŸã‚ã®vLLMã®çµ±åˆï¼ˆ`--vllm_num_engines`ï¼‰ã€‚
- è¤‡æ•°ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆ`--reward_pretrain model1,model2...`ï¼‰ãŠã‚ˆã³ãƒªãƒ¢ãƒ¼ãƒˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆ`--remote_rm_url`ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [DPOï¼ˆç›´æ¥é¸å¥½æœ€é©åŒ–ï¼‰/IPO/cDPO](./examples/scripts/train_dpo_llama.sh)ãŠã‚ˆã³[Kahneman-Tversky Optimizationï¼ˆKTOï¼‰](./examples/scripts/train_kto_llama.sh)ã®å®Ÿè£…ã€‚
- [åå¾©DPO](./examples/scripts/train_iterative_dpo_llama.sh)ï¼ˆ[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [æ‹’å¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](./examples/scripts/train_rejection_sampling_llama.sh)ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [æ¡ä»¶ä»˜ãSFT](./examples/scripts/train_conditional_llama.sh)ï¼ˆ[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)ï¼‰ã®å®Ÿè£…ã€‚
- [çŸ¥è­˜è’¸ç•™](./examples/scripts/train_knowledge_distillation.sh)ï¼ˆ[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- [ãƒ—ãƒ­ã‚»ã‚¹å ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆPRMï¼‰](./examples/scripts/train_prm_mistral.sh)ã®çµ±åˆã€‚
- SFTã€DPOã€RMã€PRMã€ãŠã‚ˆã³PPOã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ‘ãƒƒã‚­ãƒ³ã‚°ï¼ˆ`--packing_samples`ï¼‰ã€‚
- [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)ã®å®Ÿè£…ï¼ˆ`--ring_attn_size`ã€`--ring_head_stride`ï¼‰ã€‚
- [å°‚é–€å®¶ã®æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆMoEï¼‰](./examples/test_scripts/train_sft_mixtral_lora.sh)ã®ã‚µãƒãƒ¼ãƒˆï¼ˆ`--aux_loss_coef`ï¼‰ã€‚
- FlashAttention2ã®çµ±åˆï¼ˆ`--flash_attn`ï¼‰ã€‚
- QLoRAï¼ˆ`--load_in_4bit`ï¼‰ãŠã‚ˆã³[LoRA](./examples/scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--lora_rank`ã€`--target_modules`ï¼‰ã®ã‚µãƒãƒ¼ãƒˆã€‚
- HuggingFaceã®`tokenizer.apply_chat_template`ã¨ã®äº’æ›æ€§ï¼ˆ`--apply_chat_template`ãŠã‚ˆã³`--input_key`ï¼‰ã€‚
- Wandbï¼ˆ`--use_wandb`ï¼‰ãŠã‚ˆã³TensorBoardï¼ˆ`--use_tensorboard`ï¼‰ã«ã‚ˆã‚‹ãƒ­ã‚°è¨˜éŒ²ã®ã‚µãƒãƒ¼ãƒˆã€‚
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å›å¾©æ©Ÿèƒ½ï¼ˆ`--load_checkpoint`ãŠã‚ˆã³`--save_steps`ï¼‰ã€‚
- [DPO](./examples/scripts/train_llama_slurm.sh)ãŠã‚ˆã³[Ray PPO](./examples/scripts/train_ppo_llama_ray_slurm.sh)ãªã©ã®ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã€‚

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

OpenRLHFã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã¾ãšDockerã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•ã—ï¼ˆ**æ¨å¥¨**ï¼‰ã€Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§`pip install`ã‚’å®Ÿè¡Œã—ã¦openrlhfã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼š

```bash
# Dockerã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•
docker run --runtime=nvidia -it --rm --shm-size="10g" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:25.02-py3 bash
sudo pip uninstall xgboost transformer_engine flash_attn pynvml -y

# pip install
pip install openrlhf

# vLLMåŠ é€Ÿã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆvLLM 0.10.0ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
pip install openrlhf[vllm]
# æœ€æ–°ã®vLLMã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™
pip install openrlhf[vllm_latest]
# vLLMã€ring-flash-attentionã€ãŠã‚ˆã³Liger-Kernelã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install openrlhf[vllm,ring,liger]

# æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’pip install
pip install git+https://github.com/OpenRLHF/OpenRLHF.git

# ã¾ãŸã¯git clone
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
```

> [!NOTE]
>vLLM 0.10.0ä»¥é™ã®ä½¿ç”¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚
>ã¾ãŸã€[vLLMç”¨ã®Dockerfile](./dockerfile/)ãŠã‚ˆã³[Nvidia-Dockerã®ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](./examples/scripts/nvidia_docker_install.sh)ã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
OpenRLHFã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹å†…ã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ–¹æ³•ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
ä¾‹ãˆã°ã€[Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6)ã§ã¯ï¼š

```python
def preprocess_data(data, input_template=None, input_key="input", apply_chat_template=None) -> str:
    if apply_chat_template:
        chat = data[input_key]
        if isinstance(chat, str):
            chat = [{"role": "user", "content": chat}]
        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    else:
        prompt = data[input_key]
        if input_template:
            prompt = input_template.format(prompt)
    return prompt
```

- `--input_key`ã‚’ä½¿ç”¨ã—ã¦ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®`JSON key name`ã‚’æŒ‡å®šã—ã€`--prompt_data {name or path}`ï¼ˆPPOï¼‰ã¾ãŸã¯`--dataset {name or path}`ã‚’ä½¿ç”¨ã—ã€`--apply_chat_template`ã‚’ä½¿ç”¨ã—ã¦[Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating)ã®`chat_template`ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚
- `--apply_chat_template`ã‚’ä½¿ç”¨ã—ãŸããªã„å ´åˆã¯ã€ä»£ã‚ã‚Šã«`--input_template`ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§å‰å‡¦ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- OpenRLHFã¯ã€`--prompt_data_probs 0.1,0.4,0.5`ï¼ˆPPOï¼‰ã¾ãŸã¯`--dataset_probs 0.1,0.4,0.5`ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ··åˆã™ã‚‹ã“ã¨ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

Chat Templatingã®å‹•ä½œæ–¹æ³•ï¼š

```python
dataset = [{"input_key": [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]}]

tokenizer.apply_chat_template(dataset[0]["input_key"], tokenize=False)

"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æŒ‡å®šæ–¹æ³•ã¯ï¼Ÿ

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ã¯ ``--eval_dataset {name or path}`` ã‚’ä½¿ç”¨ã—ã¦è¨­å®šã—ã¦ãã ã•ã„ã€‚

> [!NOTE]
> ``JSON key`` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ä¾å­˜ã—ã¾ã™ã€‚è©³ç´°ã¯ [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) ãŠã‚ˆã³ [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### æ•™å¸«ã‚ã‚Šå¾®èª¿æ•´

OpenRLHFã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯HuggingFaceãƒ¢ãƒ‡ãƒ«ã¨å®Œå…¨ã«äº’æ›æ€§ãŒã‚ã‚Šã¾ã™ã€‚`--pretrain  {name or path}`ã€`--reward_pretrain  {name or path}`ã€ãŠã‚ˆã³`--critic_pretrain  {name or path}`ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã„ãã¤ã‹ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’[HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF)ã§æä¾›ã—ã¦ã„ã¾ã™ã€‚

æ¬¡ã«ã€[examples/scripts](./examples/scripts/)ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æä¾›ã•ã‚Œã¦ã„ã‚‹èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã§ãã¾ã™ã€‚

```bash 
deepspeed --module openrlhf.cli.train_sft \
   --max_len 4096 \
   --dataset Open-Orca/OpenOrca \
   --input_key question \
   --output_key response \
   --input_template $'User: {}\nAssistant: ' \
   --train_batch_size 256 \
   --micro_train_batch_size 2 \
   --max_samples 500000 \
   --pretrain meta-llama/Meta-Llama-3-8B \
   --save_path ./checkpoint/llama3-8b-sft \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --zero_stage 2 \
   --max_epochs 1 \
   --packing_samples \
   --bf16 \
   --flash_attn \
   --learning_rate 5e-6 \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

# HF tokenizer.apply_chat_templateã®ã‚µãƒãƒ¼ãƒˆ
# --apply_chat_template 
# --tokenizer_chat_template {HF Chat Template}

# RingAttentionã®ã‚µãƒãƒ¼ãƒˆ
# pip install ring_flash_attn
#   --ring_attn_size 2 \
#   --ring_head_stride 2 \

# ç¶™ç¶šçš„ãªäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚‚ä½¿ç”¨ã§ãã¾ã™
# --pretrain_mode
```

> [!NOTE]
> OpenRLHF SFT/DPO/RewardModel/PPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã¯`--packing_samples`ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ [`--flash_attn`ã«åŸºã¥ã](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)

### å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
deepspeed --module openrlhf.cli.train_rm \
   --save_path ./checkpoint/llama3-8b-rm \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --train_batch_size 256 \
   --micro_train_batch_size 1 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --bf16 \
   --max_epochs 1 \
   --max_len 8192 \
   --zero_stage 3 \
   --learning_rate 9e-6 \
   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
   --apply_chat_template \
   --chosen_key chosen \
   --rejected_key rejected \
   --flash_attn \
   --packing_samples \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

```

å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®`--value_prefix_head`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’`score`ã«è¨­å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€`AutoModelForSequenceClassification`ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š

```python
reward_model = AutoModelForSequenceClassification.from_pretrained(
              reward_model_path,
              num_labels=1,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              use_cache=False,
          )
inputs = xxxx (Left Padding Input Tokens)
reward = reward_model.model(*inputs).last_hidden_state
reward = reward_model.score(reward)[:, -1]
```

### Rayã¨vLLMã‚’ä½¿ç”¨ã—ãŸPPO/REINFORCE++

RLHFãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã‹ã€70Bãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«ã€Rayã¨vLLMåŠ é€Ÿã‚’ä½¿ç”¨ã—ãŸPPOã‚’ä½¿ç”¨ã§ãã¾ã™ (Hybrid Engine)

```bash
# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§Rayã®ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‚’èµ·å‹•
ray start --head --node-ip-address 0.0.0.0 --num-gpus 8

# ã•ã‚‰ã«å¤šãã®ãƒãƒ¼ãƒ‰ã§Rayã‚’èµ·å‹•ã™ã‚‹å ´åˆã¯
ray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8

ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  --ref_num_nodes 1 \
  --ref_num_gpus_per_node 8 \
  --reward_num_nodes 1 \
  --reward_num_gpus_per_node 8 \
  --critic_num_nodes 1 \
  --critic_num_gpus_per_node 8 \
  --actor_num_nodes 1 \
  --actor_num_gpus_per_node 8 \
  --vllm_num_engines 4 \
  --vllm_tensor_parallel_size 2 \
  --colocate_all_models \
  --vllm_gpu_memory_utilization 0.5 \
  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
  --reward_pretrain OpenRLHF/Llama-3-8b-rm-700k \
  --save_path /openrlhf/examples/test_scripts/final/llama3-8b-rlhf \
  --ckpt_path /openrlhf/examples/test_scripts/ckpt/llama3-8b-rlhf \
  --save_hf_ckpt \
  --micro_train_batch_size 8 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 16 \
  --rollout_batch_size 1024 \
  --n_samples_per_prompt 1 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --max_samples 100000 \
  --generate_max_len 1024 \
  --zero_stage 3 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data OpenRLHF/prompt-collection-v0.1 \
  --input_key context_messages \
  --apply_chat_template \
  --normalize_reward \
  --gradient_checkpointing \
  --packing_samples \
  --vllm_sync_backend nccl \
  --enforce_eager \
  --vllm_enable_sleep \
  --deepspeed_enable_sleep \
  --use_wandb {wandb_token}

# REINFORCE++  | RLOO | REINFORCE++-baseline | GRPO | Dr. GRPO ã‚’ã‚µãƒãƒ¼ãƒˆ
# --advantage_estimator reinforce | rloo | reinforce_baseline | group_norm | dr_grpo

# --init_kl_coef ã‚’ 0 ã«è¨­å®šã™ã‚‹ã¨å‚ç…§ãƒ¢ãƒ‡ãƒ«ãŒèµ·å‹•ã—ã¾ã›ã‚“

# ãƒªãƒ¢ãƒ¼ãƒˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆHTTPï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆ
# --remote_rm_url http://localhost:5000/get_reward

# Nå€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆ
# --n_samples_per_prompt 4
```
> [!NOTE]
> ã¾ãŸã€``setup_commands``ã‚’ä½¿ç”¨ã—ã¦Rayã«ç’°å¢ƒã‚’è‡ªå‹•çš„ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã›ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ä¾‹ï¼š`--runtime-env-json='{"setup_commands": ["pip install openrlhf[vllm]"]}'`

> [!NOTE]
> OpenRLHFã®RLOOã¨REINFORCE++-baselineã¯REINFORCE++ã«åŸºã¥ãä¿®æ­£ç‰ˆã§ã™ï¼š
> - REINFORCE++ã¯ã€PPOã®ä¸»è¦ãªæœ€é©åŒ–æŠ€è¡“ï¼ˆã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸æ­£è¦åŒ–ã‚„PPO-clipãƒ­ã‚¹ãªã©ï¼‰ã‚’çµ±åˆã—ã€criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¿…è¦æ€§ã‚’æ’é™¤ã—ã¾ã™ã€‚
> - REINFORCE++-baselineã¯ã€`åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸè¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡å ±é…¬` RLVRè¨­å®šã§ã¯ã€å ±é…¬é–¢æ•°ã¯0/1ã¾ãŸã¯-1/1ã«å¯¾ã—ã¦æ•æ„Ÿã§ã¯ãªã„ãŸã‚ã€REINFORCE++ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸æ­£è¦åŒ–ã‚’é©ç”¨ã—ã¾ã™ã€‚
> - OpenRLHFã®RLOOã¯ã€`ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®KLå ±é…¬`ã‚’å°å…¥ã—ã€`PPO-clipãƒ­ã‚¹`ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å…ƒã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¿®æ­£ã—ã¦ã„ã¾ã™ã€‚
> - Dr. GRPOã¯ã€GRPOã®ã‚°ãƒ«ãƒ¼ãƒ—æ­£è¦åŒ– `/std` ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

> [!NOTE]
> deepspeedãŒGPUãƒ‡ãƒã‚¤ã‚¹ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹éš›ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²å¤–ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ç’°å¢ƒå¤‰æ•° [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py) ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ä¸€æ™‚çš„ãªè§£æ±ºç­–ã¨ã—ã¦å¯¾å¿œã§ãã¾ã™ã€‚
>   ```bash
>   # NVIDIA GPUã®å ´åˆï¼š
>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1
>   ```

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯[example/scripts](./examples/scripts/)ãŠã‚ˆã³[Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)ã«ã‚ã‚Šã¾ã™ã€‚

### å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (RFT)

OpenRLHFã¯ã€ä¾¿åˆ©ã§åŠ¹ç‡çš„ãªå¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã‚«ã‚¹ã‚¿ãƒ `reward_func`é–¢æ•°ã‚’å«ã‚€[ãƒ•ã‚¡ã‚¤ãƒ«](./examples/scripts/reward_func.py)ã‚’å®Ÿè£…ã—ã€ãã®ãƒ‘ã‚¹ã‚’`remote_rm_url`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æ¸¡ã™ã ã‘ã§æ¸ˆã¿ã¾ã™ã€‚ä¾‹ãˆã°ï¼š

```python
# reward_func.py
import torch

def reward_func(queries, prompts, labels):
    # queriesã¯prompts + responses
    # labelsã¯answers
    print(queries)

    # ä¾‹ã¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ãªå ±é…¬ã‚’ç”Ÿæˆ
    # å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ã“ã‚Œã‚’å®Ÿéš›ã®å ±é…¬è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã«ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
    reward = torch.randint(0, 2, (len(queries),)).float()

    return {
        "rewards": reward,  # ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ç”¨ã®å ±é…¬
        "scores": reward,  # å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ã‚¹ã‚³ã‚¢ï¼ˆ0-1å ±é…¬ï¼‰
        "extra_logs": {"dummy_scores": reward},  # wandbç”¨ã®è¿½åŠ ãƒ­ã‚°æƒ…å ±
    }
```

ãã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã™ã‚‹ã ã‘ã§ã™ï¼š

```shell 
ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  ...
  --remote_rm_url /path/to/reward_func.py \
  --label_key answer
```

ã“ã“ã§ã€`label_key`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ç­”ãˆãªã©ã®è¿½åŠ ã®ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã‚’å ±é…¬é–¢æ•°ã«æ¸¡ã™ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

## éåŒæœŸRLHFã¨Agent RLHF

OpenRLHFã¯ã€éåŒæœŸRLHFã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®RLHFå®Ÿè£…ã®ä¸¡æ–¹ã‚’åŒ…æ‹¬çš„ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã«`--async_train`ã¨`--agent_func_path`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã ã‘ã§ã™ã€‚

Agent APIã¯ã€ã‚ˆã‚Šè‰¯ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ€§ã¨æ‹¡å¼µæ€§ã‚’æä¾›ã™ã‚‹ãŸã‚ã«ã€`AgentInstanceBase`ã¨`AgentExecutorBase`ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ãŸã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å†è¨­è¨ˆã•ã‚Œã¾ã—ãŸã€‚

```python
# agent_func.py
import random
from typing import Any, Dict

import torch
from openrlhf.utils.agent import AgentExecutorBase, AgentInstanceBase


# A simple n-step random environment
class AgentInstance(AgentInstanceBase):
    async def __init__(self, *args, **kwargs):
        self.step_idx = 0
        self.max_steps = random.randint(1, 3)  # 1-3 steps

    async def reset(self, states: dict, **kwargs):
        return {"observation": states["observation"]}  # Return original text observation

    async def step(self, states: dict, **kwargs) -> Dict[str, Any]:
        print(f"step_idx: {self.step_idx}, max_steps: {self.max_steps}")

        observation_text = states["observation_text"]
        action_text = states["action_text"]
        label = states["label"]

        # Check if episode is done
        done = self.step_idx >= self.max_steps
        reward = torch.randint(0, 2, (1,)).float() if done else torch.tensor(0)

        # Generate environment feedback based on whether episode is done
        environment_feedback = (
            "\n\nHuman: [CORRECT]\n</s>"
            if done
            else "\n\nHuman: [INCORRECT]\nPlease analyze the issues and try again.\n</s>\n\nAssistant: "
        )

        self.step_idx += 1

        return {
            "rewards": reward,  # Rewards for advantage calculation
            "scores": reward,  # Scores for dynamic filtering (0-1 reward)
            "environment_feedback": environment_feedback,  # Environment feedback text
            "done": done,  # Boolean indicating if the episode is complete
            "sampling_params": states.get("sampling_params", None),  # Parameters for vLLM sampling in next step
            "extra_logs": {"dummy_scores": reward},  # Additional logging information
        }


# You could override the execute function of AgentExecutorBase to add custom agent running logic
class AgentExecutor(AgentExecutorBase):
    def __init__(self, max_steps, max_length, llm_engine, hf_tokenizer, result_queue):
        super().__init__(AgentInstance, max_steps, max_length, llm_engine, hf_tokenizer, result_queue)

    async def execute(self, prompt, label, sampling_params):
        # You could override the execute function of AgentExecutorBase to add custom agent running logic
        return await super().execute(prompt, label, sampling_params)
```

ã¾ãŸã€`export OPENRLHF_ASYNC_NUM_TASKS=128`ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€vLLMã‚¨ãƒ³ã‚¸ãƒ³ã”ã¨ã®æœ€å¤§åŒæ™‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°ã‚’è¨­å®šã§ãã¾ã™ã€‚
ã•ã‚‰ã«ã€ç’°å¢ƒã§`export OPENRLHF_ASYNC_QUEUE_SIZE=1`ï¼ˆã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒæ•°ã‚’åˆ¶å¾¡ã—ã¾ã™ï¼‰ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç¨‹åº¦ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚

> [!NOTE]
> `AgentExecutorBase`ã®`execute`é–¢æ•°ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã™ã‚‹ã“ã¨ã§ã€å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè£…ã§ãã¾ã™ã€‚ã“ã®è¨­è¨ˆã¯**token-in-token-outåŸå‰‡**ã«å¾“ã„ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«é–“ã®ä¸€è²«æ€§ã‚’ç¢ºä¿ã—ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã®å‡¦ç†ã§ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ä¸æ•´åˆã‚’å›é¿ã—ã¾ã™ã€‚



> [!NOTE] 
> OpenRLHFã®Agent RLHFã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€`--async_train`ãƒ•ãƒ©ã‚°ã‚’å‰Šé™¤ã—ã€`--colocate_all_models`ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚

> [!WARNING] 
> éåŒæœŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³ã¾ãŸã¯åŒæœŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’å„ªå…ˆã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

### LoRA
`LoRA (Low-Rank Adaptation)`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€`OpenRLHF`ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å®Œå…¨ãªé‡ã¿ã‚’ä¿å­˜ã›ãšã€ä»£ã‚ã‚Šã«`LoRA Adapter`ã‚’ä¿å­˜ã—ã¾ã™ã€‚ã‚¿ã‚¹ã‚¯ã‚’æ­£å¸¸ã«ç¶šè¡Œã™ã‚‹ã«ã¯ã€`Adapter`ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨çµåˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

```bash
python -m openrlhf.cli.lora_combiner \
    --model_path meta-llama/Meta-Llama-3-8B \
    --lora_path ./checkpoint/llama3-8b-rm \
    --output_path ./checkpoint/llama-3-8b-rm-combined \
    --is_rm \
    --bf16
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã«ã€ãƒãƒ¼ãƒ‰ã‚’ `vLLM:Actor:Critic = 1:1:1` ã®æ¯”ç‡ã§å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

- ä¾‹ãˆã°ã€70Bãƒ¢ãƒ‡ãƒ«ã¨48å€‹ã®A100 GPUã®å ´åˆã€16å€‹ã®A100 GPUã‚’vLLMã‚¨ãƒ³ã‚¸ãƒ³ã«ã€16å€‹ã®GPUã‚’Actorãƒ¢ãƒ‡ãƒ«ã«ã€æ®‹ã‚Šã®16å€‹ã®GPUã‚’Criticãƒ¢ãƒ‡ãƒ«ã«å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚
- RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åæŸæ€§ãŒè¦æ±‚ã‚’æº€ãŸã™å ´åˆã¯ã€éåŒæœŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° `--async_train` ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚
- GPUãƒ¡ãƒ¢ãƒªãŒååˆ†ã«ã‚ã‚‹å ´åˆã¯ã€åˆ†æ•£RLHFã§ã¯ãªãã€hybrid engine `--colocate_all_models` ã¨ `--vllm_enable_sleep` ãŠã‚ˆã³ `--deepspeed_enable_sleep` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- `--colocate_critic_reward`ã€`--colocate_actor_ref` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹ã«ã—ã¦ãƒãƒ¼ãƒ‰ã‚’çµ±åˆã—ã¾ã™ã€‚
- `rollout_micro_batch_size` ã‚’å¯èƒ½ãªé™ã‚Šå¢—ã‚„ã—ï¼ˆvLLMã‚¨ãƒ³ã‚¸ãƒ³ã®TPã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–ï¼‰ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯ `--micro_train_batch_size` ã‚’å¤§ããã—ã€`--packing_samples` ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚
- GPUãƒ¡ãƒ¢ãƒªãŒååˆ†ã«ã‚ã‚‹å ´åˆã¯ã€`--adam_offload` ã‚’ç„¡åŠ¹ã«ã—ã€`--overlap_comm` ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€`--deepcompile` ã‚’æœ‰åŠ¹ã«ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã—ã¦ãã ã•ã„ã€‚
- vLLMã«ã¯ `--vllm_sync_backend nccl` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- `n_samples_per_prompts` > 1 ã®å ´åˆã¯ã€vLLMç”Ÿæˆã§ [enable_prefix_caching](https://docs.vllm.ai/en/stable/automatic_prefix_caching/apc.html) ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚
- å¤§è¦æ¨¡ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€OOMãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€`--colocate_xxxx` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

## OpenRLHFã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ä¼æ¥­ã¨çµ„ç¹”

- Google
- ByteDance
- Tencent
- Alibaba
- Baidu
- China Telecom
- Vivo
- Allen AI
- NexusFlow
- JÃ¼lich Supercomputing Centre (JSC)
- Berkeley Starling Team
- M-A-P
- ...

## å‚åŠ æ–¹æ³•

**å‚åŠ æ–¹æ³•ã¯ï¼Ÿ**

1. janhu9527@gmail.com ã«ãƒ¡ãƒ¼ãƒ«ã‚’é€ã‚‹ã‹ã€[GitHub Organization](https://github.com/OpenRLHF) ã«å‚åŠ ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®è©³ç´°ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
   - ãŠåå‰
   - GitHubãƒ¦ãƒ¼ã‚¶ãƒ¼å
   - èˆˆå‘³ã®ã‚ã‚‹åˆ†é‡
   - NLPã‚„AIã«é–¢é€£ã™ã‚‹ã‚¹ã‚­ãƒ«ã¨çµŒé¨“
2. å…¬å¼GitHub [OpenRLHF â†—](https://github.com/OpenRLHF/OpenRLHF) ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰å‚åŠ ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚è²¢çŒ®ã¸ã®èˆˆå‘³ã«ã¤ã„ã¦issueã‚’ä½œæˆã™ã‚‹ã ã‘ã§ã€ç§ãŸã¡ãŒå¯¾å¿œã—ã¾ã™ã€‚

**ä½•ãŒã§ãã¾ã™ã‹ï¼Ÿ**

1. ãƒãƒ¼ãƒ ã«å‚åŠ ã—ã¦OpenRLHFãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é–‹ç™ºã«å‚åŠ ã™ã‚‹
2. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æå‡ºã—ã¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«è²¢çŒ®ã™ã‚‹
3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ”¹å–„ã€ãƒã‚°ã®ä¿®æ­£ã€æ–°æ©Ÿèƒ½ã®ä½œæˆã‚’æ”¯æ´ã™ã‚‹
4. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å…±æœ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æˆé•·ã‚’æ”¯æ´ã™ã‚‹

## ã‚¹ãƒãƒ³ã‚µãƒ¼

ã‚¹ãƒãƒ³ã‚µãƒ¼ã‚·ãƒƒãƒ—ã¯ã€OpenRLHFã®ç¶­æŒã¨æ”¹å–„ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå½¹ç«‹ã¤ã¨æ„Ÿã˜ãŸå ´åˆã¯ã€[Open Collective â†—](https://opencollective.com/OpenRLHF) ã§ã‚¹ãƒãƒ³ã‚µãƒ¼ã«ãªã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚

## ã‚¹ã‚¿ãƒ¼å±¥æ­´

[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)

## è²¢çŒ®è€…

ã™ã¹ã¦ã®è²¢çŒ®è€…ã«æ„Ÿè¬ã—ã¾ã™ï¼è²¢çŒ®ã—ãŸã„å ´åˆã¯ã€ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹ã‹ã€issueã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

<a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF" />
</a>

## å‚è€ƒæ–‡çŒ®ã¨è¬è¾

AIã¨NLPåˆ†é‡ã¸ã®è²¢çŒ®ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨çµ„ç¹”ã«æ„Ÿè¬ã®æ„ã‚’è¡¨ã—ã¾ã™ï¼š

- [Hugging Face Transformers â†—](https://github.com/huggingface/transformers)
- [OpenAI GPT â†—](https://github.com/openai/gpt-3)
- [LLaMA â†—](https://llama.meta.com/)
- [DeepSpeed â†—](https://github.com/microsoft/DeepSpeed)
- [Ray â†—](https://github.com/ray-project/ray)

ç§ãŸã¡ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalChat) ã¨ [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat) ã«ã‚‚æ„Ÿè¬ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸæ®µéšã§ã€å½¼ã‚‰ã®ã‚³ãƒ¼ãƒ‰è¨­è¨ˆã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚
ç§ãŸã¡ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ãƒªãƒ³ã‚°ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é–‹ç™ºã®GPUã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¦ãã‚ŒãŸ [Netmind.AI](https://www.netmind.ai/) ã«ã‚‚æ„Ÿè¬ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚

(2024/7) ç§ãŸã¡ã®GitHubçµ„ç¹”ã¯OpenLLMAIã‹ã‚‰OpenRLHFã«å¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚

## å¼•ç”¨
OpenRLHF
```
@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}
```
REINFORCE++-baseline
```
@article{hu2025reinforce++,
  title={Reinforce++: A simple and efficient approach for aligning large language models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}
```

______________________________________________________________________

*OpenRLHF Â© 2025 OpenRLHF. All Rights Reserved.*