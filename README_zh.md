<div align="center">
    <img alt="OpenRLHF logo" src="./docs/logo.png" style="height: 140px;" />
</div>
<div align="center">
<p align="center">
      <a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/discussions">
        <img alt="Issues" src="https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf" />
      </a>
      <a href="https://deepwiki.com/OpenRLHF/OpenRLHF"><img src="https://deepwiki.com/badge.svg" alt="Ask DeepWiki"></a>
      <br>
      <em>å¼€æº / å…¨é¢ / è½»é‡çº§ / æ˜“ç”¨</em>
    </p>
</div>

<hr>

<span>[ <a href="README.md">English</a> | ä¸­æ–‡ | <a href="README_ja.md">æ—¥æœ¬èª</a> ]</span>

OpenRLHF æ˜¯**é¦–ä¸ª**ç»“åˆ **Ray + vLLM åˆ†å¸ƒå¼æ¶æ„**ä¸**ç»Ÿä¸€ Agent è®¾è®¡èŒƒå¼**çš„é«˜æ€§èƒ½ã€ç”Ÿäº§å°±ç»ªçš„å¼€æº RLHF æ¡†æ¶ï¼Œç”¨äºå¯æ‰©å±•å’Œå¯æ‰©å±•çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ã€‚

ğŸ“š **äº†è§£æ›´å¤š**ï¼š[æ–‡æ¡£](https://openrlhf.readthedocs.io/) | [PPT](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [æŠ€æœ¯æŠ¥å‘Š](https://www.researchgate.net/publication/393414548_OpenRLHF_An_Easy-to-use_Scalable_and_High-performance_RLHF_Framework) | [è§†é¢‘](https://www.bilibili.com/video/BV1dv2jBxEQG/)

## ğŸ“– ç›®å½•

- [ğŸ—ï¸ æ–°é—»](#æ–°é—»)
- [ğŸ—ï¸ æ¶æ„åŸºç¡€](#æ¶æ„åŸºç¡€ray--vllm-åˆ†å¸ƒå¼) - Ray + vLLM + DeepSpeed åˆ†å¸ƒå¼åŸºç¡€è®¾æ–½
- [ğŸ¯ è®¾è®¡èŒƒå¼](#è®¾è®¡èŒƒå¼åŸºäº-agent-çš„æ‰§è¡Œ) - ç»Ÿä¸€çš„ Agent æ‰§è¡Œæµç¨‹
- [ğŸš€ RL ç®—æ³•](#æœ€å…ˆè¿›çš„-rl-ç®—æ³•) - PPOã€REINFORCE++ã€GRPOã€RLOO
- [ğŸ“‹ ç‰¹æ€§æ¦‚è§ˆ](#å…¨é¢ç‰¹æ€§) - å®Œæ•´çš„ RLHF æµç¨‹èƒ½åŠ›
- [ğŸ¬ å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) - å®‰è£…å’Œå…¸å‹å·¥ä½œæµ
- [ğŸ“ è®­ç»ƒæŒ‡å—](#ç›‘ç£å¾®è°ƒ) - SFTã€å¥–åŠ±æ¨¡å‹ã€RL è®­ç»ƒ
- [ğŸ¯ å•è½® Agent](#å•è½®-agentå¼ºåŒ–å¾®è°ƒä¸è‡ªå®šä¹‰å¥–åŠ±) - è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
- [ğŸ¤– å¤šè½® Agent](#å¤šè½®-agentå¤æ‚ç¯å¢ƒäº¤äº’) - å¤æ‚ç¯å¢ƒ
- [ğŸ”§ é«˜çº§ä¸»é¢˜](#é«˜çº§ä¸»é¢˜) - LoRAã€æ€§èƒ½è°ƒä¼˜

---

<a id="æ–°é—»"></a>
## æ–°é—»

<details>
<summary>å±•å¼€æ–°é—»</summary>

- [2026/2] [ProRL V2](https://developer.nvidia.com/blog/scaling-llm-reinforcement-learning-with-prolonged-training-using-prorl-v2/) ä½¿ç”¨ REINFORCE++-baseline é€šè¿‡é•¿æœŸ RL è®­ç»ƒè®­ç»ƒæœ€å…ˆè¿›çš„ 1.5B æ¨ç†æ¨¡å‹ã€‚è®­ç»ƒè„šæœ¬ï¼š[train_prorlv2_math_hybrid_engine.sh](./examples/scripts/train_prorlv2_math_hybrid_engine.sh)
- [2025/10] [ScaleRL](https://arxiv.org/abs/2510.13786) éªŒè¯äº† REINFORCE++-baseline åœ¨å¤§è§„æ¨¡è®­ç»ƒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚å‘å¸ƒ [REINFORCE++ PPT](https://docs.google.com/presentation/d/1stieP_3PM1z4Hq1YWR3GywFkxcHEAlstXMaS23KlGN4)
- [2025/6] [Magistral](https://mistral.ai/static/research/magistral.pdf) ä½¿ç”¨ä¸ REINFORCE++-baseline éå¸¸ç›¸ä¼¼çš„æ–¹æ³•è®­ç»ƒæ¨ç†æ¨¡å‹ã€‚
- [2025/5] [MARTI](https://github.com/TsinghuaC3I/MARTI) ä½œä¸º OpenRLHF çš„åˆ†æ”¯å‘å¸ƒã€‚å®ƒæ—¨åœ¨é€šè¿‡é›†æˆä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“äº¤äº’ä¸åˆ†å¸ƒå¼ç­–ç•¥è®­ç»ƒæ¥è®­ç»ƒåŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚
- [2025/5] OpenRLHF 0.8.0 æ”¯æŒé€šè¿‡ `--async_train` å¯ç”¨å¼‚æ­¥ RLHF è®­ç»ƒï¼Œå¹¶é€šè¿‡ `--agent_func_path` å¯ç”¨å¼‚æ­¥ Agent RLHFã€‚å¯è¿è¡Œç¤ºä¾‹è§ [train_reinforce_baseline_ray_agent_async.sh](./examples/scripts/train_reinforce_baseline_ray_agent_async.sh)ã€‚
- [2025/4] å‘å¸ƒåšå®¢ [Accelerating RLHF with vLLM, Best Practice from OpenRLHF](https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html)
- [2025/4] Clean OpenRLHFï¼šåŸºäºå•æ§åˆ¶å™¨å’Œç»Ÿä¸€æ‰“åŒ…æ ·æœ¬é‡æ„äº†æºä»£ç 
- [2025/3] CMU [é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç† 2025 æ˜¥å­£](https://cmu-l3.github.io/anlp-spring2025/)è¯¾ç¨‹ä½¿ç”¨ OpenRLHF ä½œä¸º RLHF æ¡†æ¶æ•™å­¦æ¡ˆä¾‹ã€‚
- [2025/2] [Logic-RL](https://arxiv.org/abs/2502.14768) å’Œ [PRIME](https://arxiv.org/abs/2502.01456) è¯æ˜ REINFORCE++ ç›¸æ¯” GRPO æ›´ç¨³å®šï¼Œæ¯” PPO æ›´å¿«ã€‚
- [2025/2] [LMM-R1](https://github.com/TideDra/lmm-r1) æ˜¯ OpenRLHF çš„åˆ†æ”¯ï¼Œæ—¨åœ¨ä¸ºå¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„ DeepSeek-R1 å¤ç°æä¾›é«˜æ€§èƒ½ RL åŸºç¡€è®¾æ–½ã€‚
- [2025/2] MIT å’Œå¾®è½¯ä½¿ç”¨ OpenRLHF æå‡º [On the Emergence of Thinking in LLMs I: Searching for the Right Intuition](https://arxiv.org/pdf/2502.06773)
- [2025/1] HKUST ä½¿ç”¨ OpenRLHF å¤ç°äº†[å°æ¨¡å‹ä¸Šçš„ DeepSeek-R1-Zero å’Œ DeepSeek-R1 è®­ç»ƒ](https://github.com/hkust-nlp/simpleRL-reason)
- [2024/12] æˆ‘ä»¬"æå‡º"äº†ğŸ˜Š [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://www.researchgate.net/publication/387487679_REINFORCE_An_Efficient_RLHF_Algorithm_with_Robustnessto_Both_Prompt_and_Reward_Models)ã€‚
- [2024/12] æˆ‘ä»¬åœ¨ [Notion åšæ–‡](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05)ä¸­åˆ†æäº† PPOã€REINFORCE++ã€GRPO å’Œ RLOOã€‚
- [2023/8] OpenRLHF å¼€æºã€‚

</details>

---

<a id="æ¶æ„åŸºç¡€ray--vllm-åˆ†å¸ƒå¼"></a>
## ğŸ—ï¸ æ¶æ„åŸºç¡€ï¼šRay + vLLM åˆ†å¸ƒå¼

OpenRLHF æ˜¯**é¦–ä¸ª**åŸºäº Ray + vLLM åˆ†å¸ƒå¼æ¶æ„æ„å»ºçš„ RLHF æ¡†æ¶ï¼Œå¯é«˜æ•ˆåœ°è·¨ GPU ç¼–æ’å¤šä¸ªç»„ä»¶ï¼š

<div align="center">
  <img alt="OpenRLHF æ¶æ„ï¼ˆRay + vLLMï¼‰" src="./docs/openrlhf_architecture.svg" style="max-width: 100%; height: auto;" />
</div>

### æ ¸å¿ƒåŸºç¡€è®¾æ–½ç»„ä»¶

**Ray - åˆ†å¸ƒå¼è°ƒåº¦å™¨å’Œæ§åˆ¶å™¨**  
OpenRLHF åˆ©ç”¨ [Ray](https://github.com/ray-project/ray) å®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼è°ƒåº¦ã€‚å®ƒå°† Actorã€Rewardã€Reference å’Œ Critic æ¨¡å‹åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸Šï¼Œæ”¯æŒè®­ç»ƒé«˜è¾¾ **70B+ å‚æ•°**çš„æ¨¡å‹ã€‚

**æ··åˆå¼•æ“è°ƒåº¦**ï¼šæ‰€æœ‰æ¨¡å‹å’Œ vLLM å¼•æ“å¯ä»¥å…±äº« GPU èµ„æºï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘ç©ºé—²æ—¶é—´å¹¶æé«˜ GPU åˆ©ç”¨ç‡ã€‚è¿™å…è®¸åœ¨æœ‰é™çš„ç¡¬ä»¶ä¸Šè¿è¡Œå®Œæ•´çš„ RLHF æµç¨‹ã€‚

**vLLM - é«˜æ€§èƒ½æ¨ç†å¼•æ“**  
RLHF è®­ç»ƒä¸­ **80% çš„æ—¶é—´**èŠ±åœ¨æ ·æœ¬ç”Ÿæˆä¸Šã€‚é€šè¿‡ [vLLM](https://github.com/vllm-project/vllm) ä¸è‡ªåŠ¨å¼ é‡å¹¶è¡Œï¼ˆAutoTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ï¼ŒOpenRLHF æä¾›é«˜ååé‡ã€å†…å­˜é«˜æ•ˆçš„ç”Ÿæˆã€‚

**DeepSpeed - å†…å­˜é«˜æ•ˆè®­ç»ƒ**  
åŸºäº [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ZeRO-3ã€[deepcompile](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md)ã€[AutoTP](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md) å’Œ RingAttentionã€‚æ”¯æŒå¤§æ¨¡å‹è®­ç»ƒè€Œæ— éœ€é‡é‡çº§æ¡†æ¶ï¼Œç›´æ¥ä¸ HuggingFace æ¨¡å‹é…åˆä½¿ç”¨ã€‚

**Transformers - æ¨¡å‹æ¥å£**  
ä¸ HuggingFace Transformers åŸç”Ÿé›†æˆï¼Œå¯æ— ç¼åŠ è½½æ¨¡å‹ã€çŠ¶æ€ç®¡ç†å’Œå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚

**NCCL / CUDA IPC - é«˜é€Ÿé€šä¿¡**  
é«˜æ•ˆçš„ GPU é—´é€šä¿¡ï¼Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†ã€‚

---

<a id="è®¾è®¡èŒƒå¼åŸºäº-agent-çš„æ‰§è¡Œ"></a>
## ğŸ¯ è®¾è®¡èŒƒå¼ï¼šåŸºäº Agent çš„æ‰§è¡Œ

**åœ¨ Ray åˆ†å¸ƒå¼æ¶æ„ä¹‹ä¸Š**ï¼ŒOpenRLHF æ˜¯**é¦–ä¸ª**å®ç°**ç»Ÿä¸€ Agent èŒƒå¼**çš„ RLHF æ¡†æ¶ã€‚æ— è®ºæ˜¯æ ‡å‡† PPO è¿˜æ˜¯å¤æ‚çš„å¤šè½®æ¨ç†ï¼Œæ¯æ¬¡è®­ç»ƒè¿è¡Œéƒ½éµå¾ªä¸€è‡´çš„ Agent æ‰§è¡Œæµç¨‹ã€‚

### ä¸ºä»€ä¹ˆé‡‡ç”¨ Agent èŒƒå¼ï¼Ÿ

OpenRLHF **é€šè¿‡ token-in-token-out çš„ Agent æ‰§è¡Œç»Ÿä¸€ç”Ÿæˆå’Œè®­ç»ƒ**ï¼Œç¡®ä¿å®Œç¾ä¸€è‡´æ€§ã€è½»æ¾çš„å•è½®/å¤šè½®æ‰©å±•ï¼Œä»¥åŠé›¶æ–‡æœ¬çº§åˆ«ä¸åŒ¹é…ã€‚

### Agent æ¶æ„

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    AgentExecutorBase        â”‚
                 â”‚  (Token-in-Token-out æ ¸å¿ƒ)  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â†“                         â†“
         SingleTurnExecutor        MultiTurnExecutor
                 â”‚                         â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â†“                     â†“   â†“                    â†“
  æ ‡å‡† RLHF          è‡ªå®šä¹‰å¥–åŠ±    å¤šæ­¥æ¨ç†        å¤–éƒ¨ç¯å¢ƒ
  (å•æ¬¡ç”Ÿæˆ)           å‡½æ•°                      (NeMo Gym)
      â†“                     â†“           â†“                â†“
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    ä¸€è‡´çš„ Token è½¨è¿¹
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  RL ç®—æ³•          â”‚
                    â”‚  (è§£è€¦)           â”‚
                    â”‚                   â”‚
                    â”‚  PPO, REINFORCE++ â”‚
                    â”‚  GRPO, RLOO ç­‰    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

<details>
<summary>å±•å¼€æ ¸å¿ƒè®¾è®¡åŸåˆ™</summary>

| åŸåˆ™ | æè¿° | ä¼˜åŠ¿ |
|------|------|------|
| **Token-in-Token-out** | æ‰€æœ‰é‡‡æ ·äº§ç”Ÿ token çº§è½¨è¿¹ | é›¶æ–‡æœ¬çº§ä¸åŒ¹é… |
| **ç»Ÿä¸€æ¥å£** | æ‰€æœ‰æ¨¡å¼ä½¿ç”¨ç›¸åŒçš„ `AgentExecutorBase` API | ä¸€ä¸ªæ ‡å¿—åˆ‡æ¢æ¨¡å¼ |
| **ç®—æ³•æ— å…³** | RL ç®—æ³•ï¼ˆPPOã€REINFORCE++ ç­‰ï¼‰ä¸ Agent æ‰§è¡Œå™¨è§£è€¦ | ä»»ä½•ç®—æ³•é€‚ç”¨äºä»»ä½•æ¨¡å¼ |
| **å¯æ‰©å±•** | è½»æ¾æ’å…¥è‡ªå®šä¹‰å¥–åŠ±/ç¯å¢ƒ | å¿«é€Ÿå®éªŒ |
| **ç”Ÿäº§å°±ç»ª** | æ”¯æŒåŒæ­¥/å¼‚æ­¥/æ··åˆå¼•æ“ | ä»ç ”ç©¶åˆ°éƒ¨ç½² |

</details>

### ä¸¤ç§æ‰§è¡Œæ¨¡å¼ï¼ˆä¸ RL ç®—æ³•æ­£äº¤ï¼‰

Agent æ‰§è¡Œæ¨¡å¼ä¸æ‚¨é€‰æ‹©çš„ RL ç®—æ³•**ç‹¬ç«‹**ã€‚æ‚¨å¯ä»¥å°†**ä»»ä½•ç®—æ³•**ï¼ˆPPOã€REINFORCE++ã€GRPO ç­‰ï¼‰ä¸**ä»»ä½•æ‰§è¡Œæ¨¡å¼**é…åˆä½¿ç”¨ï¼š

| æ¨¡å¼ | ä½¿ç”¨åœºæ™¯ | æ¥å£ | å¤æ‚åº¦ |
|------|---------|------|--------|
| **å•è½®** | æ ‡å‡† RLHFã€è‡ªå®šä¹‰å¥–åŠ±å‡½æ•° | å¯é€‰ `reward_func()` | â­ é»˜è®¤ï¼ˆ99% ç”¨ä¾‹ï¼‰|
| **å¤šè½®** | å¤šæ­¥æ¨ç†ã€äº¤äº’å¼ç¯å¢ƒ | `reset()` + `step()` | â­â­ é«˜çº§ |

---

<a id="æœ€å…ˆè¿›çš„-rl-ç®—æ³•"></a>
## ğŸš€ æœ€å…ˆè¿›çš„ RL ç®—æ³•

OpenRLHF å®ç°äº† **PPOã€REINFORCE++ã€REINFORCE++-baselineã€GRPOã€RLOO**ï¼Œé‡‡ç”¨å—å®è·µæŒ‡å—å’Œç¤¾åŒºæœ€ä½³å®è·µå¯å‘çš„é«˜çº§ä¼˜åŒ–æŠ€å·§ã€‚

**å…³é”®è®¾è®¡**ï¼šRL ç®—æ³•ä¸ Agent æ‰§è¡Œæ¨¡å¼**è§£è€¦**ã€‚æ‰€æœ‰ç®—æ³•éƒ½å¯ä»¥ä¸å•è½®å’Œå¤šè½® Agent æ‰§è¡Œå™¨æ— ç¼é…åˆï¼Œé€šè¿‡ç»Ÿä¸€çš„ token-in-token-out æµç¨‹è¿è¡Œï¼Œç¡®ä¿è¡Œä¸ºä¸€è‡´ã€‚

<details>
<summary>å±•å¼€ç®—æ³•å¯¹æ¯”è¡¨</summary>

| ç®—æ³• | `--advantage_estimator` | å…³é”®ç‰¹æ€§ | æœ€ä½³ç”¨ä¾‹ |
|------|-------------------------|---------|---------|
| **PPO** | (é»˜è®¤) | å®Œæ•´ critic ç½‘ç»œ | ç¨³å®šè®­ç»ƒï¼Œæˆç†Ÿç»“æœ |
| **REINFORCE++** | `reinforce` | æ—  critic çš„ PPO æŠ€å·§ | é«˜æ•ˆè®­ç»ƒï¼Œæ›´å°‘å†…å­˜ |
| **REINFORCE++-baseline** | `reinforce_baseline` | å‡å€¼å¥–åŠ±åŸºçº¿ | æ¨ç†ä»»åŠ¡ï¼ˆRLVRï¼‰ï¼Œå¯¹å¥–åŠ±å°ºåº¦é²æ£’ |
| **RLOO** | `rloo` | Per-token KL + PPO-clip | å¤šæ ·æœ¬è®­ç»ƒ |
| **GRPO** | `group_norm` | ç»„å½’ä¸€åŒ– | åŸºäºæ‰¹æ¬¡çš„è®­ç»ƒ |
| **Dr. GRPO** | `dr_grpo` | ç®€åŒ–çš„ GRPO | ç§»é™¤å±€éƒ¨ `/std` å½’ä¸€åŒ– |

</details>

å‚è€ƒï¼š[çŸ¥ä¹æ–‡ç« ](https://zhuanlan.zhihu.com/p/622134699) | [Notion æœ€ä½³å®è·µ](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361)

---
 

<a id="å…¨é¢ç‰¹æ€§"></a>
## ğŸ“‹ å…¨é¢ç‰¹æ€§

OpenRLHF æä¾›å®Œæ•´çš„ RLHF æµç¨‹ï¼Œå…·æœ‰åŸºäº Agent çš„çµæ´»æ€§ï¼š

### ğŸ¯ åŸºäº Agent çš„ RL è®­ç»ƒï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰

<details>
<summary>å±•å¼€åŸºäº Agent çš„ RL è®­ç»ƒç»†èŠ‚</summary>

**å•è½®æ¨¡å¼**ï¼ˆé»˜è®¤ - 99% çš„ç”¨ä¾‹ï¼‰
- æ¯ä¸ªæç¤ºå•æ¬¡ç”Ÿæˆ
- é€‚ç”¨äºæ‰€æœ‰ RL ç®—æ³•ï¼š[PPO](./examples/scripts/train_ppo_ray_hybrid_engine.sh)ã€[REINFORCE++/baseline/GRPO/RLOO](./examples/scripts/train_reinforce_baseline_hybrid_engine.sh)
- [è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°](./examples/scripts/train_ppo_with_reward_fn.sh)ï¼ˆ`--remote_rm_url`ï¼‰
- [æ··åˆå¼•æ“](./examples/scripts/train_ppo_ray_hybrid_engine.sh)ä»¥æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡

**å¤šè½®æ¨¡å¼**ï¼ˆé«˜çº§ - äº¤äº’å¼ä»»åŠ¡ï¼‰
- ä¸ç¯å¢ƒåé¦ˆçš„å¤šæ­¥äº¤äº’
- é€‚ç”¨äºæ‰€æœ‰ RL ç®—æ³•
- [è‡ªå®šä¹‰ Agent å‡½æ•°](./examples/scripts/train_reinforce_baseline_ray_agent_async.sh)ï¼ˆ`--agent_func_path`ï¼‰
- NeMo Gym é›†æˆï¼šå‚è§ `examples/python/agent_func_nemogym_executor.py`ï¼ˆé›†æˆ NeMo Gym rollout çš„ agent executor ç¤ºä¾‹ï¼‰
- å¼‚æ­¥æµæ°´çº¿ï¼ˆ`--async_train`ï¼‰æé«˜ååé‡ï¼š[train_reinforce_baseline_ray_agent_async.sh](./examples/scripts/train_reinforce_baseline_ray_agent_async.sh)

</details>

### ğŸ“ ç›‘ç£è®­ç»ƒå’Œåå¥½å­¦ä¹ 

<details>
<summary>å±•å¼€ç›‘ç£è®­ç»ƒä¸åå¥½å­¦ä¹ è¡¨</summary>

| æ–¹æ³• | è„šæœ¬ | æè¿° |
|------|------|------|
| **SFT** | [train_sft.sh](./examples/scripts/train_sft.sh) | å¸¦æ‰“åŒ…çš„ç›‘ç£å¾®è°ƒ |
| **DPO/IPO/cDPO** | [train_dpo_llama.sh](./examples/scripts/train_dpo_llama.sh) | ç›´æ¥åå¥½ä¼˜åŒ– |
| **KTO** | [train_kto_llama.sh](./examples/scripts/train_kto_llama.sh) | Kahneman-Tversky ä¼˜åŒ– |
| **è¿­ä»£ DPO** | [train_iterative_dpo.sh](./examples/scripts/train_iterative_dpo.sh) | åœ¨çº¿åå¥½å­¦ä¹  |
| **å¥–åŠ±æ¨¡å‹** | [train_rm.sh](./examples/scripts/train_rm.sh) | è®­ç»ƒå¥–åŠ±æ¨¡å‹ |
| **è¿‡ç¨‹å¥–åŠ±æ¨¡å‹** | [train_prm_mistral.sh](./examples/scripts/train_prm_mistral.sh) | é€æ­¥å¥–åŠ±æ¨¡å‹ |
| **æ‹’ç»é‡‡æ ·** | [train_rejection_sampling_llama.sh](./examples/scripts/train_rejection_sampling_llama.sh) | Best-of-N é‡‡æ · |
| **æ¡ä»¶ SFT** | [train_conditional.sh](./examples/scripts/train_conditional.sh) | è´¨é‡æ¡ä»¶è®­ç»ƒ |
| **è’¸é¦** | [train_knowledge_distillation.sh](./examples/scripts/train_knowledge_distillation.sh) | çŸ¥è¯†è¿ç§» |

</details>

### âš¡ é«˜çº§èƒ½åŠ›

<details>
<summary>å±•å¼€é«˜çº§èƒ½åŠ›</summary>

**æ•ˆç‡ä¼˜åŒ–**
- æ‰€æœ‰è®­ç»ƒæ¨¡å¼çš„æ ·æœ¬æ‰“åŒ…ï¼ˆ`--packing_samples`ï¼‰
- å¿«é€Ÿç”Ÿæˆçš„ vLLM åŠ é€Ÿï¼ˆ`--vllm_num_engines`ï¼‰
- DAPO [åŠ¨æ€è¿‡æ»¤](./examples/scripts/train_dapo_ray_hybrid_engine.sh)ï¼ˆ`--dynamic_filtering`ï¼‰
  - ğŸ² Dynamic Samplingï¼šå¯¹æ¯ä¸ª prompt ç”Ÿæˆå¤šæ¡å“åº”ï¼Œå¹¶æ ¹æ®å¥–åŠ±å‡½æ•°/Agent è¿”å›çš„ **0â€“1 `scores`** ä¿¡å·è¿›è¡Œè¿‡æ»¤
    - å¼€å¯ï¼š`--dynamic_filtering`
    - è®¾ç½®åˆ†æ•°èŒƒå›´ï¼š`--dynamic_filtering_reward_range 0.0 1.0`
    - å‰ç½®æ¡ä»¶ï¼š`--n_samples_per_prompt > 1`ï¼Œå¹¶æä¾› `--remote_rm_url`ï¼ˆå¥–åŠ±å‡½æ•°ï¼‰æˆ– `--agent_func_path`ï¼ˆAgentï¼‰
    - ç¤ºä¾‹ï¼š`./examples/scripts/train_dapo_ray_hybrid_engine.sh`

**å¯æ‰©å±•æ€§**
- å¼ é‡å¹¶è¡Œçš„ DeepSpeed AutoTPï¼ˆå‚è§è®­ç»ƒè„šæœ¬ä¸­çš„ `--ds_tensor_parallel_size`ï¼‰
- é•¿ä¸Šä¸‹æ–‡çš„ [RingAttention](./examples/test_scripts/train_dpo_ring_llama.sh)ï¼ˆ`--ring_attn_size`ï¼‰
- ä½¿ç”¨ [SLURM](./examples/scripts/train_ppo_ray_slurm.sh) çš„å¤šèŠ‚ç‚¹è®­ç»ƒ

**æ¨¡å‹æ”¯æŒ**
- [LoRA/QLoRA](./examples/scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--lora_rank`ã€`--load_in_4bit`ï¼‰
- [ä¸“å®¶æ··åˆï¼ˆMoEï¼‰](./examples/test_scripts/train_sft_moe.sh)ï¼ˆ`--aux_loss_coef`ï¼‰
- FlashAttentionï¼ˆ`--attn_implementation`ï¼‰
- HuggingFace èŠå¤©æ¨¡æ¿ï¼ˆ`--apply_chat_template`ï¼‰

**ç”Ÿäº§ç‰¹æ€§**
- Wandbï¼ˆ`--use_wandb`ï¼‰å’Œ TensorBoardï¼ˆ`--use_tensorboard`ï¼‰æ—¥å¿—
- æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆ`--load_checkpoint`ã€`--save_steps`ï¼‰
- è¯„ä¼°æ•°æ®é›†ï¼ˆ`--eval_dataset`ï¼‰

</details>

---

<a id="å¿«é€Ÿå¼€å§‹"></a>
## ğŸ¬ å¿«é€Ÿå¼€å§‹

### å®‰è£…

**æ¨è**ï¼šä½¿ç”¨ Docker ä»¥å®ç°æ— å¿§è®¾ç½®

```bash
# 1. å¯åŠ¨ Docker å®¹å™¨
docker run --runtime=nvidia -it --rm --shm-size="10g" --cap-add=SYS_ADMIN \
  -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:25.11-py3bash

# 2. æ¸…ç†å†²çªåŒ…
sudo pip uninstall xgboost transformer_engine flash_attn pynvml -y

# 3. å®‰è£… OpenRLHFï¼ˆé€‰æ‹©ä¸€ä¸ªï¼‰
pip install openrlhf                    # åŸºç¡€
pip install openrlhf[vllm]              # + vLLM 0.15.0ï¼ˆæ¨èï¼‰
pip install openrlhf[vllm_latest]       # + æœ€æ–° vLLM
pip install openrlhf[vllm,ring,liger]   # + æ‰€æœ‰ä¼˜åŒ–
```

**æ›¿ä»£æ–¹æ¡ˆï¼šä»æºç å®‰è£…**

```bash
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
```

> [!TIP]
> æˆ‘ä»¬æ¨è **vLLM 0.15.0+** ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚å‚è§ [Dockerfiles](./dockerfile/) å’Œ [Nvidia-Docker å®‰è£…è„šæœ¬](./examples/scripts/nvidia_docker_install.sh)ã€‚

### å‡†å¤‡æ•°æ®é›†

OpenRLHF æä¾›çµæ´»çš„æ•°æ®å¤„ç†æ–¹æ³•ï¼š

**å…³é”®å‚æ•°**ï¼š
- `--input_key`ï¼šæŒ‡å®šè¾“å…¥æ•°æ®çš„ JSON é”®å
- `--apply_chat_template`ï¼šä½¿ç”¨ HuggingFace tokenizer çš„[èŠå¤©æ¨¡æ¿](https://huggingface.co/docs/transformers/main/en/chat_templating)
- `--input_template`ï¼šè‡ªå®šä¹‰æ¨¡æ¿å­—ç¬¦ä¸²ï¼ˆèŠå¤©æ¨¡æ¿çš„æ›¿ä»£æ–¹æ¡ˆï¼‰
- `--prompt_data_probs` / `--dataset_probs`ï¼šæ··åˆå¤šä¸ªæ•°æ®é›†ï¼ˆä¾‹å¦‚ `0.1,0.4,0.5`ï¼‰
- `--eval_dataset`ï¼šæŒ‡å®šè¯„ä¼°æ•°æ®é›†è·¯å¾„

**èŠå¤©æ¨¡æ¿ç¤ºä¾‹**ï¼š

```python
dataset = [{"input_key": [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]}]

tokenizer.apply_chat_template(dataset[0]["input_key"], tokenize=False)
# è¾“å‡º: "<s>[INST] Hello, how are you? [/INST]I'm doing great...</s> [INST] I'd like to show off... [/INST]"
```

> [!NOTE]
> JSON é”®é€‰é¡¹å› æ•°æ®é›†ç±»å‹è€Œå¼‚ã€‚å‚è§[å¥–åŠ±æ•°æ®é›†](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10)ã€[SFT æ•°æ®é›†](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)å’Œ[æç¤ºæ•°æ®é›†](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6)

<a id="ç›‘ç£å¾®è°ƒ"></a>
### ç›‘ç£å¾®è°ƒ

OpenRLHF çš„æ¨¡å‹æ£€æŸ¥ç‚¹ä¸ HuggingFace æ¨¡å‹å®Œå…¨å…¼å®¹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `--pretrain {name or path}`ã€`--reward_pretrain {name or path}` å’Œ `--critic_pretrain {name or path}` æŒ‡å®šæ¨¡å‹åç§°æˆ–è·¯å¾„ã€‚æˆ‘ä»¬åœ¨ [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF) ä¸Šæä¾›äº†ä¸€äº›é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å’Œæ•°æ®é›†ã€‚

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨ [examples/scripts](./examples/scripts/) ç›®å½•ä¸­æä¾›çš„å¯åŠ¨è„šæœ¬ï¼Œæˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒã€‚

<details>
<summary>SFT å‘½ä»¤</summary>

```bash
deepspeed --module openrlhf.cli.train_sft \
   --max_len 4096 \
   --dataset Open-Orca/OpenOrca \
   --input_key question \
   --output_key response \
   --input_template $'User: {}\nAssistant: ' \
   --train_batch_size 256 \
   --micro_train_batch_size 2 \
   --max_samples 500000 \
   --pretrain meta-llama/Meta-Llama-3-8B \
   --save_path ./checkpoint/llama3-8b-sft \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --zero_stage 2 \
   --max_epochs 1 \
   --packing_samples \
   --param_dtype bf16 \
   --learning_rate 5e-6 \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

# é™„åŠ é€‰é¡¹ï¼š
# --apply_chat_template                # ä½¿ç”¨ HF tokenizer èŠå¤©æ¨¡æ¿
# --ring_attn_size 2                   # å¯ç”¨ RingAttentionï¼ˆå…ˆå®‰è£… ring_flash_attnï¼‰
# --multiturn                          # å¤šè½®å¾®è°ƒæŸå¤±
# --pretrain_mode                      # ç»§ç»­é¢„è®­ç»ƒæ¨¡å¼
```

</details>

### å¥–åŠ±æ¨¡å‹è®­ç»ƒ

<details>
<summary>å¥–åŠ±æ¨¡å‹è®­ç»ƒå‘½ä»¤</summary>

```bash
deepspeed --module openrlhf.cli.train_rm \
   --save_path ./checkpoint/llama3-8b-rm \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --train_batch_size 256 \
   --micro_train_batch_size 1 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --param_dtype bf16 \
   --max_epochs 1 \
   --max_len 8192 \
   --zero_stage 3 \
   --learning_rate 9e-6 \
   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
   --apply_chat_template \
   --chosen_key chosen \
   --rejected_key rejected \
   --packing_samples \
   --gradient_checkpointing \
   --use_wandb {wandb_token}
```

</details>

å»ºè®®å°†å¥–åŠ±æ¨¡å‹çš„ `--value_prefix_head` é€‰é¡¹è®¾ç½®ä¸º `score`ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `AutoModelForSequenceClassification` åŠ è½½æ¨¡å‹ï¼š

```python
reward_model = AutoModelForSequenceClassification.from_pretrained(
              reward_model_path,
              num_labels=1,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              use_cache=False,
          )
inputs = xxxx (å·¦å¡«å……è¾“å…¥ Tokens)
reward = reward_model.model(*inputs).last_hidden_state
reward = reward_model.score(reward)[:, -1]
```

### RL è®­ç»ƒï¼šä½¿ç”¨ Ray å’Œ vLLM çš„ PPO/REINFORCE++

OpenRLHF ä¸­çš„æ‰€æœ‰ RL è®­ç»ƒéƒ½é€šè¿‡ **Agent æ‰§è¡Œæµç¨‹**è¿è¡Œã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†ä½¿ç”¨æ··åˆå¼•æ“çš„å•è½® Agent æ‰§è¡Œï¼ˆé»˜è®¤æ¨¡å¼ï¼‰ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼š

```bash
# åœ¨å®¹å™¨ä¸­å¯åŠ¨ ray çš„ä¸»èŠ‚ç‚¹
ray start --head --node-ip-address 0.0.0.0 --num-gpus 8

# å¦‚æœè¦åœ¨æ›´å¤šèŠ‚ç‚¹ä¸Šå¯åŠ¨ rayï¼Œä½¿ç”¨
ray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8

ray job submit --address="http://127.0.0.1:8265" \
   --runtime-env-json='{"working_dir": "/openrlhf"}' \
   -- python3 -m openrlhf.cli.train_ppo_ray \
   --ref_num_nodes 1 \
   --ref_num_gpus_per_node 8 \
   --reward_num_nodes 1 \
   --reward_num_gpus_per_node 8 \
   --critic_num_nodes 1 \
   --critic_num_gpus_per_node 8 \
   --actor_num_nodes 1 \
   --actor_num_gpus_per_node 8 \
   --vllm_num_engines 4 \
   --vllm_tensor_parallel_size 2 \
   --colocate_all_models \
   --vllm_gpu_memory_utilization 0.5 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --reward_pretrain OpenRLHF/Llama-3-8b-rm-700k \
   --save_path /openrlhf/examples/test_scripts/final/llama3-8b-rlhf \
   --ckpt_path /openrlhf/examples/test_scripts/ckpt/llama3-8b-rlhf \
   --save_hf_ckpt \
   --train_batch_size 128 \
   --rollout_batch_size 1024 \
   --use_dynamic_batch \
   --n_samples_per_prompt 1 \
   --max_epochs 1 \
   --prompt_max_len 1024 \
   --max_samples 100000 \
   --generate_max_len 1024 \
   --zero_stage 3 \
   --param_dtype bf16 \
   --actor_learning_rate 5e-7 \
   --critic_learning_rate 9e-6 \
   --init_kl_coef 0.01 \
   --prompt_data OpenRLHF/prompt-collection-v0.1 \
   --input_key context_messages \
   --apply_chat_template \
   --normalize_reward \
   --gradient_checkpointing \
   --packing_samples \
   --vllm_sync_backend nccl \
   --enforce_eager \
   --vllm_enable_sleep \
   --deepspeed_enable_sleep \
   --use_wandb {wandb_token}

# ç®—æ³•å˜ä½“ï¼ˆæ‰€æœ‰ç®—æ³•éƒ½ä½¿ç”¨å•è½® Agent æ‰§è¡Œï¼‰ï¼š
# --advantage_estimator reinforce        # REINFORCE++
# --advantage_estimator rloo             # RLOO
# --advantage_estimator reinforce_baseline  # REINFORCE++-baselineï¼ˆRLVR æœ€ä½³ï¼‰
# --advantage_estimator group_norm       # GRPO
# --advantage_estimator dr_grpo          # Dr. GRPO

# é«˜çº§é€‰é¡¹ï¼š
# --init_kl_coef 0                      # æ— å‚è€ƒæ¨¡å‹
# --remote_rm_url http://host:5000/get_reward  # HTTP å¥–åŠ±æ¨¡å‹
# --n_samples_per_prompt 4              # æ¯ä¸ªæç¤ºå¤šä¸ªæ ·æœ¬
# --enable_vllm_is_correction           # TISï¼ˆvLLM é‡è¦æ€§é‡‡æ ·ä¿®æ­£ï¼‰ï¼šç”¨äº off-policy rolloutï¼ˆä»… PPO ç”Ÿæ•ˆï¼‰
# --vllm_is_truncated_threshold 0.5 5.0 # TIS æˆªæ–­åŒºé—´ï¼š[low, high]
# --use_icepop                          # ICEPOPï¼šå°†åŒºé—´å¤–ç³»æ•°ç½® 0ï¼ˆè€Œä¸æ˜¯ clampï¼‰
```

> [!TIP]
> **å¯¹äºæ¨ç†ä»»åŠ¡ï¼ˆRLVRï¼‰**ï¼šä½¿ç”¨ `--advantage_estimator reinforce_baseline` ç”¨äº REINFORCE++-baselineâ€”â€”å®ƒå¯¹ä¸åŒçš„å¥–åŠ±å°ºåº¦å…·æœ‰é²æ£’æ€§ã€‚

> [!NOTE]
> **Ray ç¯å¢ƒè®¾ç½®**ï¼šè®© Ray ä½¿ç”¨ `--runtime-env-json='{"setup_commands": ["pip install openrlhf[vllm]"]}'` è‡ªåŠ¨éƒ¨ç½²

> [!NOTE]
> **GPU ç´¢å¼•é”™è¯¯æ•…éšœæ’é™¤**ï¼šå¦‚æœé‡åˆ° DeepSpeed GPU è®¾å¤‡è®¾ç½®é—®é¢˜ï¼Œè¯·è®¾ç½® `export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1`ã€‚

ğŸ“š **æ›´å¤šç¤ºä¾‹**ï¼šå‚è§ [example/scripts](./examples/scripts/) å’Œ[æ–‡æ¡£](https://openrlhf.readthedocs.io/en/latest/usage.html)

---

<a id="å•è½®-agentå¼ºåŒ–å¾®è°ƒä¸è‡ªå®šä¹‰å¥–åŠ±"></a>
## ğŸ¯ å•è½® Agentï¼šå¼ºåŒ–å¾®è°ƒä¸è‡ªå®šä¹‰å¥–åŠ±

**å•è½® Agent æ‰§è¡Œ**ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰æ”¯æŒè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°â€”â€”éå¸¸é€‚åˆæ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„å¼ºåŒ–å¾®è°ƒã€‚æ‚¨å¯ä»¥æä¾›ä¸€ä¸ª Python å‡½æ•°æ¥å³æ—¶è®¡ç®—å¥–åŠ±ï¼Œè€Œä¸æ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚

**é€‚ç”¨äº**ï¼š
- åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆé•¿åº¦ã€æ ¼å¼ã€ä»£ç æ‰§è¡Œã€æ•°å­¦éªŒè¯ï¼‰
- å¤–éƒ¨ API å¥–åŠ±ï¼ˆè¯„åˆ¤æ¨¡å‹ã€ç¼–è¯‘å™¨ã€æµ‹è¯•å¥—ä»¶ï¼‰
- æ··åˆå¥–åŠ±ï¼ˆç»„åˆå¤šä¸ªä¿¡å·ï¼‰

### ç¤ºä¾‹ï¼šè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

```python
# reward_func.py
import torch

def reward_func(queries, prompts, labels):
    """
    è®¡ç®—ç”Ÿæˆå“åº”çš„è‡ªå®šä¹‰å¥–åŠ±ã€‚
    
    å‚æ•°ï¼š
        queries: List[str] - å®Œæ•´æ–‡æœ¬ï¼ˆæç¤º + å“åº”ï¼‰
        prompts: List[str] - ä»…åŸå§‹æç¤º
        labels: List[str] - çœŸå®æ ‡ç­¾ï¼ˆæ¥è‡ª --label_keyï¼‰
    
    è¿”å›ï¼š
        åŒ…å«ä»¥ä¸‹å†…å®¹çš„å­—å…¸ï¼š
            - rewards: ç”¨äºä¼˜åŠ¿è®¡ç®—çš„å¼ é‡
            - scores: ç”¨äºåŠ¨æ€è¿‡æ»¤çš„å¼ é‡ï¼ˆ0-1 èŒƒå›´ï¼‰
            - extra_logs: ç”¨äº wandb æ—¥å¿—çš„å­—å…¸
    """
    batch_size = len(queries)
    
    # ç¤ºä¾‹ï¼šéšæœºå¥–åŠ±ï¼ˆæ›¿æ¢ä¸ºæ‚¨çš„é€»è¾‘ï¼‰
    # çœŸå®ç¤ºä¾‹ï¼šä»£ç æ‰§è¡Œã€æ•°å­¦éªŒè¯ã€æ ¼å¼æ£€æŸ¥
    reward = torch.randint(0, 2, (batch_size,)).float()
    
    return {
        "rewards": reward,           # ç”¨äº RL ä¼˜åŠ¿è®¡ç®—
        "scores": reward,            # ç”¨äºåŠ¨æ€è¿‡æ»¤ï¼ˆ--dynamic_filteringï¼‰
        "extra_logs": {              # è®°å½•åˆ° wandb
            "custom_metric": reward.mean().item(),
        },
    }
```

### ä½¿ç”¨æ–¹æ³•

```bash
ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  --pretrain meta-llama/Meta-Llama-3-8B \
  --use_dynamic_batch \
  --remote_rm_url /path/to/reward_func.py \
  --label_key answer \
  --prompt_data your_prompt_dataset \
  ... # å…¶ä»–è®­ç»ƒå‚æ•°
```

**å…³é”®å‚æ•°**ï¼š`--label_key answer` å°†æ•°æ®é›†ä¸­çš„"answer"å­—æ®µä¼ é€’ç»™ `reward_func` ä½œä¸º `labels`ã€‚

> [!TIP]
> **ä½¿ç”¨æ¡ˆä¾‹**ï¼šä»£ç ç”Ÿæˆï¼ˆæ‰§è¡Œæµ‹è¯•ï¼‰ã€æ•°å­¦ï¼ˆéªŒè¯è§£å†³æ–¹æ¡ˆï¼‰ã€æ ¼å¼åŒ–ï¼ˆæ£€æŸ¥ç»“æ„ï¼‰ã€å¤šç›®æ ‡ï¼ˆç»„åˆå¤šä¸ªä¿¡å·ï¼‰

ğŸ“– **å®Œæ•´ç¤ºä¾‹**ï¼š[examples/scripts/train_ppo_with_reward_fn.sh](./examples/scripts/train_ppo_with_reward_fn.sh)

---

<a id="å¤šè½®-agentå¤æ‚ç¯å¢ƒäº¤äº’"></a>
## ğŸ¤– å¤šè½® Agentï¼šå¤æ‚ç¯å¢ƒäº¤äº’

å¯¹äºéœ€è¦**å¤šæ­¥äº¤äº’**ï¼ˆæ¨ç†é“¾ã€å¸¦åé¦ˆçš„ç¼–ç ã€æ¸¸æˆï¼‰çš„ä»»åŠ¡ï¼ŒOpenRLHF æä¾›**å¤šè½® Agent æ‰§è¡Œ**æ¨¡å¼ã€‚

### æ„å»ºè‡ªå®šä¹‰å¤šè½® Agent

ä½¿ç”¨ `reset/step` æ–¹æ³•å®ç° `AgentInstanceBase`ï¼š

```python
# agent_func.py
import random
from typing import Any, Dict

import torch
from openrlhf.utils.agent import AgentInstanceBase, MultiTurnAgentExecutor


# ä¸€ä¸ªç®€å•çš„ n æ­¥éšæœºç¯å¢ƒ
class AgentInstance(AgentInstanceBase):
    async def __init__(self, *args, **kwargs):
        self.step_idx = 0
        self.max_steps = random.randint(1, 3)  # 1-3 æ­¥

    async def reset(self, states: dict, **kwargs):
        return {"observation": states["observation"]}  # è¿”å›åŸå§‹æ–‡æœ¬è§‚å¯Ÿ

    async def step(self, states: dict, **kwargs) -> Dict[str, Any]:
        print(f"step_idx: {self.step_idx}, max_steps: {self.max_steps}")

        observation_text = states["observation_text"]
        action_text = states["action_text"]
        label = states["label"]

        # æ£€æŸ¥å›åˆæ˜¯å¦ç»“æŸ
        done = self.step_idx >= self.max_steps
        reward = torch.randint(0, 2, (1,)).float() if done else torch.tensor(0)

        # æ ¹æ®å›åˆæ˜¯å¦ç»“æŸç”Ÿæˆç¯å¢ƒåé¦ˆ
        environment_feedback = (
            "\n\nHuman: [CORRECT]\n</s>"
            if done
            else "\n\nHuman: [INCORRECT]\nPlease analyze the issues and try again.\n</s>\n\nAssistant: "
        )

        self.step_idx += 1

        return {
            "rewards": reward,  # ç”¨äºä¼˜åŠ¿è®¡ç®—çš„å¥–åŠ±
            "scores": reward,  # ç”¨äºåŠ¨æ€è¿‡æ»¤çš„åˆ†æ•°ï¼ˆ0-1 å¥–åŠ±ï¼‰
            "environment_feedback": environment_feedback,  # ç¯å¢ƒåé¦ˆæ–‡æœ¬
            "done": done,  # æŒ‡ç¤ºå›åˆæ˜¯å¦å®Œæˆçš„å¸ƒå°”å€¼
            "sampling_params": states.get("sampling_params", None),  # ä¸‹ä¸€æ­¥ vLLM é‡‡æ ·å‚æ•°
            "extra_logs": {"dummy_scores": reward},  # é¢å¤–çš„æ—¥å¿—ä¿¡æ¯
        }


class AgentExecutor(MultiTurnAgentExecutor):
    def __init__(self):
        super().__init__(AgentInstance)
```

ç„¶åå¯åŠ¨ï¼š

```bash
ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  ...
  --use_dynamic_batch \
  --agent_func_path /path/to/agent_func.py \
  --async_train  # å¯é€‰ï¼šå¯ç”¨å¼‚æ­¥æµæ°´çº¿
```

### é…ç½®é€‰é¡¹

**å¼‚æ­¥æµæ°´çº¿**ï¼ˆæé«˜ååé‡ï¼‰ï¼š
- å¯ç”¨ï¼š`--async_train`
- ç¼“å†²åŒºå¤§å°ï¼š`--async_queue_size 1`ï¼ˆè¶Šå¤§ = è¶Šå¤š off-policyï¼Œé»˜è®¤ 1ï¼‰

**è®­ç»ƒæ¨¡å¼**ï¼š
- **åŒæ­¥**ï¼šé»˜è®¤ï¼Œæ›´å¥½çš„ç¨³å®šæ€§
- **å¼‚æ­¥**ï¼šæ›´é«˜ååé‡ï¼Œå¯èƒ½å½±å“æ”¶æ•›
- **æ··åˆå¼•æ“**ï¼šä½¿ç”¨ `--colocate_all_models` å®ç°æœ€ä½³ GPU åˆ©ç”¨ç‡ï¼ˆç§»é™¤ `--async_train`ï¼‰

> [!NOTE]
> å¯¹äºå®Œå…¨è‡ªå®šä¹‰çš„ token çº§æ‰§è¡Œï¼Œç»§æ‰¿ `AgentExecutorBase` å¹¶å®ç° `execute()`ã€‚æ­¤è®¾è®¡å¼ºåˆ¶æ‰§è¡Œ **token-in-token-out åŸåˆ™**ä»¥ä¿æŒé‡‡æ ·å’Œè®­ç»ƒä¸€è‡´ã€‚

> [!WARNING] 
> å¼‚æ­¥è®­ç»ƒå¯èƒ½ä¼šå½±å“è®­ç»ƒç¨³å®šæ€§ã€‚ä»…åœ¨ååé‡è‡³å…³é‡è¦ä¸”æ”¶æ•›å·²éªŒè¯æ—¶ä½¿ç”¨ã€‚

ğŸ“š **ç¤ºä¾‹**ï¼š
- å•è½®ï¼š[train_ppo_ray_hybrid_engine.sh](./examples/scripts/train_ppo_ray_hybrid_engine.sh)
- è‡ªå®šä¹‰å¥–åŠ±ï¼š[train_ppo_with_reward_fn.sh](./examples/scripts/train_ppo_with_reward_fn.sh)
- å¤šè½®ï¼š[train_reinforce_baseline_ray_agent_async.sh](./examples/scripts/train_reinforce_baseline_ray_agent_async.sh)
- NeMo Gymï¼š`examples/python/agent_func_nemogym_executor.py`

---

<a id="é«˜çº§ä¸»é¢˜"></a>
## ğŸ”§ é«˜çº§ä¸»é¢˜

### LoRAï¼šåˆå¹¶é€‚é…å™¨

ä½¿ç”¨ LoRA/QLoRA æ—¶ï¼ŒOpenRLHF ä»…ä¿å­˜é€‚é…å™¨æƒé‡ã€‚è¦éƒ¨ç½²æˆ–ç»§ç»­è®­ç»ƒï¼Œè¯·å°†é€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ï¼š

```bash
python -m openrlhf.cli.lora_combiner \
    --model_path meta-llama/Meta-Llama-3-8B \
    --lora_path ./checkpoint/llama3-8b-rm \
    --output_path ./checkpoint/llama-3-8b-rm-combined \
    --is_rm \
    --param_dtype bf16
```

### æ€§èƒ½è°ƒä¼˜æŒ‡å—

é€šè¿‡ä»¥ä¸‹å»ºè®®é’ˆå¯¹æ‚¨çš„ç¡¬ä»¶å’Œå·¥ä½œè´Ÿè½½ä¼˜åŒ– OpenRLHFï¼š

#### ğŸ¯ èµ„æºåˆ†é…ï¼ˆåˆ†å¸ƒå¼æ¨¡å¼ï¼‰

**æ¨èæ¯”ä¾‹**ï¼š`vLLM : Actor : Critic = 1:1:1`

```bash
# ç¤ºä¾‹ï¼š48 ä¸ª A100 GPU ä¸Šçš„ 70B æ¨¡å‹
# - 16 ä¸ª GPU â†’ vLLM å¼•æ“
# - 16 ä¸ª GPU â†’ Actor
# - 16 ä¸ª GPU â†’ Critic
```

#### âš¡ é€Ÿåº¦ä¼˜åŒ–

| ä¼˜åŒ– | æ ‡å¿— | ä½•æ—¶ä½¿ç”¨ |
|------|------|---------|
| **æ··åˆå¼•æ“** | `--colocate_all_models`<br>`--vllm_enable_sleep`<br>`--deepspeed_enable_sleep` | GPU å†…å­˜å……è¶³ |
| **å¼‚æ­¥è®­ç»ƒ** | `--async_train` | æ”¶æ•›å·²éªŒè¯ï¼Œéœ€è¦ååé‡ |
| **æ ·æœ¬æ‰“åŒ…** | `--packing_samples` | å§‹ç»ˆï¼ˆå°¤å…¶æ˜¯è®­ç»ƒï¼‰ |
| **DeepCompile** | `--deepcompile` | PyTorch 2.0+ |
| **é‡å é€šä¿¡** | `--overlap_comm` | GPU å†…å­˜å……è¶³ |
| **åŠ¨æ€æ‰¹æ¬¡** | `--use_dynamic_batch` | å¯å˜åºåˆ—é•¿åº¦ |
| **å‰ç¼€ç¼“å­˜** | vLLM é…ç½® | `n_samples_per_prompt` > 1 |

#### ğŸ’¾ å†…å­˜ç®¡ç†

**å½“æ‚¨æœ‰è¶³å¤Ÿå†…å­˜æ—¶**ï¼š
- âœ… ç¦ç”¨ `--adam_offload`
- âœ… å¯ç”¨ `--overlap_comm`
- âœ… ä½¿ç”¨ `--colocate_critic_reward` å’Œ `--colocate_actor_ref`

**é‡åˆ° OOM æ—¶**ï¼š
- âŒ ç¦ç”¨æ‰€æœ‰ `--colocate_*` é€‰é¡¹
- âœ… å‡å°‘æ‰¹æ¬¡å¤§å°
- âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

#### ğŸ® æ‰¹æ¬¡å¤§å°è°ƒä¼˜

1. **ç”Ÿæˆé˜¶æ®µ**ï¼šæœ€å¤§åŒ– `--micro_rollout_batch_size`ï¼Œæœ€å°åŒ– vLLM TP å¤§å°
2. **è®­ç»ƒé˜¶æ®µ**ï¼šæœ€å¤§åŒ– `--micro_train_batch_size`ï¼Œå¯ç”¨ `--packing_samples`
3. **vLLM**ï¼šå§‹ç»ˆä½¿ç”¨ `--vllm_sync_backend nccl`

> [!TIP]
> **å¿«é€Ÿå¼€å§‹æ¨¡æ¿**ï¼šå¯¹äº 8x A100ï¼ˆ80GBï¼‰ï¼Œå°è¯•æ··åˆå¼•æ“ + `--vllm_gpu_memory_utilization 0.5` + `--colocate_all_models`

ğŸ“– **æ›´å¤šè¯¦æƒ…**ï¼š[æ€§èƒ½è°ƒä¼˜æ–‡æ¡£](https://openrlhf.readthedocs.io/en/latest/performance.html)

## ä½¿ç”¨ OpenRLHF çš„å…¬å¸å’Œç»„ç»‡

- Google
- ByteDance
- Tencent
- Alibaba
- Baidu
- China Telecom
- Vivo
- Allen AI
- NexusFlow
- JÃ¼lich Supercomputing Centre (JSC)
- Berkeley Starling Team
- M-A-P
- ...

## åŠ å…¥æˆ‘ä»¬

**å¦‚ä½•åŠ å…¥ï¼Ÿ**

1. å‘é€ç”µå­é‚®ä»¶è‡³ janhu9527@gmail.com æˆ–åŠ å…¥ [GitHub Organization](https://github.com/OpenRLHF)ã€‚è¯·åŒ…å«ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ï¼š
   - æ‚¨çš„å§“å
   - æ‚¨çš„ GitHub ç”¨æˆ·å
   - æ‚¨æ„Ÿå…´è¶£çš„é¢†åŸŸ
   - æ‚¨ä¸ NLP å’Œ/æˆ– AI ç›¸å…³çš„æŠ€èƒ½å’Œç»éªŒ
2. æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡å®˜æ–¹ GitHub [OpenRLHF â†—](https://github.com/OpenRLHF/OpenRLHF) é¡¹ç›®é¡µé¢åŠ å…¥æˆ‘ä»¬ã€‚åªéœ€åˆ›å»ºä¸€ä¸ªå…³äºæ‚¨è´¡çŒ®å…´è¶£çš„ issueï¼Œæˆ‘ä»¬ä¼šå›å¤æ‚¨ã€‚

**æ‚¨å¯ä»¥åšä»€ä¹ˆï¼Ÿ**

1. åŠ å…¥å›¢é˜Ÿå¹¶å‚ä¸ OpenRLHF é¡¹ç›®çš„å¼€å‘ã€‚
2. é€šè¿‡æäº¤ pull request ä¸ºé¡¹ç›®åšå‡ºè´¡çŒ®ã€‚
3. å¸®åŠ©æ”¹è¿›æ–‡æ¡£ã€ä¿®å¤é”™è¯¯æˆ–åˆ›å»ºæ–°åŠŸèƒ½ã€‚
4. åˆ†äº«é¡¹ç›®å¹¶å¸®åŠ©æˆ‘ä»¬å‘å±•ç¤¾åŒºã€‚

## èµåŠ©æˆ‘ä»¬

æ‚¨çš„èµåŠ©å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç»´æŠ¤å’Œæ”¹è¿› OpenRLHFã€‚å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰ç”¨ï¼Œè¯·è€ƒè™‘èµåŠ©æˆ‘ä»¬ã€‚æ‚¨å¯ä»¥åœ¨ [Open Collective â†—](https://opencollective.com/OpenRLHF) ä¸ŠèµåŠ©æˆ‘ä»¬ã€‚

## Star å†å²

[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)

## è´¡çŒ®è€…

éå¸¸æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…ï¼å¦‚æœæ‚¨æƒ³è´¡çŒ®ï¼Œè¯·éšæ—¶æäº¤ pull request æˆ–åˆ›å»º issueã€‚

<a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF" />
</a>

## å‚è€ƒæ–‡çŒ®ä¸è‡´è°¢

æˆ‘ä»¬è¦æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®å’Œç»„ç»‡å¯¹ AI å’Œ NLP é¢†åŸŸçš„è´¡çŒ®ï¼š

- [Hugging Face Transformers â†—](https://github.com/huggingface/transformers)
- [OpenAI GPT â†—](https://github.com/openai/gpt-3)
- [LLaMA â†—](https://llama.meta.com/)
- [DeepSpeed â†—](https://github.com/microsoft/DeepSpeed)
- [Ray â†—](https://github.com/ray-project/ray)

æˆ‘ä»¬çš„é¡¹ç›®è¿˜è¦æ„Ÿè°¢ [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalChat) å’Œ [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)ã€‚åœ¨é¡¹ç›®æ—©æœŸï¼Œæˆ‘ä»¬å‚è€ƒäº†ä»–ä»¬çš„ä»£ç è®¾è®¡ã€‚æˆ‘ä»¬çš„é¡¹ç›®è¦æ„Ÿè°¢ [Netmind.AI](https://www.netmind.ai/) ä¸ºå¼€å‘ ring attention æä¾›çš„ GPU æ”¯æŒã€‚

ï¼ˆ2024/7ï¼‰æˆ‘ä»¬çš„ GitHub ç»„ç»‡å·²ä» OpenLLMAI æ›´æ”¹ä¸º OpenRLHFã€‚

## å¼•ç”¨

OpenRLHF
```
@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}
```

REINFORCE++-baseline
```
@article{hu2025reinforce++,
  title={Reinforce++: A simple and efficient approach for aligning large language models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}
```

______________________________________________________________________

*OpenRLHF Â© 2025 OpenRLHF. All Rights Reserved.*
