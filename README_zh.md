<div align="center">
<p align="center">
<img alt="" src="./docs/logo.png" style="display: inline-block; height: 140px" />
</p>
</div>

<div align="center">
<p align="center">
      <a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/discussions">
        <img alt="Issues" src="https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff" />
      <a href="https://github.com/OpenRLHF/OpenRLHF/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf" />
      </a>
      <a href="https://deepwiki.com/OpenRLHF/OpenRLHF"><img src="https://deepwiki.com/badge.svg" alt="Ask DeepWiki"></a>
      <br>
      <em>å¼€æº / å…¨é¢ / è½»é‡çº§ / æ˜“ç”¨</em>
    </p>
</p>
</div>

<hr>

<span>[ <a href="README.md">English</a> | ä¸­æ–‡ | <a href="README_ja.md">æ—¥æœ¬èª</a> ]</span>

OpenRLHF æ˜¯ç¬¬ä¸€ä¸ªåŸºäº Rayã€vLLMã€ZeRO-3 å’Œ HuggingFace Transformers æ„å»ºçš„å¼€æºé«˜æ€§èƒ½ RLHF æ¡†æ¶ï¼Œæ—¨åœ¨è®© RLHF è®­ç»ƒå˜å¾—ç®€å•æ˜“ç”¨ï¼š

- **åŸºäº Ray çš„åˆ†å¸ƒå¼æ¶æ„**  
  OpenRLHF åˆ©ç”¨ [Ray](https://github.com/ray-project/ray) å®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼è°ƒåº¦ã€‚å®ƒå°† Actorã€Rewardã€Reference å’Œ Critic æ¨¡å‹åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸Šï¼Œæ”¯æŒé«˜è¾¾ 70B å‚æ•°æ¨¡å‹çš„è®­ç»ƒã€‚  
  å®ƒè¿˜æ”¯æŒ **Hybrid Engine** è°ƒåº¦ï¼Œå…è®¸æ‰€æœ‰æ¨¡å‹å’Œ vLLM å¼•æ“å…±äº« GPU èµ„æºï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘ç©ºé—²æ—¶é—´å¹¶æé«˜ GPU åˆ©ç”¨ç‡ã€‚
- **vLLM æ¨ç†åŠ é€Ÿ + AutoTP**  
  RLHF è®­ç»ƒä¸­ 80% çš„æ—¶é—´éƒ½èŠ±åœ¨æ ·æœ¬ç”Ÿæˆé˜¶æ®µã€‚åŸºäº [vLLM](https://github.com/vllm-project/vllm) å’Œè‡ªåŠ¨å¼ é‡å¹¶è¡Œ (AutoTP)ï¼ŒOpenRLHF æä¾›é«˜ååé‡ã€å†…å­˜é«˜æ•ˆçš„æ¨ç†ã€‚ä¸ HuggingFace Transformers çš„åŸç”Ÿé›†æˆç¡®ä¿äº†æ— ç¼ä¸”å¿«é€Ÿçš„ç”Ÿæˆï¼Œä½¿å…¶æˆä¸ºç›®å‰æœ€å¿«çš„ RLHF æ¡†æ¶ã€‚
- **åŸºäº ZeRO-3 / AuoTP çš„å†…å­˜é«˜æ•ˆè®­ç»ƒ**  
  åŸºäº [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) çš„ ZeRO-3, [deepcompile](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md) ä»¥åŠ [AutoTP](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md)ï¼ŒOpenRLHF æ— éœ€é‡é‡çº§æ¡†æ¶å³å¯å®ç°å¤§æ¨¡å‹è®­ç»ƒã€‚å®ƒç›´æ¥ä¸ HuggingFace é…åˆä½¿ç”¨ï¼Œæ–¹ä¾¿åŠ è½½å’Œå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚
- **ä¼˜åŒ–çš„ PPO å®ç°**  
  é›†æˆäº†å—å®è·µæŒ‡å—å’Œç¤¾åŒºæœ€ä½³å®è·µå¯å‘çš„å…ˆè¿› PPO æŠ€å·§ï¼Œæé«˜äº† RLHF å·¥ä½œæµç¨‹ä¸­çš„è®­ç»ƒç¨³å®šæ€§å’Œå¥–åŠ±è´¨é‡ã€‚å‚è€ƒ [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/622134699) å’Œ [Advanced Tricks for Training Large Language Models with Proximal Policy Optimization](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361)ã€‚

æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [PPT](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [æŠ€æœ¯æŠ¥å‘Š](https://www.researchgate.net/publication/393414548_OpenRLHF_An_Easy-to-use_Scalable_and_High-performance_RLHF_Framework) | [ä½¿ç”¨æ–‡æ¡£](https://openrlhf.readthedocs.io/)


## æ–°é—»  
- [2025/11] [NeMo Gym](https://github.com/NVIDIA-NeMo/Gym) OpenRLHF ç°å·²æ”¯æŒä¸ NeMo-Gym é›†æˆï¼Œç”¨äºåŸºäºå¤–éƒ¨è¯„ä¼°ç¯å¢ƒçš„é«˜çº§æ™ºèƒ½ä½“ RLHF è®­ç»ƒã€‚
- [2025/10] [ScaleRL](https://arxiv.org/abs/2510.13786) åœ¨å¤§è§„æ¨¡è®­ç»ƒåœºæ™¯ä¸­éªŒè¯äº† REINFORCE++-baseline çš„æœ‰æ•ˆæ€§ã€‚å‘å¸ƒ [REINFORCE++ slides](https://docs.google.com/presentation/d/1stieP_3PM1z4Hq1YWR3GywFkxcHEAlstXMaS23KlGN4)
- [2025/8] [ProRL V2](https://hijkzzz.notion.site/prorl-v2) ä½¿ç”¨ REINFORCE++-baseline è®­ç»ƒæœ€å…ˆè¿›çš„ 1.5B æ¨ç†æ¨¡å‹ï¼Œå¹¶å‘å¸ƒåšå®¢æ–‡ç«  [REINFORCE++-baseline is all you need in RLVR](https://medium.com/@janhu9527/reinforce-baseline-is-all-you-need-in-rlvr-f5406930aa85)ã€‚
- [2025/6] [Magistral](https://mistral.ai/static/research/magistral.pdf) ä½¿ç”¨æå…¶ç±»ä¼¼äº REINFORCE++-baseline çš„ç®—æ³•è®­ç»ƒæ¨ç†æ¨¡å‹.
- [2025/5] [MARTI](https://github.com/TsinghuaC3I/MARTI) ä½œä¸º OpenRLHF çš„åˆ†æ”¯ç‰ˆæœ¬å·²å‘å¸ƒã€‚å®ƒé€šè¿‡é›†æˆé›†ä¸­å¼å¤šæ™ºèƒ½ä½“äº¤äº’ä¸åˆ†å¸ƒå¼ç­–ç•¥è®­ç»ƒï¼Œä¸“ä¸ºä½¿ç”¨ RL è®­ç»ƒåŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè€Œè®¾è®¡ã€‚
- [2025/5] OpenRLHF 0.8.0 æ”¯æŒ [Async Pipeline RLHF](./examples/scripts/train_reinforce_baseline_llama_ray_async.sh) (`--async_train`) å’Œ [Async Agent RLHF](./examples/scripts/train_reinforce_baseline_llama_ray_agent_async.sh)(`--agent_func_path`) ä»¥åŠé‡æ–°è®¾è®¡çš„åŸºäºç±»çš„ä»£ç† API
- [2025/4] å‘å¸ƒåšå®¢ [Accelerating RLHF with vLLM, Best Practice from OpenRLHF](https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html)
- [2025/4] Clean OpenRLHF: åŸºäº Single Controller å’Œ Unified Packing Samples é‡æ„äº†æºç 
- [2025/3] CMUçš„[2025æ˜¥å­£é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†è¯¾ç¨‹](https://cmu-l3.github.io/anlp-spring2025/)ä½¿ç”¨OpenRLHFä½œä¸ºRLHFæ¡†æ¶æ•™å­¦æ¡ˆä¾‹ã€‚
- [2025/2] [Logic-RL](https://arxiv.org/abs/2502.14768) å’Œ [PRIME](https://arxiv.org/abs/2502.01456) å±•ç¤ºäº† REINFORCE++ åœ¨è®­ç»ƒç¨³å®šæ€§ä¸Šä¼˜äº GRPO å¹¶ä¸”æ¯” PPO æ›´å¿«ã€‚
- [2025/2] [LMM-R1](https://github.com/TideDra/lmm-r1) æ˜¯ OpenRLHF çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨ä¸ºå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå¤ç° DeepSeek-R1 æä¾›é«˜æ€§èƒ½çš„ RL åŸºç¡€è®¾æ–½ã€‚
- [2025/2] MIT & Microsoft æå‡ºäº† [On the Emergence of Thinking in LLMs I: Searching for the Right Intuition](https://arxiv.org/pdf/2502.06773) åŸºäº OpenRLHF
- [2025/1] æ¸¯ç§‘å¤§å¤ç°äº† [DeepSeek-R1-Zero and DeepSeek-R1 training on small models ä½¿ç”¨ OpenRLHF](https://github.com/hkust-nlp/simpleRL-reason)
- [2024/12] æˆ‘ä»¬"æå‡º"äº† ğŸ˜Š [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://www.researchgate.net/publication/387487679_REINFORCE_An_Efficient_RLHF_Algorithm_with_Robustnessto_Both_Prompt_and_Reward_Models).
- [2024/12] åœ¨ [Notion Blog](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05) ä¸­ï¼Œæˆ‘ä»¬å¯¹ PPOã€REINFORCE++ã€GRPO å’Œ RLOO è¿›è¡Œäº†åˆ†æã€‚  
- [2023/8] OpenRLHF å¼€å¯å¼€æºä¹‹æ—…. 

## ç‰¹æ€§  

- åŸºäº Ray çš„åˆ†å¸ƒå¼ [PPO](./examples/scripts/train_ppo_llama_ray.sh) å’Œ [REINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray.sh) å®ç°ã€‚  
- æ”¯æŒå¯¹ [è¶…è¿‡ 700 äº¿å‚æ•°çš„æ¨¡å‹](./examples/scripts/train_ppo_llama_ray_70b.sh) è¿›è¡Œå®Œæ•´çš„ RLHF å¾®è°ƒã€‚  
- æ”¯æŒåŸºäº Ray å’Œ Hybrid Engine çš„ [PPO](./examples/scripts/train_ppo_llama_ray_hybrid_engine.sh) å’Œ [REINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh) (`--colocate_all_models`, `--vllm_enable_sleep` and `--vllm_gpu_memory_utilization 0.5`)
- [Ray-based Reinforced Finetuning](./examples/scripts/train_ppo_llama_with_reward_fn.sh)
- é›†æˆ [NeMo Gym](./examples/scripts/train_reinforce_nemogym.sh)ï¼Œæ”¯æŒåŸºäºå¤–éƒ¨è¯„ä¼°ç¯å¢ƒçš„æ™ºèƒ½ä½“ RLHFï¼ˆé€šè¿‡ `--agent_func_path` é…åˆ NeMo Gym é›†æˆï¼‰
- æ”¯æŒ Dynamic Sampling from DAPO(`--dynamic_filtering` and `--dynamic_filtering_reward_range`)
- æ”¯æŒ [DeepSpeed AutoTP è®­ç»ƒ](./examples/scripts/train_sft_llama_tensor_parallelism.sh) (`--ds_tensor_parallel_size`)
- é›†æˆ vLLMï¼ŒåŠ é€Ÿ RLHF ä»»åŠ¡ä¸­çš„æ ·æœ¬ç”Ÿæˆï¼ˆ`--vllm_num_engines`ï¼‰ã€‚  
- æ”¯æŒå¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆ`--reward_pretrain model1,model2...`ï¼‰å’Œè¿œç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆ`--remote_rm_url`ï¼‰ã€‚  
- å®ç° [DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰/IPO/cDPO](./examples/scripts/train_dpo_llama.sh) å’Œ [Kahneman-Tversky Optimizationï¼ˆKTOï¼‰](./examples/scripts/train_kto_llama.sh)ã€‚  
- æ”¯æŒ [è¿­ä»£ DPO](./examples/scripts/train_iterative_dpo_llama.sh)ï¼ˆ[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)ï¼‰ã€‚  
- æ”¯æŒ [æ‹’ç»é‡‡æ ·](./examples/scripts/train_rejection_sampling_llama.sh)ã€‚  
- å®ç° [æ¡ä»¶ SFT](./examples/scripts/train_conditional_llama.sh)ï¼ˆ[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)ï¼‰ã€‚  
- æ”¯æŒ [çŸ¥è¯†è’¸é¦](./examples/scripts/train_knowledge_distillation.sh)ï¼ˆ[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)ï¼‰ã€‚  
- é›†æˆ [è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰](./examples/scripts/train_prm_mistral.sh)ã€‚  
- æ”¯æŒ SFTã€DPOã€RMã€PRM å’Œ PPO çš„è®­ç»ƒæ ·æœ¬æ‰“åŒ…ï¼ˆ`--packing_samples`ï¼‰ã€‚  
- å®ç° [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)ï¼ˆ`--ring_attn_size`ï¼Œ`--ring_head_stride`ï¼‰ã€‚  
- æ”¯æŒ [ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰](./examples/test_scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--aux_loss_coef`ï¼‰ã€‚  
- é›†æˆ FlashAttention2ï¼ˆ`--attn_implementation`ï¼‰ã€‚  
- æ”¯æŒ QLoRAï¼ˆ`--load_in_4bit`ï¼‰å’Œ [LoRA](./examples/scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--lora_rank`ï¼Œ`--target_modules`ï¼‰ã€‚  
- å…¼å®¹ HuggingFace çš„ `tokenizer.apply_chat_template` æ•°æ®é›†æ ¼å¼ï¼ˆ`--apply_chat_template` å’Œ `--input_key`ï¼‰ã€‚  
- æ”¯æŒä½¿ç”¨ Wandbï¼ˆ`--use_wandb`ï¼‰å’Œ TensorBoardï¼ˆ`--use_tensorboard`ï¼‰è¿›è¡Œæ—¥å¿—è®°å½•ã€‚  
- æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆ`--load_checkpoint` å’Œ `--save_steps`ï¼‰ã€‚  
- æä¾›äº†å¤šèŠ‚ç‚¹è®­ç»ƒè„šæœ¬, æ¯”å¦‚ [DPO](./examples/scripts/train_llama_slurm.sh) å’Œ [RLHF](./examples/scripts/train_ppo_llama_ray_slurm.sh)


## å¿«é€Ÿå¼€å§‹

### å®‰è£…

è¦ä½¿ç”¨ OpenRLHFï¼Œé¦–å…ˆå¯åŠ¨ Docker å®¹å™¨ï¼ˆ**æ¨è**ï¼‰ç„¶åæ‰§è¡Œ `pip install` å®‰è£… `openrlhf`ï¼š

```bash
# å¯åŠ¨ docker container
docker run --runtime=nvidia -it --rm --shm-size="10g" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:25.02-py3 bash
sudo pip uninstall xgboost transformer_engine flash_attn pynvml -y

# pip install
pip install openrlhf

# å¦‚æœä½ éœ€è¦ä½¿ç”¨ vLLM åŠ é€Ÿ (å®‰è£… vLLM 0.11.0)
pip install openrlhf[vllm]
# æœ€æ–°çš„ vLLM ä¹Ÿæ˜¯æ”¯æŒçš„
pip install openrlhf[vllm_latest]
# å®‰è£… vLLMã€ring-flash-attention å’Œ Liger-Kernel
pip install openrlhf[vllm,ring,liger]

# pip install GitHub ä¸Šçš„æœ€æ–°ç‰ˆ
pip install git+https://github.com/OpenRLHF/OpenRLHF.git

# æˆ–è€… git clone
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
```

> [!NOTE]
>æˆ‘ä»¬æ¨èä½¿ç”¨ vLLM 0.11.0 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚
>æˆ‘ä»¬ä¹Ÿæä¾›äº† [Dockerfiles for vLLM](./dockerfile/) å’Œ [Nvidia-Docker ä¸€é”®å®‰è£…è„šæœ¬](./examples/scripts/nvidia_docker_install.sh)ã€‚

### å‡†å¤‡æ•°æ®é›†
OpenRLHF åœ¨å…¶æ•°æ®é›†ç±»ä¸­æä¾›äº†å¤šç§æ•°æ®å¤„ç†æ–¹æ³•ã€‚
ä¾‹å¦‚åœ¨ [Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6) ä¸­ï¼š

```python
def preprocess_data(data, input_template=None, input_key="input", apply_chat_template=None) -> str:
    if apply_chat_template:
        chat = data[input_key]
        if isinstance(chat, str):
            chat = [{"role": "user", "content": chat}]
        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    else:
        prompt = data[input_key]
        if input_template:
            prompt = input_template.format(prompt)
    return prompt
```

- æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `--input_key` æŒ‡å®š `JSON key name` ä¸ºè¾“å…¥æ•°æ®é›† `--prompt_data {name or path}` (PPO) æˆ– `--dataset {name or path}`ï¼Œå¹¶ä½¿ç”¨ `--apply_chat_template` åˆ©ç”¨ [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating) ä¸­çš„ `chat_template`ã€‚
- å¦‚æœä¸æƒ³ä½¿ç”¨ `--apply_chat_template`ï¼Œå¯ä»¥æ”¹ç”¨ `--input_template`ï¼Œæˆ–é¢„å…ˆç¦»çº¿å¤„ç†æ•°æ®é›†ã€‚
- OpenRLHF è¿˜æ”¯æŒä½¿ç”¨ `--prompt_data_probs 0.1,0.4,0.5` (PPO) æˆ– `--dataset_probs 0.1,0.4,0.5` æ··åˆå¤šä¸ªæ•°æ®é›†ã€‚

Chat Templating çš„å·¥ä½œåŸç†å¦‚ä¸‹:

```python
dataset = [{"input_key": [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]}]

tokenizer.apply_chat_template(dataset[0]["input_key"], tokenize=False)

"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

å¦‚ä½•æŒ‡å®šæµ‹è¯•æ•°æ®é›† ?

è¯·ä½¿ç”¨ ``--eval_dataset {name or path}`` æ¥è®¾ç½®æµ‹è¯•æ•°æ®é›†è·¯å¾„ã€‚

> [!NOTE]
> ``JSON key`` é€‰é¡¹å–å†³äºå…·ä½“çš„æ•°æ®é›†ã€‚è¯·å‚é˜… [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) å’Œ [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)


### Supervised Fine-tuning

OpenRLHF çš„æ¨¡å‹æ£€æŸ¥ç‚¹å®Œå…¨å…¼å®¹ HuggingFace æ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `--pretrain  {name or path}`ã€`--reward_pretrain  {name or path}` å’Œ `--critic_pretrain  {name or path}` æŒ‡å®šæ¨¡å‹åç§°æˆ–è·¯å¾„ã€‚æˆ‘ä»¬åœ¨ [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF) ä¸Šæä¾›äº†ä¸€äº›é¢„è®­ç»ƒçš„æ£€æŸ¥ç‚¹å’Œæ•°æ®é›†ã€‚

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨ [examples/scripts](./examples/scripts/) ç›®å½•ä¸­æä¾›çš„å¯åŠ¨è„šæœ¬ï¼Œæˆ–è€…ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š

```bash 
deepspeed --module openrlhf.cli.train_sft \
   --max_len 4096 \
   --dataset Open-Orca/OpenOrca \
   --input_key question \
   --output_key response \
   --input_template $'User: {}\nAssistant: ' \
   --train_batch_size 256 \
   --micro_train_batch_size 2 \
   --max_samples 500000 \
   --pretrain meta-llama/Meta-Llama-3-8B \
   --save_path ./checkpoint/llama3-8b-sft \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --zero_stage 2 \
   --max_epochs 1 \
   --bf16 \
   --learning_rate 5e-6 \
   --gradient_checkpointing \
   --packing_samples \
   --load_checkpoint \
   --use_wandb {wandb_token}

# æ”¯æŒ HF tokenizer.apply_chat_template
# --apply_chat_template 
# --tokenizer_chat_template {HF Chat Template}

# æ”¯æŒ RingAttention
# pip install ring_flash_attn
#   --ring_attn_size 2 \
#   --ring_head_stride 2 \

# ä¹Ÿå¯ç”¨äº continued pre-training
# --pretrain_mode
```

> [!NOTE]
> OpenRLHF SFT/DPO/RewardModel/PPO è®­ç»ƒæ”¯æŒ `--packing_samples` [åŸºäº `flash attention`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)

### Reward Model Training
```bash
deepspeed --module openrlhf.cli.train_rm \
   --save_path ./checkpoint/llama3-8b-rm \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --train_batch_size 256 \
   --micro_train_batch_size 1 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --bf16 \
   --max_epochs 1 \
   --max_len 8192 \
   --zero_stage 3 \
   --learning_rate 9e-6 \
   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
   --apply_chat_template \
   --chosen_key chosen \
   --rejected_key rejected \
   --packing_samples \
   --gradient_checkpointing \
   --load_checkpoint \
   --use_wandb {wandb_token}

```

æ¨èè®¾ç½® Reward Model çš„ `--value_prefix_head` é€‰é¡¹ä¸º `score`, è¿™æ ·ä½¿å¾—æˆ‘ä»¬å¯ä»¥ç”¨ `AutoModelForSequenceClassification` åŠ è½½æ¨¡å‹:

```python
reward_model = AutoModelForSequenceClassification.from_pretrained(
              reward_model_path,
              num_labels=1,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              use_cache=False,
          )
inputs = xxxx (Left Padding Input Tokens)
reward = reward_model.model(*inputs).last_hidden_state
reward = reward_model.score(reward)[:, -1]
```

### ä½¿ç”¨ Ray å’Œ vLLM çš„ PPO/REINFORCE++

ä¸ºäº†æé«˜ RLHF è®­ç»ƒé€Ÿåº¦æˆ–æ”¯æŒ 70B æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Ray å’Œ vLLM åŠ é€Ÿçš„ PPO (Hybrid Engine)

```bash
# åœ¨å®¹å™¨ä¸­å¯åŠ¨ Ray çš„ä¸»èŠ‚ç‚¹
ray start --head --node-ip-address 0.0.0.0 --num-gpus 8

# å¦‚æœè¦åœ¨æ›´å¤šèŠ‚ç‚¹ä¸Šå¯åŠ¨ Rayï¼Œè¯·ä½¿ç”¨
ray start --address {MASTER-NODE-ADDRESS}:6379 --num-gpus 8

ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  --vllm_num_engines 4 \
  --vllm_tensor_parallel_size 2 \
  --colocate_all_models \
  --vllm_gpu_memory_utilization 0.5 \
  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
  --reward_pretrain OpenRLHF/Llama-3-8b-rm-700k \
  --save_path /openrlhf/examples/test_scripts/final/llama3-8b-rlhf \
  --ckpt_path /openrlhf/examples/test_scripts/ckpt/llama3-8b-rlhf \
  --save_hf_ckpt \
  --micro_train_batch_size 8 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 16 \
  --rollout_batch_size 1024 \
  --n_samples_per_prompt 1 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --max_samples 100000 \
  --generate_max_len 1024 \
  --zero_stage 3 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data OpenRLHF/prompt-collection-v0.1 \
  --input_key context_messages \
  --apply_chat_template \
  --normalize_reward \
  --gradient_checkpointing \
  --packing_samples \
  --vllm_sync_backend nccl \
  --enforce_eager \
  --vllm_enable_sleep \
  --deepspeed_enable_sleep \
  --use_wandb {wandb_token}

# æ”¯æŒ REINFORCE++  | RLOO | REINFORCE++-baseline | GRPO | Dr. GRPO
# --advantage_estimator reinforce | rloo | reinforce_baseline | group_norm | dr_grpo

# è®¾ç½® --init_kl_coef ä¸º 0 å°†ä¸ä¼šå¯åŠ¨å‚è€ƒæ¨¡å‹

# æ”¯æŒè¿œç¨‹å¥–åŠ±æ¨¡å‹ (HTTP)
# --remote_rm_url http://localhost:5000/get_reward

# æ”¯æŒ N ä¸ªæ ·æœ¬
# --n_samples_per_prompt 4
```

> [!NOTE]
> ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ ``setup_commands`` è®© Ray è‡ªåŠ¨éƒ¨ç½²ç¯å¢ƒï¼Œä¾‹å¦‚ `--runtime-env-json='{"setup_commands": ["pip install openrlhf[vllm]"]}'`ã€‚

> [!NOTE]
> OpenRLHF ä¸­çš„ RLOO å’Œ REINFORCE++-baseline æ˜¯åŸºäº REINFORCE++ çš„ä¿®æ”¹ç‰ˆæœ¬ï¼š
> - REINFORCE++ é›†æˆäº† PPO çš„å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚ä¼˜åŠ¿å½’ä¸€åŒ–å’Œ PPO-clip lossï¼‰åˆ° REINFORCEï¼ŒåŒæ—¶æ¶ˆé™¤äº†å¯¹ Critic ç½‘ç»œçš„éœ€æ±‚ã€‚
> - REINFORCE++-baseline ä½¿ç”¨`æ¥è‡ªåŒä¸€ä¸ª prompt çš„å¤šä¸ªæ ·æœ¬çš„å¹³å‡å¥–åŠ±`ä½œä¸ºåŸºçº¿æ¥é‡å¡‘å¥–åŠ±ï¼Œå› æ­¤åœ¨ RLVR è®¾ç½®ä¸‹ï¼Œç®—æ³•å¯¹ 0ï¼ˆé”™è¯¯ï¼‰/ 1ï¼ˆæ­£ç¡®ï¼‰/ -0.5ï¼ˆæ ¼å¼å¥–åŠ±ï¼‰æˆ– -1ï¼ˆé”™è¯¯ï¼‰/ 1ï¼ˆæ­£ç¡®ï¼‰/ -0.5ï¼ˆæ ¼å¼å¥–åŠ±ï¼‰ç­‰å¥–åŠ±æ¨¡å¼ä¸æ•æ„Ÿã€‚
> - OpenRLHF ä¸­çš„ RLOO é€šè¿‡å¼•å…¥`per token çš„ KL å¥–åŠ±`å¹¶ä½¿ç”¨ `PPO-clip loss` æ¥ä¿®æ”¹åŸå§‹ç‰ˆæœ¬ã€‚
> - Dr. GRPO ç§»é™¤äº† GRPO ä¸­çš„ç»„å½’ä¸€åŒ– `/std`ã€‚


> [!NOTE]
> å¦‚æœé‡åˆ° deepspeed è®¾ç½® GPU è®¾å¤‡æ—¶å‡ºç°ç´¢å¼•è¶Šç•Œé”™è¯¯ï¼Œå¯ä»¥å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py) ä½œä¸ºä¸´æ—¶è§£å†³æ–¹æ¡ˆã€‚
>   ```bash
>   # å¯¹äº NVIDIA GPUï¼š
>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1
>   ```

æ‰€æœ‰æ”¯æŒç®—æ³•çš„å¯åŠ¨è„šæœ¬å’Œæ–‡æ¡£åœ¨ [example/scripts](./examples/scripts/) å’Œ [Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)

### å¼ºåŒ–å¾®è°ƒ (RFT)

OpenRLHFæ”¯æŒä¾¿æ·é«˜æ•ˆçš„å¼ºåŒ–å¾®è°ƒã€‚æ‚¨åªéœ€è¦å®ç°ä¸€ä¸ª[åŒ…å«è‡ªå®šä¹‰`reward_func`å‡½æ•°çš„æ–‡ä»¶](./examples/scripts/reward_func.py)å¹¶å°†å…¶è·¯å¾„ä¼ é€’ç»™`remote_rm_url`å‚æ•°å³å¯ã€‚ä¾‹å¦‚ï¼š

```python
# reward_func.py
import torch

def reward_func(queries, prompts, labels):
    # queriesæ˜¯prompts + responses
    # labelsæ˜¯answers
    print(queries)

    # ç”Ÿæˆéšæœºå¥–åŠ±ä½œä¸ºç¤ºä¾‹
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™åº”è¯¥æ›¿æ¢ä¸ºå®é™…çš„å¥–åŠ±è®¡ç®—é€»è¾‘
    reward = torch.randint(0, 2, (len(queries),)).float()

    return {
        "rewards": reward,  # ç”¨äºä¼˜åŠ¿è®¡ç®—çš„å¥–åŠ±
        "scores": reward,  # ç”¨äºåŠ¨æ€è¿‡æ»¤çš„åˆ†æ•°ï¼ˆ0-1å¥–åŠ±ï¼‰
        "extra_logs": {"dummy_scores": reward},  # wandbçš„é¢å¤–æ—¥å¿—ä¿¡æ¯
    }
```

ç„¶ååªéœ€è®¾ç½®ï¼š

```shell 
ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  ...
  --remote_rm_url /path/to/reward_func.py \
  --label_key answer
```

å…¶ä¸­`label_key`å‚æ•°ç”¨äºå‘å¥–åŠ±å‡½æ•°ä¼ é€’é¢å¤–çš„æ ·æœ¬ä¿¡æ¯ï¼Œå¦‚ç­”æ¡ˆã€‚

## å¼‚æ­¥RLHFå’ŒAgent RLHF

OpenRLHFä¸ºå¼‚æ­¥RLHFå’ŒåŸºäºAgentçš„RLHFå®ç°æä¾›äº†å…¨é¢çš„æ”¯æŒã€‚è¦ä½¿ç”¨è¿™äº›åŠŸèƒ½ï¼Œåªéœ€åœ¨è®­ç»ƒé…ç½®ä¸­åŒ…å«`--async_train`å’Œ`--agent_func_path`å‚æ•°å³å¯ã€‚

Agent APIå·²ç»é‡æ–°è®¾è®¡ä¸ºä½¿ç”¨åŸºäºç±»çš„æ–¹æ³•ï¼Œé‡‡ç”¨`AgentInstanceBase`å’Œ`AgentExecutorBase`ç±»ï¼Œä»¥æä¾›æ›´å¥½çš„æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§ã€‚

```python
# agent_func.py
import random
from typing import Any, Dict

import torch
from openrlhf.utils.agent import AgentExecutorBase, AgentInstanceBase


# A simple n-step random environment
class AgentInstance(AgentInstanceBase):
    async def __init__(self, *args, **kwargs):
        self.step_idx = 0
        self.max_steps = random.randint(1, 3)  # 1-3 steps

    async def reset(self, states: dict, **kwargs):
        return {"observation": states["observation"]}  # Return original text observation

    async def step(self, states: dict, **kwargs) -> Dict[str, Any]:
        print(f"step_idx: {self.step_idx}, max_steps: {self.max_steps}")

        observation_text = states["observation_text"]
        action_text = states["action_text"]
        label = states["label"]

        # Check if episode is done
        done = self.step_idx >= self.max_steps
        reward = torch.randint(0, 2, (1,)).float() if done else torch.tensor(0)

        # Generate environment feedback based on whether episode is done
        environment_feedback = (
            "\n\nHuman: [CORRECT]\n</s>"
            if done
            else "\n\nHuman: [INCORRECT]\nPlease analyze the issues and try again.\n</s>\n\nAssistant: "
        )

        self.step_idx += 1

        return {
            "rewards": reward,  # Rewards for advantage calculation
            "scores": reward,  # Scores for dynamic filtering (0-1 reward)
            "environment_feedback": environment_feedback,  # Environment feedback text
            "done": done,  # Boolean indicating if the episode is complete
            "sampling_params": states.get("sampling_params", None),  # Parameters for vLLM sampling in next step
            "extra_logs": {"dummy_scores": reward},  # Additional logging information
        }


class AgentExecutor(AgentExecutorBase):
    def __init__(self, max_steps, max_length, llm_engine, hf_tokenizer, result_queue):
        super().__init__(AgentInstance, max_steps, max_length, llm_engine, hf_tokenizer, result_queue)

    async def execute(self, prompt, label, sampling_params):
        # You could override the execute function of AgentExecutorBase to add custom agent running logic
        return await super().execute(prompt, label, sampling_params)
```

æ‚¨è¿˜å¯ä»¥é€šè¿‡è®¾ç½®`export OPENRLHF_ASYNC_NUM_TASKS=128`æ¥é…ç½®æ¯ä¸ªvLLMå¼•æ“çš„æœ€å¤§å¹¶å‘ä»£ç†æ•°ã€‚
æ­¤å¤–ï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨ç¯å¢ƒä¸­è®¾ç½®`export OPENRLHF_ASYNC_QUEUE_SIZE=1`ï¼ˆæ­¤å‚æ•°æ§åˆ¶ç¼“å†²åŒºæœ€å¤šå¯ä»¥å­˜å‚¨å¤šå°‘æ‰¹æ•°æ®ï¼‰æ¥æ§åˆ¶ç¦»ç­–ç•¥é‡‡æ ·çš„ç¨‹åº¦ã€‚

> [!NOTE]
> é€šè¿‡é‡å†™ `AgentExecutorBase` çš„ `execute` å‡½æ•°ï¼Œæ‚¨å¯ä»¥å®ç°å®Œå…¨è‡ªå®šä¹‰çš„ä»£ç†è¿è¡Œè¿‡ç¨‹ã€‚è¯¥è®¾è®¡éµå¾ª **token-in-token-out åŸåˆ™**ï¼Œç¡®ä¿é‡‡æ ·å’Œè®­ç»ƒæ ·æœ¬ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œé¿å…æ–‡æœ¬çº§å¤„ç†å¯èƒ½å‡ºç°çš„æ½œåœ¨ä¸åŒ¹é…é—®é¢˜ã€‚



> [!NOTE] 
> OpenRLHFçš„Agent RLHFä¹Ÿæ”¯æŒæ··åˆå¼•æ“è®­ç»ƒã€‚è¦å¯ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·ç§»é™¤`--async_train`æ ‡å¿—å¹¶å¯ç”¨`--colocate_all_models`ã€‚

> [!WARNING] 
> å¼‚æ­¥è®­ç»ƒå¯èƒ½ä¼šå½±å“è®­ç»ƒç¨³å®šæ€§ã€‚å»ºè®®ä¼˜å…ˆä½¿ç”¨æ··åˆå¼•æ“æˆ–åŒæ­¥è®­ç»ƒæ¨¡å¼ã€‚

### LoRA
å¦‚æœæ‚¨ä½¿ç”¨ `LoRA (Low-Rank Adaptation)`ï¼Œ`OpenRLHF` é»˜è®¤ä¸ä¼šä¿å­˜å®Œæ•´çš„æƒé‡ï¼Œè€Œæ˜¯ä¿å­˜ `LoRA Adapter`ã€‚è¦æ­£å¸¸ç»§ç»­æ‚¨çš„ä»»åŠ¡ï¼Œæ‚¨éœ€è¦å°† `Adapter` ä¸åŸºç¡€æ¨¡å‹çš„æƒé‡åˆå¹¶

```bash
python -m openrlhf.cli.lora_combiner \
    --model_path meta-llama/Meta-Llama-3-8B \
    --lora_path ./checkpoint/llama3-8b-rm \
    --output_path ./checkpoint/llama-3-8b-rm-combined \
    --is_rm \
    --bf16
```

### æ€§èƒ½è°ƒä¼˜æŒ‡å—

ä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®æŒ‰ `vLLM:Actor:Critic = 1:1:1` çš„æ¯”ä¾‹åˆ†é…èŠ‚ç‚¹ã€‚

- ä¾‹å¦‚ï¼Œå¯¹äº 70B æ¨¡å‹å’Œ 48 ä¸ª A100 GPUï¼Œå»ºè®®å°† 16 ä¸ª A100 GPU åˆ†é…ç»™ vLLM å¼•æ“ï¼Œ16 ä¸ª GPU åˆ†é…ç»™ Actor æ¨¡å‹ï¼Œå‰©ä½™çš„ 16 ä¸ª GPU åˆ†é…ç»™ Critic æ¨¡å‹ã€‚
- å½“ RL ç®—æ³•æ”¶æ•›æ€§æ»¡è¶³è¦æ±‚æ—¶è¯·å¯ç”¨å¼‚æ­¥è®­ç»ƒ `--async_train`ã€‚
- å½“æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜æ—¶ï¼Œä½¿ç”¨ Hybrid engine `--colocate_all_models` å’Œ `--vllm_enable_sleep` ä»¥åŠ `--deepspeed_enable_sleep`ï¼Œè€Œä¸æ˜¯åˆ†å¸ƒå¼ RLHFã€‚
- å¯ç”¨ `--colocate_critic_reward`ã€`--colocate_actor_ref` é€‰é¡¹æ¥åˆå¹¶èŠ‚ç‚¹ã€‚
- åº”è¯¥å°½å¯èƒ½å¢åŠ  `rollout_micro_batch_size`ï¼ˆå¹¶æœ€å°åŒ– vLLM å¼•æ“çš„ TP å¤§å°ï¼‰ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œè¾ƒå¤§çš„ `--micro_train_batch_size` æ›´å¥½ï¼Œå¹¶å¯ç”¨ `--packing_samples`ã€‚
- å½“æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜æ—¶ï¼Œè¯·ç¦ç”¨ `--adam_offload` å¹¶å¯ç”¨ `--overlap_comm`ã€‚åŒæ—¶å¯ç”¨ `--deepcompile` æ¥åŠ é€Ÿè®­ç»ƒã€‚
- å¯¹äº vLLMï¼Œè¯·ä½¿ç”¨ `--vllm_sync_backend nccl`
- å¯åŠ¨ ``--use_dynamic_batch`` ä»¥åŠ é€Ÿ deepspeed è®­ç»ƒå’Œå‰å‘è¿‡ç¨‹.
- å½“ `n_samples_per_prompts` > 1 æ—¶ï¼Œåœ¨ vLLM ç”Ÿæˆä¸­å¯ç”¨ [enable_prefix_caching](https://docs.vllm.ai/en/stable/automatic_prefix_caching/apc.html)ã€‚
- å¯¹äºå¤§å‹åŸºç¡€æ¨¡å‹ï¼Œå¦‚æœå‘ç”Ÿ OOMï¼Œä¸è¦ä½¿ç”¨ä»»ä½• `--colocate_xxxx` é€‰é¡¹ã€‚


## ä½¿ç”¨ OpenRLHF çš„å…¬å¸å’Œç»„ç»‡

- Google
- ByteDance
- Tencent
- Alibaba
- Baidu
- China Telecom
- Allen AI
- Vivo
- NexusFlow
- JÃ¼lich Supercomputing Centre (JSC)
- Berkeley Starling Team
- M-A-P
- ...


## åŠ å…¥æˆ‘ä»¬

**å¦‚ä½•åŠ å…¥ï¼Ÿ**

1. é€šè¿‡è”ç³»é‚®ç®± janhu9527@gmail.com æˆ–è€…åŠ å…¥ [GitHub Organization](https://github.com/OpenRLHF)ã€‚è¯·åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
   - æ‚¨çš„å§“å
   - æ‚¨çš„ GitHub ç”¨æˆ·å
   - æ‚¨æ„Ÿå…´è¶£çš„é¢†åŸŸ
   - æ‚¨åœ¨ NLP å’Œ/æˆ– AI ç›¸å…³çš„æŠ€èƒ½å’Œç»éªŒ
2. æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡å®˜æ–¹ GitHub [OpenRLHF â†—](https://github.com/OpenRLHF/OpenRLHF) é¡¹ç›®é¡µé¢åŠ å…¥æˆ‘ä»¬ã€‚åªéœ€åˆ›å»ºä¸€ä¸ªå…³äºæ‚¨æƒ³è¦è´¡çŒ®çš„å…´è¶£çš„ issueï¼Œæˆ‘ä»¬ä¼šä¸æ‚¨è”ç³»ã€‚

**æ‚¨èƒ½åšä»€ä¹ˆï¼Ÿ**

1. åŠ å…¥å›¢é˜Ÿï¼Œå‚ä¸ OpenRLHF é¡¹ç›®çš„å¼€å‘ã€‚
2. é€šè¿‡æäº¤ pull è¯·æ±‚æ¥ä¸ºé¡¹ç›®åšå‡ºè´¡çŒ®ã€‚
3. å¸®åŠ©æ”¹è¿›æ–‡æ¡£ï¼Œä¿®å¤ bugs æˆ–åˆ›å»ºæ–°åŠŸèƒ½ã€‚
4. åˆ†äº«é¡¹ç›®å¹¶å¸®åŠ©æˆ‘ä»¬å‘å±•ç¤¾åŒºã€‚

## èµåŠ©æˆ‘ä»¬

æ‚¨çš„èµåŠ©å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç»´æŠ¤å’Œæ”¹è¿› OpenRLHFã€‚å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰ç”¨ï¼Œè¯·è€ƒè™‘èµåŠ©æˆ‘ä»¬ã€‚æ‚¨å¯ä»¥åœ¨ [Open Collective â†—](https://opencollective.com/OpenRLHF) ä¸ŠèµåŠ©æˆ‘ä»¬ã€‚

## æ˜Ÿå›¾

[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)

## è´¡çŒ®è€…

éå¸¸æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ï¼å¦‚æœæ‚¨æƒ³è´¡çŒ®ï¼Œè¯·éšæ—¶åˆ›å»º pull è¯·æ±‚æˆ–åˆ›å»º issueã€‚

<a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF" />
</a>

## å¼•ç”¨ä¸è‡´è°¢

æˆ‘ä»¬æƒ³è¦å¯¹ä»¥ä¸‹é¡¹ç›®å’Œç»„ç»‡åœ¨ AI å’Œ NLP é¢†åŸŸçš„è´¡çŒ®è¡¨ç¤ºæ„Ÿè°¢ï¼š

- [Hugging Face Transformers â†—](https://github.com/huggingface/transformers)
- [OpenAI GPT â†—](https://github.com/openai/gpt-3)
- [LLaMA â†—](https://llama.meta.com/)
- [DeepSpeed â†—](https://github.com/microsoft/DeepSpeed)
- [Ray â†—](https://github.com/ray-project/ray)

æˆ‘ä»¬çš„é¡¹ç›®è¿˜æƒ³è¦æ„Ÿè°¢ [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalChat) å’Œ [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)ã€‚åœ¨é¡¹ç›®çš„æ—©æœŸé˜¶æ®µï¼Œæˆ‘ä»¬å‚è€ƒäº†ä»–ä»¬çš„ä»£ç è®¾è®¡ã€‚
æˆ‘ä»¬çš„é¡¹ç›®è¿˜æƒ³è¦æ„Ÿè°¢ [Netmind.AI](https://www.netmind.ai/) å¯¹äºring attentionå¼€å‘çš„GPUæ”¯æŒã€‚

(2024/7) æˆ‘ä»¬çš„ GitHub ç»„ç»‡ä» OpenLLMAI è¿ç§»åˆ°äº† OpenRLHF.

## å¼•ç”¨
OpenRLHF
```
@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}
```
REINFORCE++-baseline
```
@article{hu2025reinforce++,
  title={Reinforce++: A simple and efficient approach for aligning large language models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}
```


______________________________________________________________________

*OpenRLHF Â© 2025 OpenRLHF. ç‰ˆæƒæ‰€æœ‰ã€‚*
