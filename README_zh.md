<div align="center">
<p align="center">
<img alt="" src="./docs/logo.png" style="display: inline-block; height: 140px" />
</p>
</div>

<div align="center">
<p align="center">
      <a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/discussions">
        <img alt="Issues" src="https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff" />
      <a href="https://github.com/OpenRLHF/OpenRLHF/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf" />
      </a>
      <br>
      <em>å¼€æº / å…¨é¢ / è½»é‡çº§ / æ˜“ç”¨</em>
    </p>
</p>
</div>

<hr>

<span>[ <a href="README.md">English</a> | ä¸­æ–‡ | <a href="README_ja.md">æ—¥æœ¬èª</a> ]</span>

OpenRLHF æ˜¯ä¸€ä¸ªåŸºäº Rayã€DeepSpeed å’Œ HF Transformers æ„å»ºçš„é«˜æ€§èƒ½ RLHF æ¡†æ¶ï¼š

- **ç®€å•æ˜“ç”¨**: OpenRLHF æ˜¯ç›®å‰å¯ç”¨çš„æœ€ç®€å•çš„é«˜æ€§èƒ½ RLHF åº“ä¹‹ä¸€ï¼Œæ— ç¼å…¼å®¹ Huggingface æ¨¡å‹å’Œæ•°æ®é›†ã€‚
- **é«˜æ€§èƒ½**: RLHF è®­ç»ƒä¸­ 80% çš„æ—¶é—´ç”¨äºæ ·æœ¬ç”Ÿæˆé˜¶æ®µã€‚å¾—ç›Šäºä½¿ç”¨ Ray, Packing Samples ä»¥åŠ vLLM ç”ŸæˆåŠ é€Ÿçš„èƒ½åŠ›ï¼ŒOpenRLHF çš„æ€§èƒ½æ˜¯æè‡´ä¼˜åŒ–çš„ DeepSpeedChat with Hybrid Engine çš„3~4å€ä»¥ä¸Šã€‚
- **åˆ†å¸ƒå¼ RLHF**:  OpenRLHF ä½¿ç”¨ Ray å°† Actorã€Rewardã€Reference å’Œ Critic æ¨¡å‹åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸Šï¼ŒåŒæ—¶å°† Adam ä¼˜åŒ–å™¨æ”¾åœ¨ CPU ä¸Šã€‚è¿™ä½¿å¾—ä½¿ç”¨å¤šä¸ª A100 80G GPU å’Œ vLLM å¯ä»¥å…¨é¢å¾®è°ƒè¶…è¿‡ 70B+ çš„æ¨¡å‹ ä»¥åŠåœ¨å¤šä¸ª 24GB RTX 4090 GPU ä¸Šå¾®è°ƒ 7B æ¨¡å‹ã€‚
- **Hybrid Engine**: OpenRLHF è¿˜æ”¯æŒè¿˜æ”¯æŒ Hybrid engine æ‰€æœ‰è®­ç»ƒå¼•æ“å’Œæ¨ç†å¼•æ“å…±ç”¨GPUæ¥é¿å…èµ„æºé—²ç½®ã€‚
- **PPO å®ç°æŠ€å·§**: æˆ‘ä»¬é›†æˆäº† PPO çš„å®ç°æŠ€å·§ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œè¯¦æƒ…å‚è€ƒ [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/622134699) å’Œ [Notion blog](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361).

æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [PPT](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2405.11143) | [ä½¿ç”¨æ–‡æ¡£](https://openrlhf.readthedocs.io/)


## æ–°é—»  
- [2025/2] MIT & Microsoft æå‡ºäº† [On the Emergence of Thinking in LLMs I: Searching for the Right Intuition](https://arxiv.org/pdf/2502.06773) åŸºäº OpenRLHF
- [2025/1] æ¸¯ç§‘å¤§å¤ç°äº† [DeepSeek-R1-Zero and DeepSeek-R1 training on small models ä½¿ç”¨ OpenRLHF](https://github.com/hkust-nlp/simpleRL-reason)
- [2024/12] æˆ‘ä»¬"æå‡º"äº† ğŸ˜Š [REINFORCE++ å¯¹é½ç®—æ³•](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS).
- [2024/12] åœ¨ [Notion Blog](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05) ä¸­ï¼Œæˆ‘ä»¬å¯¹ PPOã€REINFORCE++ã€GRPO å’Œ RLOO è¿›è¡Œäº†åˆ†æã€‚  

## ç‰¹æ€§  

- åŸºäº Ray çš„åˆ†å¸ƒå¼ [PPO](./examples/scripts/train_ppo_llama_ray.sh) å’Œ [REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray.sh) å®ç°ã€‚  
- æ”¯æŒå¯¹ [è¶…è¿‡ 700 äº¿å‚æ•°çš„æ¨¡å‹](./examples/scripts/train_ppo_llama_ray_70b.sh) è¿›è¡Œå®Œæ•´çš„ RLHF å¾®è°ƒã€‚  
- æ”¯æŒåŸºäº Ray å’Œ Hybrid Engine çš„ [PPO](./examples/scripts/train_ppo_llama_ray_hybrid_engine.sh) å’Œ [REINFORCE++/RLOO](./examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh) (`--colocate_all_models`, `--vllm_enable_sleep` and `--vllm_gpu_memory_utilization 0.5`)
- [Ray-based Reinforced Finetuning](./examples/scripts/train_ppo_llama_with_reward_fn.sh)
- é›†æˆ vLLMï¼ŒåŠ é€Ÿ RLHF ä»»åŠ¡ä¸­çš„æ ·æœ¬ç”Ÿæˆï¼ˆ`--vllm_num_engines`ï¼‰ã€‚  
- æ”¯æŒå¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆ`--reward_pretrain model1,model2...`ï¼‰å’Œè¿œç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆ`--remote_rm_url`ï¼‰ã€‚  
- å®ç° [DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰/IPO/cDPO](./examples/scripts/train_dpo_llama.sh) å’Œ [Kahneman-Tversky Optimizationï¼ˆKTOï¼‰](./examples/scripts/train_kto_llama.sh)ã€‚  
- æ”¯æŒ [è¿­ä»£ DPO](./examples/scripts/train_iterative_dpo_llama.sh)ï¼ˆ[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)ï¼‰ã€‚  
- æ”¯æŒ [æ‹’ç»é‡‡æ ·](./examples/scripts/train_rejection_sampling_llama.sh)ã€‚  
- å®ç° [æ¡ä»¶ SFT](./examples/scripts/train_conditional_llama.sh)ï¼ˆ[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)ï¼‰ã€‚  
- æ”¯æŒ [çŸ¥è¯†è’¸é¦](./examples/scripts/train_knowledge_distillation.sh)ï¼ˆ[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)ï¼‰ã€‚  
- é›†æˆ [è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰](./examples/scripts/train_prm_mistral.sh)ã€‚  
- æ”¯æŒ SFTã€DPOã€RMã€PRM å’Œ PPO çš„è®­ç»ƒæ ·æœ¬æ‰“åŒ…ï¼ˆ`--packing_samples`ï¼‰ã€‚  
- å®ç° [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)ï¼ˆ`--ring_attn_size`ï¼Œ`--ring_head_stride`ï¼‰ã€‚  
- æ”¯æŒ [ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰](./examples/test_scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--aux_loss_coef`ï¼‰ã€‚  
- é›†æˆ FlashAttention2ï¼ˆ`--flash_attn`ï¼‰ã€‚  
- æ”¯æŒ QLoRAï¼ˆ`--load_in_4bit`ï¼‰å’Œ [LoRA](./examples/scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--lora_rank`ï¼Œ`--target_modules`ï¼‰ã€‚  
- å…¼å®¹ HuggingFace çš„ `tokenizer.apply_chat_template` æ•°æ®é›†æ ¼å¼ï¼ˆ`--apply_chat_template` å’Œ `--input_key`ï¼‰ã€‚  
- æ”¯æŒä½¿ç”¨ Wandbï¼ˆ`--use_wandb`ï¼‰å’Œ TensorBoardï¼ˆ`--use_tensorboard`ï¼‰è¿›è¡Œæ—¥å¿—è®°å½•ã€‚  
- æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆ`--load_checkpoint` å’Œ `--save_steps`ï¼‰ã€‚  
- æä¾›äº†å¤šèŠ‚ç‚¹è®­ç»ƒè„šæœ¬, æ¯”å¦‚ [DPO](./examples/scripts/train_llama_slurm.sh) å’Œ [RLHF](./examples/scripts/train_ppo_llama_ray_slurm.sh)


### PPO æ”¯æŒçŸ©é˜µ

| ç‰¹æ€§ | OpenRLHF | DSChat | CAIChat | TRL |
| ------------- |:-------------:| :-------------:| :-------------:| :-------------:| 
| ä½¿ç”¨ 16 ä¸ª A100 å®Œæˆ 70B+ å…¨å¾®è°ƒ      | âœ… | âŒ | âŒ | âŒ ||
| ä½¿ç”¨ 4 ä¸ª RTX4090 å®Œæˆ 7B å…¨å¾®è°ƒ | âœ…      |    âŒ | âŒ | âŒ | 
| ä½¿ç”¨ 8 ä¸ª A100 å®Œæˆ 34B DPO å…¨å¾®è°ƒ | âœ…      |    âŒ | âŒ | âŒ |   
| æ”¯æŒæ¨ç†å¼•æ“åŠ é€Ÿ | âœ…      |    âœ… | âŒ | âŒ |  
| PPO å®ç°æŠ€å·§ | âœ…      |    âŒ | âŒ | âœ… | 
| æ”¯æŒ QLoRA | âœ…      |    âŒ | âŒ | âœ… | 
| æ”¯æŒ Mixtral 8*7b | âœ…      |    âŒ | âŒ | âŒ | 
| æ”¯æŒæœªåˆå¹¶çš„ Actor-Critic | âœ…     |   âœ… | âœ… | âŒ | 
| æ”¯æŒå¤šä¸ªå¥–åŠ±æ¨¡å‹ | âœ…      |    âŒ | âŒ | âŒ |   
| æ”¯æŒ Huggingface æ¨¡å‹ | âœ…      |    âœ… | âœ… | âœ… | 
| æ˜“äºä½¿ç”¨ | âœ…      |   âŒ (HybridEngine bugs) | âœ… | âœ… | 

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

è¦ä½¿ç”¨ OpenRLHFï¼Œé¦–å…ˆå¯åŠ¨ Docker å®¹å™¨ï¼ˆ**æ¨è**ï¼‰ç„¶åæ‰§è¡Œ `pip install` å®‰è£… `openrlhf`ï¼š

```bash
# å¯åŠ¨ docker container
docker run --runtime=nvidia -it --rm --shm-size="10g" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash
sudo pip uninstall xgboost transformer_engine flash_attn pynvml -y

# pip install
pip install openrlhf

# å¦‚æœä½ éœ€è¦ä½¿ç”¨ vLLM åŠ é€Ÿ (å®‰è£… vLLM 0.7.2)
pip install openrlhf[vllm]
# æœ€æ–°çš„ vLLM ä¹Ÿæ˜¯æ”¯æŒçš„
pip install openrlhf[vllm_latest]

# pip install GitHub ä¸Šçš„æœ€æ–°ç‰ˆ
pip install git+https://github.com/OpenRLHF/OpenRLHF.git

# æˆ–è€… git clone
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
```

> [!NOTE]
>æˆ‘ä»¬æ¨èä½¿ç”¨ vLLM 0.7.2 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚
>æˆ‘ä»¬ä¹Ÿæä¾›äº† [Dockerfiles for vLLM](./dockerfile/) å’Œ [Nvidia-Docker ä¸€é”®å®‰è£…è„šæœ¬](./examples/scripts/nvidia_docker_install.sh)ã€‚

### å‡†å¤‡æ•°æ®é›†
OpenRLHF åœ¨å…¶æ•°æ®é›†ç±»ä¸­æä¾›äº†å¤šç§æ•°æ®å¤„ç†æ–¹æ³•ã€‚
ä¾‹å¦‚åœ¨ [Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6) ä¸­ï¼š

```python
def preprocess_data(data, input_template=None, input_key="input", apply_chat_template=None) -> str:
    if apply_chat_template:
        chat = data[input_key]
        if isinstance(chat, str):
            chat = [{"role": "user", "content": chat}]
        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    else:
        prompt = data[input_key]
        if input_template:
            prompt = input_template.format(prompt)
    return prompt
```

- æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `--input_key` æŒ‡å®š `JSON key name` ä¸ºè¾“å…¥æ•°æ®é›† `--prompt_data {name or path}` (PPO) æˆ– `--dataset {name or path}`ï¼Œå¹¶ä½¿ç”¨ `--apply_chat_template` åˆ©ç”¨ [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating) ä¸­çš„ `chat_template`ã€‚
- å¦‚æœä¸æƒ³ä½¿ç”¨ `--apply_chat_template`ï¼Œå¯ä»¥æ”¹ç”¨ `--input_template`ï¼Œæˆ–é¢„å…ˆç¦»çº¿å¤„ç†æ•°æ®é›†ã€‚
- OpenRLHF è¿˜æ”¯æŒä½¿ç”¨ `--prompt_data_probs 0.1,0.4,0.5` (PPO) æˆ– `--dataset_probs 0.1,0.4,0.5` æ··åˆå¤šä¸ªæ•°æ®é›†ã€‚

Chat Templating çš„å·¥ä½œåŸç†å¦‚ä¸‹:

```python
dataset = [{"input_key": [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]}]

tokenizer.apply_chat_template(dataset[0]["input_key"], tokenize=False)

"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

å¦‚ä½•æŒ‡å®šè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†åŒº ?

ä½ å¯ä»¥ä½¿ç”¨ `data_type@data_dir` çš„æ–¹å¼æŒ‡å®š, æ¯”å¦‚ä¸‹é¢çš„æ•°æ®é›†å¯ä»¥è®¾ç½®ä¸º `--dataset json@./data`

```
data
â”œâ”€â”€ test.jsonl
â””â”€â”€ train.jsonl
```

> [!NOTE]
>é»˜è®¤æƒ…å†µä¸‹æˆ‘ä»¬ä½¿ç”¨ `train` å’Œ `test` ä½œä¸º split åŒºåˆ† Huggingface çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®ã€‚
>`JSON key` é€‰é¡¹å–å†³äºå…·ä½“çš„æ•°æ®é›†ã€‚è¯·å‚é˜… [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) å’Œ [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/mai


### Supervised Fine-tuning

OpenRLHF çš„æ¨¡å‹æ£€æŸ¥ç‚¹å®Œå…¨å…¼å®¹ HuggingFace æ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `--pretrain  {name or path}`ã€`--reward_pretrain  {name or path}` å’Œ `--critic_pretrain  {name or path}` æŒ‡å®šæ¨¡å‹åç§°æˆ–è·¯å¾„ã€‚æˆ‘ä»¬åœ¨ [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF) ä¸Šæä¾›äº†ä¸€äº›é¢„è®­ç»ƒçš„æ£€æŸ¥ç‚¹å’Œæ•°æ®é›†ã€‚

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨ [examples/scripts](./examples/scripts/) ç›®å½•ä¸­æä¾›çš„å¯åŠ¨è„šæœ¬ï¼Œæˆ–è€…ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š

```bash 
deepspeed --module openrlhf.cli.train_sft \
   --max_len 4096 \
   --dataset Open-Orca/OpenOrca \
   --input_key question \
   --output_key response \
   --input_template $'User: {}\nAssistant: ' \
   --train_batch_size 256 \
   --micro_train_batch_size 2 \
   --max_samples 500000 \
   --pretrain meta-llama/Meta-Llama-3-8B \
   --save_path ./checkpoint/llama3-8b-sft \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --zero_stage 2 \
   --max_epochs 1 \
   --bf16 \
   --flash_attn \
   --learning_rate 5e-6 \
   --gradient_checkpointing \
   --packing_samples \
   --load_checkpoint \
   --use_wandb {wandb_token}

# æ”¯æŒ HF tokenizer.apply_chat_template
# --apply_chat_template 
# --tokenizer_chat_template {HF Chat Template}

# æ”¯æŒ RingAttention
# pip install ring_flash_attn
#   --ring_attn_size 2 \
#   --ring_head_stride 2 \

# ä¹Ÿå¯ç”¨äº continued pre-training
# --pretrain_mode
```

> [!NOTE]
> OpenRLHF SFT/DPO/RewardModel/PPO è®­ç»ƒæ”¯æŒ `--packing_samples` [åŸºäº `--flash_attn`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)

### Reward Model Training
```bash
deepspeed --module openrlhf.cli.train_rm \
   --save_path ./checkpoint/llama3-8b-rm \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --train_batch_size 256 \
   --micro_train_batch_size 1 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --bf16 \
   --max_epochs 1 \
   --max_len 8192 \
   --zero_stage 3 \
   --learning_rate 9e-6 \
   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
   --apply_chat_template \
   --chosen_key chosen \
   --rejected_key rejected \
   --flash_attn \
   --packing_samples \
   --gradient_checkpointing \
   --load_checkpoint \
   --use_wandb {wandb_token}

```

æ¨èè®¾ç½® Reward Model çš„ `--value_prefix_head` é€‰é¡¹ä¸º `score`, è¿™æ ·ä½¿å¾—æˆ‘ä»¬å¯ä»¥ç”¨ `AutoModelForSequenceClassification` åŠ è½½æ¨¡å‹:

```python
reward_model = AutoModelForSequenceClassification.from_pretrained(
              reward_model_path,
              num_labels=1,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              use_cache=False,
          )
inputs = xxxx (Left Padding Input Tokens)
reward = reward_model.model(*inputs).last_hidden_state
reward = reward_model.score(reward)[:, -1]
```

### ä¸ä½¿ç”¨ Ray çš„ PPO

```bash
deepspeed --module openrlhf.cli.train_ppo \
  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \
  --save_path ./checkpoint/llama-3-8b-rlhf \
  --save_steps -1 \
  --logging_steps 1 \
  --eval_steps -1 \
  --micro_train_batch_size 2 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 4 \
  --rollout_batch_size 1024 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --generate_max_len 1024 \
  --zero_stage 2 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data OpenRLHF/prompt-collection-v0.1 \
  --input_key context_messages \
  --apply_chat_template \
  --max_samples 100000 \
  --normalize_reward \
  --adam_offload \
  --flash_attn \
  --gradient_checkpointing \
  --load_checkpoint \
  --use_wandb {wandb_token}

# æ”¯æŒè¿œç¨‹ reward model (HTTP)
# --remote_rm_url http://localhost:5000/get_reward
```

### ä½¿ç”¨ Ray å’Œ vLLM çš„ PPO/REINFORCE++

ä¸ºäº†æé«˜ RLHF è®­ç»ƒé€Ÿåº¦æˆ–æ”¯æŒ 70B æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Ray å’Œ vLLM åŠ é€Ÿçš„ PPO

```bash
# åœ¨å®¹å™¨ä¸­å¯åŠ¨ Ray çš„ä¸»èŠ‚ç‚¹
ray start --head --node-ip-address 0.0.0.0 --num-gpus 8

# å¦‚æœè¦åœ¨æ›´å¤šèŠ‚ç‚¹ä¸Šå¯åŠ¨ Rayï¼Œè¯·ä½¿ç”¨
ray start --address {MASTER-NODE-ADDRESS}:6379 --num-gpus 8

ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  --ref_num_nodes 1 \
  --ref_num_gpus_per_node 2 \
  --reward_num_nodes 1 \
  --reward_num_gpus_per_node 2 \
  --critic_num_nodes 1 \
  --critic_num_gpus_per_node 2 \
  --actor_num_nodes 1 \
  --actor_num_gpus_per_node 2 \
  --vllm_num_engines 2 \
  --vllm_tensor_parallel_size 2 \
  --colocate_critic_reward \
  --colocate_actor_ref \
  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
  --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture \
  --save_path /openrlhf/examples/checkpoint/llama3-8b-rlhf \
  --micro_train_batch_size 8 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 32 \
  --rollout_batch_size 1024 \
  --max_samples 100000 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --generate_max_len 1024 \
  --zero_stage 3 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data OpenRLHF/prompt-collection-v0.1 \
  --input_key context_messages \
  --apply_chat_template \
  --normalize_reward \
  --packing_samples \
  --adam_offload \
  --flash_attn \
  --gradient_checkpointing \
  --load_checkpoint \
  --use_wandb {wandb_token}

# æ”¯æŒ REINFORCE++  | RLOO  | REINFORCE++-baseline
# --advantage_estimator reinforce | rloo | reinforce_baseline

# æ”¯æŒè¿œç¨‹ reward model (HTTP)
# --remote_rm_url http://localhost:5000/get_reward

# æ”¯æŒ N å€é‡‡æ ·
# --n_samples_per_prompt 4
```

> [!NOTE]
> ä¸è®¾ç½® `--vllm_num_engines` åˆ™æ˜¯ä¸ä½¿ç”¨ vLLM engineã€‚
> æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ ``setup_commands`` è®© Ray è‡ªåŠ¨åˆå§‹åŒ–ç¯å¢ƒ, æ¯”å¦‚ `--runtime-env-json='{"setup_commands": ["pip install openrlhf[vllm]"]}'`

> [!NOTE]
> OpenRLHF ä¸­çš„ RLOO å’Œ REINFORCE++-baseline æ˜¯åŸºäº REINFORCE++ çš„ä¿®æ”¹ç‰ˆæœ¬:
> - REINFORCE++ åœ¨ REINFORCE çš„åŸºç¡€ä¸Šé›†æˆäº† PPO çš„å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼Œæ‰€ä»¥ä¸ä¾èµ–äº Value Network
> - REINFORCE++-baseline ä½¿ç”¨äº†ç›¸åŒ prompt å¤šæ¬¡é‡‡æ ·çš„ reward å‡å€¼ä½œä¸º baseline
> - OpenRLHF ä¸­çš„ RLOO åœ¨åŸç‰ˆçš„åŸºç¡€ä¸Šé‡‡ç”¨äº† **per token KL reward** å’Œ **PPO-clip loss** 


> [!NOTE]
> å¦‚æœæ‚¨ç”±äºæŸç§åŸå› ï¼Œåœ¨ deepspeed è®¾ç½®æ˜¾å¡è®¾å¤‡æ—¶é‡åˆ°ä¸ç´¢å¼•è¶…å‡ºèŒƒå›´ç›¸å…³çš„é”™è¯¯ï¼Œæ‚¨å¯ä»¥å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py)ã€‚
> ```bash
> # å¯¹äº NVIDIA æ˜¾å¡:
> export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1
> ```

æ‰€æœ‰æ”¯æŒç®—æ³•çš„å¯åŠ¨è„šæœ¬å’Œæ–‡æ¡£åœ¨ [example/scripts](./examples/scripts/) å’Œ [Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)


### ä½¿ç”¨ LoRA
å¦‚æœæ‚¨ä½¿ç”¨äº† `LoRA (Low-Rank Adaptation)`ï¼Œé»˜è®¤ä¿å­˜ä¸‹æ¥çš„æ–‡ä»¶**å¹¶é**å®Œæ•´æ¨¡å‹æƒé‡ï¼Œè€Œæ˜¯ `LoRA Adapter`ï¼Œè‹¥æƒ³æŒ‰å®Œæ•´æƒé‡çš„æ–¹å¼è¿›è¡Œåç»­ä»»åŠ¡ï¼Œæ‚¨éœ€è¦å°† `Adapter` ä¸è®­ç»ƒå‰çš„æ¨¡å‹æƒé‡è¿›è¡Œåˆå¹¶

```bash
python -m openrlhf.cli.lora_combiner \
    --model_path meta-llama/Meta-Llama-3-8B \
    --lora_path ./checkpoint/llama3-8b-rm \
    --output_path ./checkpoint/llama-3-8b-rm-combined \
    --is_rm \
    --bf16
```

## æ€§èƒ½
æˆ‘ä»¬é€šè¿‡å¯ç”¨Adamå¸è½½ã€å¥–åŠ±æ¨¡å‹(RM)å’Œå‚è€ƒæ¨¡å‹(Ref)å¸è½½ç­‰æŠ€æœ¯,å°½å¯èƒ½ä¼˜åŒ–äº†DSChatçš„æ€§èƒ½,ä»è€Œåœ¨æ¨ç†é˜¶æ®µå¢åŠ å°æ‰¹é‡å¤§å°å¹¶é¿å…å†…å­˜ä¸è¶³é—®é¢˜ã€‚æˆ‘ä»¬ç”šè‡³ä¿®å¤äº†DSChatä¸­çš„ä¸€äº›bug,ä»¥å¯ç”¨LLaMA2çš„æ··åˆå¼•æ“(HE)ã€‚ä½¿ç”¨ä¼˜åŒ–åçš„DSChatå’ŒOpenRLHFè®­ç»ƒ1024ä¸ªæç¤ºéœ€è¦1ä¸ªPPOè½®æ¬¡çš„å¹³å‡æ—¶é—´(ç§’)å¦‚ä¸‹:

| **Size** | **NVIDIA A800 GPUs** | **Optimized DSChat (with  Hybrid Engine)** | **OpenRLHF** | **Speedup** |
| :---: | :---: | :---: | :---: | :---: |
| 7B | 16 | 855.09 | 471.11 | 1.82x |
| 13B | 32 | 1528.93 | 608.93 | 2.5x |
| 34B | 32 | 3634.98 | 1526.4 | 2.4x |
| 70B | 32 | 10407.0 | 4488.53 | 2.3x |


> [!NOTE]
> æ•°æ®å·²ç»è¿‡æ—¶; è¯·å‚è€ƒåé¢çš„è°ƒä¼˜æŒ‡å—é‡æ–°æµ‹è¯•

## è°ƒä¼˜æŒ‡å—
ä¸ºäº†è·å¾—æœ€ä½³çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åˆ†é…èŠ‚ç‚¹ä¸º `vLLM:Actor:Critic = 1:1:1`ã€‚ä¾‹å¦‚ï¼Œå¯¹äº 70B æ¨¡å‹ä»¥åŠ 48 å¼  A100ï¼Œå»ºè®®åˆ†é… 16 å¼ ä»¥ä¸Š A100 ç»™ vLLM Engineï¼Œ16 å¼ ç»™ Actor æ¨¡å‹ï¼Œä»¥åŠæœ€å 16 å¼ ç»™ Critic æ¨¡å‹ï¼ŒåŒæ—¶å¼€å¯ `--colocate_critic_reward`, `--colocate_actor_ref` æˆ–è€… `--ref_reward_offload (å¯é€‰)` é€‰é¡¹åˆå¹¶éƒ¨åˆ†èŠ‚ç‚¹ã€‚æœ€åæ‚¨åº”è¯¥å°½å¯èƒ½å¢å¤§ `--rollout_micro_batch_size` ï¼Œä»¥åŠå‡å° vLLM çš„ TP åˆ‡åˆ†æ•°é‡ã€‚è®­ç»ƒé˜¶æ®µçš„ `micro_train_batch_size` ä¹Ÿæ˜¯è¶Šå¤§è¶Šå¥½ï¼Œè¯·åŒæ—¶ä½¿ç”¨ `--packing_samples` ã€‚å½“ GPU æ•°é‡è¶³å¤Ÿæ—¶è¯·å…³é—­ `--adam_offload` ä»¥åŠå¯ç”¨ `--overlap_comm`. å¯¹äºå¤šèŠ‚ç‚¹ RLHF, è¯·ä½¿ç”¨ `--vllm_sync_backend nccl` with vLLM 0.7.2+. å¯ç”¨ `enable_prefix_caching` å¯¹äº vLLM å½“ ``n_samples_per_prompts`` > 1. å½“æ¨¡å‹è§„æ¨¡å’Œä¸Šä¸‹æ–‡é•¿åº¦è¾ƒå°æ—¶ï¼Œä½¿ç”¨ Hybrid Engine `--colocate_all_models` å’Œ `--vllm_enable_sleep`ï¼Œè€Œä¸æ˜¯åˆ†å¸ƒå¼ RLHFã€‚å¯¹äºå°ºå¯¸å¤§çš„åŸºç¡€æ¨¡å‹, å¦‚æœå‡ºç° OOM, è¯·ä¸è¦ä½¿ç”¨ä»»ä½•`--colocate_xxxx`é€‰é¡¹ã€‚

## ä½¿ç”¨ OpenRLHF çš„å…¬å¸å’Œç»„ç»‡

- Google
- ByteDance
- Tencent
- Alibaba
- Baidu
- China Telecom
- Allen AI
- Vivo
- NexusFlow
- JÃ¼lich Supercomputing Centre (JSC)
- Berkeley Starling Team
- M-A-P
- ...


## åŠ å…¥æˆ‘ä»¬

**å¦‚ä½•åŠ å…¥ï¼Ÿ**

1. é€šè¿‡è”ç³»é‚®ç®± janhu9527@gmail.com æˆ–è€…åŠ å…¥ [GitHub Organization](https://github.com/OpenRLHF)ã€‚è¯·åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
   - æ‚¨çš„å§“å
   - æ‚¨çš„ GitHub ç”¨æˆ·å
   - æ‚¨æ„Ÿå…´è¶£çš„é¢†åŸŸ
   - æ‚¨åœ¨ NLP å’Œ/æˆ– AI ç›¸å…³çš„æŠ€èƒ½å’Œç»éªŒ
2. æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡å®˜æ–¹ GitHub [OpenRLHF â†—](https://github.com/OpenRLHF/OpenRLHF) é¡¹ç›®é¡µé¢åŠ å…¥æˆ‘ä»¬ã€‚åªéœ€åˆ›å»ºä¸€ä¸ªå…³äºæ‚¨æƒ³è¦è´¡çŒ®çš„å…´è¶£çš„ issueï¼Œæˆ‘ä»¬ä¼šä¸æ‚¨è”ç³»ã€‚

**æ‚¨èƒ½åšä»€ä¹ˆï¼Ÿ**

1. åŠ å…¥å›¢é˜Ÿï¼Œå‚ä¸ OpenRLHF é¡¹ç›®çš„å¼€å‘ã€‚
2. é€šè¿‡æäº¤ pull è¯·æ±‚æ¥ä¸ºé¡¹ç›®åšå‡ºè´¡çŒ®ã€‚
3. å¸®åŠ©æ”¹è¿›æ–‡æ¡£ï¼Œä¿®å¤ bugs æˆ–åˆ›å»ºæ–°åŠŸèƒ½ã€‚
4. åˆ†äº«é¡¹ç›®å¹¶å¸®åŠ©æˆ‘ä»¬å‘å±•ç¤¾åŒºã€‚

## èµåŠ©æˆ‘ä»¬

æ‚¨çš„èµåŠ©å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç»´æŠ¤å’Œæ”¹è¿› OpenRLHFã€‚å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰ç”¨ï¼Œè¯·è€ƒè™‘èµåŠ©æˆ‘ä»¬ã€‚æ‚¨å¯ä»¥åœ¨ [Open Collective â†—](https://opencollective.com/OpenRLHF) ä¸ŠèµåŠ©æˆ‘ä»¬ã€‚

## æ˜Ÿå›¾

[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)

## è´¡çŒ®è€…

éå¸¸æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ï¼å¦‚æœæ‚¨æƒ³è´¡çŒ®ï¼Œè¯·éšæ—¶åˆ›å»º pull è¯·æ±‚æˆ–åˆ›å»º issueã€‚

<a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF" />
</a>

## å¼•ç”¨ä¸è‡´è°¢

æˆ‘ä»¬æƒ³è¦å¯¹ä»¥ä¸‹é¡¹ç›®å’Œç»„ç»‡åœ¨ AI å’Œ NLP é¢†åŸŸçš„è´¡çŒ®è¡¨ç¤ºæ„Ÿè°¢ï¼š

- [Hugging Face Transformers â†—](https://github.com/huggingface/transformers)
- [OpenAI GPT â†—](https://github.com/openai/gpt-3)
- [LLaMA â†—](https://llama.meta.com/)
- [DeepSpeed â†—](https://github.com/microsoft/DeepSpeed)
- [Ray â†—](https://github.com/ray-project/ray)

æˆ‘ä»¬çš„é¡¹ç›®è¿˜æƒ³è¦æ„Ÿè°¢ [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) å’Œ [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)ã€‚åœ¨é¡¹ç›®çš„æ—©æœŸé˜¶æ®µï¼Œæˆ‘ä»¬å‚è€ƒäº†ä»–ä»¬çš„ä»£ç è®¾è®¡ã€‚

(2024/7) æˆ‘ä»¬çš„ GitHub ç»„ç»‡ä» OpenLLMAI è¿ç§»åˆ°äº† OpenRLHF.

## å¼•ç”¨
```
@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}
```


______________________________________________________________________

*OpenRLHF Â© 2025 OpenRLHF. ç‰ˆæƒæ‰€æœ‰ã€‚*
