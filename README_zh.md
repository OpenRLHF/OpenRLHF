<div align="center">
<p align="center">
<img alt="" src="./docs/logo.png" style="display: inline-block; height: 140px" />
</p>
</div>

<div align="center">
<p align="center">
      <a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/discussions">
        <img alt="Issues" src="https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff" />
      </a>
      <a href="https://github.com/OpenRLHF/OpenRLHF/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff" />
      <a href="https://github.com/OpenRLHF/OpenRLHF/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf" />
      </a>
      <br>
      <em>å¼€æº / å…¨é¢ / è½»é‡çº§ / æ˜“ç”¨</em>
    </p>
</p>
</div>

<hr>

<span>[ <a href="README.md">English</a> | ä¸­æ–‡ | <a href="README_ja.md">æ—¥æœ¬èª</a> ]</span>

OpenRLHF æ˜¯ç¬¬ä¸€ä¸ªåŸºäº Rayã€vLLMã€ZeRO-3 å’Œ HuggingFace Transformers æ„å»ºçš„å¼€æºé«˜æ€§èƒ½ RLHF æ¡†æ¶ï¼Œæ—¨åœ¨è®© RLHF è®­ç»ƒå˜å¾—ç®€å•æ˜“ç”¨ï¼š

- **åŸºäº Ray çš„åˆ†å¸ƒå¼æ¶æ„**  
  OpenRLHF åˆ©ç”¨ [Ray](https://github.com/ray-project/ray) å®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼è°ƒåº¦ã€‚å®ƒå°† Actorã€Rewardã€Reference å’Œ Critic æ¨¡å‹åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸Šï¼Œæ”¯æŒé«˜è¾¾ 70B å‚æ•°æ¨¡å‹çš„è®­ç»ƒã€‚  
  å®ƒè¿˜æ”¯æŒ **Hybrid Engine** è°ƒåº¦ï¼Œå…è®¸æ‰€æœ‰æ¨¡å‹å’Œ vLLM å¼•æ“å…±äº« GPU èµ„æºï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘ç©ºé—²æ—¶é—´å¹¶æé«˜ GPU åˆ©ç”¨ç‡ã€‚
- **vLLM æ¨ç†åŠ é€Ÿ + AutoTP**  
  RLHF è®­ç»ƒä¸­ 80% çš„æ—¶é—´éƒ½èŠ±åœ¨æ ·æœ¬ç”Ÿæˆé˜¶æ®µã€‚åŸºäº [vLLM](https://github.com/vllm-project/vllm) å’Œè‡ªåŠ¨å¼ é‡å¹¶è¡Œ (AutoTP)ï¼ŒOpenRLHF æä¾›é«˜ååé‡ã€å†…å­˜é«˜æ•ˆçš„æ¨ç†ã€‚ä¸ HuggingFace Transformers çš„åŸç”Ÿé›†æˆç¡®ä¿äº†æ— ç¼ä¸”å¿«é€Ÿçš„ç”Ÿæˆï¼Œä½¿å…¶æˆä¸ºç›®å‰æœ€å¿«çš„ RLHF æ¡†æ¶ã€‚
- **åŸºäº ZeRO-3 / AuoTP çš„å†…å­˜é«˜æ•ˆè®­ç»ƒ**  
  åŸºäº [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) çš„ ZeRO-3, [deepcompile](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md) ä»¥åŠ [AutoTP](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md)ï¼ŒOpenRLHF æ— éœ€é‡é‡çº§æ¡†æ¶å³å¯å®ç°å¤§æ¨¡å‹è®­ç»ƒã€‚å®ƒç›´æ¥ä¸ HuggingFace é…åˆä½¿ç”¨ï¼Œæ–¹ä¾¿åŠ è½½å’Œå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚
- **ä¼˜åŒ–çš„ PPO å®ç°**  
  é›†æˆäº†å—å®è·µæŒ‡å—å’Œç¤¾åŒºæœ€ä½³å®è·µå¯å‘çš„å…ˆè¿› PPO æŠ€å·§ï¼Œæé«˜äº† RLHF å·¥ä½œæµç¨‹ä¸­çš„è®­ç»ƒç¨³å®šæ€§å’Œå¥–åŠ±è´¨é‡ã€‚å‚è€ƒ [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/622134699) å’Œ [Advanced Tricks for Training Large Language Models with Proximal Policy Optimization](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361)ã€‚

æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [PPT](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2405.11143) | [ä½¿ç”¨æ–‡æ¡£](https://openrlhf.readthedocs.io/)


## æ–°é—»  
- [2025/4] å‘å¸ƒåšå®¢ [Accelerating RLHF with vLLM, Best Practice from OpenRLHF](https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html)
- [2025/4] Clean OpenRLHF: åŸºäº Single Controller å’Œ Unified Packing Samples é‡æ„äº†æºç 
- [2025/3] CMUçš„[2025æ˜¥å­£é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†è¯¾ç¨‹](https://cmu-l3.github.io/anlp-spring2025/)ä½¿ç”¨OpenRLHFä½œä¸ºRLHFæ¡†æ¶æ•™å­¦æ¡ˆä¾‹ã€‚
- [2025/2] [Logic-RL](https://arxiv.org/abs/2502.14768) å’Œ [PRIME](https://arxiv.org/abs/2502.01456) å±•ç¤ºäº† REINFORCE++ åœ¨è®­ç»ƒç¨³å®šæ€§ä¸Šä¼˜äº GRPO å¹¶ä¸”æ¯” PPO æ›´å¿«ã€‚
- [2025/2] [LMM-R1](https://github.com/TideDra/lmm-r1) æ˜¯ OpenRLHF çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨ä¸ºå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå¤ç° DeepSeek-R1 æä¾›é«˜æ€§èƒ½çš„ RL åŸºç¡€è®¾æ–½ã€‚
- [2025/2] MIT & Microsoft æå‡ºäº† [On the Emergence of Thinking in LLMs I: Searching for the Right Intuition](https://arxiv.org/pdf/2502.06773) åŸºäº OpenRLHF
- [2025/1] æ¸¯ç§‘å¤§å¤ç°äº† [DeepSeek-R1-Zero and DeepSeek-R1 training on small models ä½¿ç”¨ OpenRLHF](https://github.com/hkust-nlp/simpleRL-reason)
- [2024/12] æˆ‘ä»¬"æå‡º"äº† ğŸ˜Š [REINFORCE++ å¯¹é½ç®—æ³•](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS).
- [2024/12] åœ¨ [Notion Blog](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05) ä¸­ï¼Œæˆ‘ä»¬å¯¹ PPOã€REINFORCE++ã€GRPO å’Œ RLOO è¿›è¡Œäº†åˆ†æã€‚  
- [2023/8] OpenRLHF å¼€å¯å¼€æºä¹‹æ—…. 

## ç‰¹æ€§  

- åŸºäº Ray çš„åˆ†å¸ƒå¼ [PPO](./examples/scripts/train_ppo_llama_ray.sh) å’Œ [REINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray.sh) å®ç°ã€‚  
- æ”¯æŒå¯¹ [è¶…è¿‡ 700 äº¿å‚æ•°çš„æ¨¡å‹](./examples/scripts/train_ppo_llama_ray_70b.sh) è¿›è¡Œå®Œæ•´çš„ RLHF å¾®è°ƒã€‚  
- æ”¯æŒåŸºäº Ray å’Œ Hybrid Engine çš„ [PPO](./examples/scripts/train_ppo_llama_ray_hybrid_engine.sh) å’Œ [REINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh) (`--colocate_all_models`, `--vllm_enable_sleep` and `--vllm_gpu_memory_utilization 0.5`)
- [Ray-based Reinforced Finetuning](./examples/scripts/train_ppo_llama_with_reward_fn.sh)
- æ”¯æŒ [DeepSpeed AutoTP è®­ç»ƒ](./examples/scripts/train_sft_llama_tensor_parallelism.sh) (`--ds_tensor_parallel_size`)
- é›†æˆ vLLMï¼ŒåŠ é€Ÿ RLHF ä»»åŠ¡ä¸­çš„æ ·æœ¬ç”Ÿæˆï¼ˆ`--vllm_num_engines`ï¼‰ã€‚  
- æ”¯æŒå¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆ`--reward_pretrain model1,model2...`ï¼‰å’Œè¿œç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆ`--remote_rm_url`ï¼‰ã€‚  
- å®ç° [DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰/IPO/cDPO](./examples/scripts/train_dpo_llama.sh) å’Œ [Kahneman-Tversky Optimizationï¼ˆKTOï¼‰](./examples/scripts/train_kto_llama.sh)ã€‚  
- æ”¯æŒ [è¿­ä»£ DPO](./examples/scripts/train_iterative_dpo_llama.sh)ï¼ˆ[GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)ï¼‰ã€‚  
- æ”¯æŒ [æ‹’ç»é‡‡æ ·](./examples/scripts/train_rejection_sampling_llama.sh)ã€‚  
- å®ç° [æ¡ä»¶ SFT](./examples/scripts/train_conditional_llama.sh)ï¼ˆ[arXiv:2308.12050](https://arxiv.org/abs/2308.12050)ï¼‰ã€‚  
- æ”¯æŒ [çŸ¥è¯†è’¸é¦](./examples/scripts/train_knowledge_distillation.sh)ï¼ˆ[Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)ï¼‰ã€‚  
- é›†æˆ [è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰](./examples/scripts/train_prm_mistral.sh)ã€‚  
- æ”¯æŒ SFTã€DPOã€RMã€PRM å’Œ PPO çš„è®­ç»ƒæ ·æœ¬æ‰“åŒ…ï¼ˆ`--packing_samples`ï¼‰ã€‚  
- å®ç° [RingAttention](./examples/scripts/train_dpo_ring_llama.sh)ï¼ˆ`--ring_attn_size`ï¼Œ`--ring_head_stride`ï¼‰ã€‚  
- æ”¯æŒ [ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰](./examples/test_scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--aux_loss_coef`ï¼‰ã€‚  
- é›†æˆ FlashAttention2ï¼ˆ`--flash_attn`ï¼‰ã€‚  
- æ”¯æŒ QLoRAï¼ˆ`--load_in_4bit`ï¼‰å’Œ [LoRA](./examples/scripts/train_sft_mixtral_lora.sh)ï¼ˆ`--lora_rank`ï¼Œ`--target_modules`ï¼‰ã€‚  
- å…¼å®¹ HuggingFace çš„ `tokenizer.apply_chat_template` æ•°æ®é›†æ ¼å¼ï¼ˆ`--apply_chat_template` å’Œ `--input_key`ï¼‰ã€‚  
- æ”¯æŒä½¿ç”¨ Wandbï¼ˆ`--use_wandb`ï¼‰å’Œ TensorBoardï¼ˆ`--use_tensorboard`ï¼‰è¿›è¡Œæ—¥å¿—è®°å½•ã€‚  
- æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆ`--load_checkpoint` å’Œ `--save_steps`ï¼‰ã€‚  
- æä¾›äº†å¤šèŠ‚ç‚¹è®­ç»ƒè„šæœ¬, æ¯”å¦‚ [DPO](./examples/scripts/train_llama_slurm.sh) å’Œ [RLHF](./examples/scripts/train_ppo_llama_ray_slurm.sh)


## å¿«é€Ÿå¼€å§‹

### å®‰è£…

è¦ä½¿ç”¨ OpenRLHFï¼Œé¦–å…ˆå¯åŠ¨ Docker å®¹å™¨ï¼ˆ**æ¨è**ï¼‰ç„¶åæ‰§è¡Œ `pip install` å®‰è£… `openrlhf`ï¼š

```bash
# å¯åŠ¨ docker container
docker run --runtime=nvidia -it --rm --shm-size="10g" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:24.07-py3 bash
sudo pip uninstall xgboost transformer_engine flash_attn pynvml -y

# pip install
pip install openrlhf

# å¦‚æœä½ éœ€è¦ä½¿ç”¨ vLLM åŠ é€Ÿ (å®‰è£… vLLM 0.8.5.post1)
pip install openrlhf[vllm]
# æœ€æ–°çš„ vLLM ä¹Ÿæ˜¯æ”¯æŒçš„
pip install openrlhf[vllm_latest]

# pip install GitHub ä¸Šçš„æœ€æ–°ç‰ˆ
pip install git+https://github.com/OpenRLHF/OpenRLHF.git

# æˆ–è€… git clone
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
```

> [!NOTE]
>æˆ‘ä»¬æ¨èä½¿ç”¨ vLLM 0.8.5.post1 åŠä»¥ä¸Šç‰ˆæœ¬ã€‚
>æˆ‘ä»¬ä¹Ÿæä¾›äº† [Dockerfiles for vLLM](./dockerfile/) å’Œ [Nvidia-Docker ä¸€é”®å®‰è£…è„šæœ¬](./examples/scripts/nvidia_docker_install.sh)ã€‚

### å‡†å¤‡æ•°æ®é›†
OpenRLHF åœ¨å…¶æ•°æ®é›†ç±»ä¸­æä¾›äº†å¤šç§æ•°æ®å¤„ç†æ–¹æ³•ã€‚
ä¾‹å¦‚åœ¨ [Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6) ä¸­ï¼š

```python
def preprocess_data(data, input_template=None, input_key="input", apply_chat_template=None) -> str:
    if apply_chat_template:
        chat = data[input_key]
        if isinstance(chat, str):
            chat = [{"role": "user", "content": chat}]
        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    else:
        prompt = data[input_key]
        if input_template:
            prompt = input_template.format(prompt)
    return prompt
```

- æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `--input_key` æŒ‡å®š `JSON key name` ä¸ºè¾“å…¥æ•°æ®é›† `--prompt_data {name or path}` (PPO) æˆ– `--dataset {name or path}`ï¼Œå¹¶ä½¿ç”¨ `--apply_chat_template` åˆ©ç”¨ [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating) ä¸­çš„ `chat_template`ã€‚
- å¦‚æœä¸æƒ³ä½¿ç”¨ `--apply_chat_template`ï¼Œå¯ä»¥æ”¹ç”¨ `--input_template`ï¼Œæˆ–é¢„å…ˆç¦»çº¿å¤„ç†æ•°æ®é›†ã€‚
- OpenRLHF è¿˜æ”¯æŒä½¿ç”¨ `--prompt_data_probs 0.1,0.4,0.5` (PPO) æˆ– `--dataset_probs 0.1,0.4,0.5` æ··åˆå¤šä¸ªæ•°æ®é›†ã€‚

Chat Templating çš„å·¥ä½œåŸç†å¦‚ä¸‹:

```python
dataset = [{"input_key": [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]}]

tokenizer.apply_chat_template(dataset[0]["input_key"], tokenize=False)

"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

å¦‚ä½•æŒ‡å®šè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†åŒº ?

ä½ å¯ä»¥ä½¿ç”¨ `data_type@data_dir` çš„æ–¹å¼æŒ‡å®š, æ¯”å¦‚ä¸‹é¢çš„æ•°æ®é›†å¯ä»¥è®¾ç½®ä¸º `--dataset json@./data`

```
data
â”œâ”€â”€ test.jsonl
â””â”€â”€ train.jsonl
```

å¦‚ä½•æŒ‡å®šæµ‹è¯•æ•°æ®é›† ?

è¯·ä½¿ç”¨ ``--eval_dataset {name or path}`` æ¥è®¾ç½®æµ‹è¯•æ•°æ®é›†è·¯å¾„ã€‚

> [!NOTE]
> ``JSON key`` é€‰é¡¹å–å†³äºå…·ä½“çš„æ•°æ®é›†ã€‚è¯·å‚é˜… [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) å’Œ [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)


### Supervised Fine-tuning

OpenRLHF çš„æ¨¡å‹æ£€æŸ¥ç‚¹å®Œå…¨å…¼å®¹ HuggingFace æ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `--pretrain  {name or path}`ã€`--reward_pretrain  {name or path}` å’Œ `--critic_pretrain  {name or path}` æŒ‡å®šæ¨¡å‹åç§°æˆ–è·¯å¾„ã€‚æˆ‘ä»¬åœ¨ [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF) ä¸Šæä¾›äº†ä¸€äº›é¢„è®­ç»ƒçš„æ£€æŸ¥ç‚¹å’Œæ•°æ®é›†ã€‚

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨ [examples/scripts](./examples/scripts/) ç›®å½•ä¸­æä¾›çš„å¯åŠ¨è„šæœ¬ï¼Œæˆ–è€…ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š

```bash 
deepspeed --module openrlhf.cli.train_sft \
   --max_len 4096 \
   --dataset Open-Orca/OpenOrca \
   --input_key question \
   --output_key response \
   --input_template $'User: {}\nAssistant: ' \
   --train_batch_size 256 \
   --micro_train_batch_size 2 \
   --max_samples 500000 \
   --pretrain meta-llama/Meta-Llama-3-8B \
   --save_path ./checkpoint/llama3-8b-sft \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --zero_stage 2 \
   --max_epochs 1 \
   --bf16 \
   --flash_attn \
   --learning_rate 5e-6 \
   --gradient_checkpointing \
   --packing_samples \
   --load_checkpoint \
   --use_wandb {wandb_token}

# æ”¯æŒ HF tokenizer.apply_chat_template
# --apply_chat_template 
# --tokenizer_chat_template {HF Chat Template}

# æ”¯æŒ RingAttention
# pip install ring_flash_attn
#   --ring_attn_size 2 \
#   --ring_head_stride 2 \

# ä¹Ÿå¯ç”¨äº continued pre-training
# --pretrain_mode
```

> [!NOTE]
> OpenRLHF SFT/DPO/RewardModel/PPO è®­ç»ƒæ”¯æŒ `--packing_samples` [åŸºäº `--flash_attn`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)

### Reward Model Training
```bash
deepspeed --module openrlhf.cli.train_rm \
   --save_path ./checkpoint/llama3-8b-rm \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --train_batch_size 256 \
   --micro_train_batch_size 1 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --bf16 \
   --max_epochs 1 \
   --max_len 8192 \
   --zero_stage 3 \
   --learning_rate 9e-6 \
   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
   --apply_chat_template \
   --chosen_key chosen \
   --rejected_key rejected \
   --flash_attn \
   --packing_samples \
   --gradient_checkpointing \
   --load_checkpoint \
   --use_wandb {wandb_token}

```

æ¨èè®¾ç½® Reward Model çš„ `--value_prefix_head` é€‰é¡¹ä¸º `score`, è¿™æ ·ä½¿å¾—æˆ‘ä»¬å¯ä»¥ç”¨ `AutoModelForSequenceClassification` åŠ è½½æ¨¡å‹:

```python
reward_model = AutoModelForSequenceClassification.from_pretrained(
              reward_model_path,
              num_labels=1,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              use_cache=False,
          )
inputs = xxxx (Left Padding Input Tokens)
reward = reward_model.model(*inputs).last_hidden_state
reward = reward_model.score(reward)[:, -1]
```

### ä½¿ç”¨ Ray å’Œ vLLM çš„ PPO/REINFORCE++

ä¸ºäº†æé«˜ RLHF è®­ç»ƒé€Ÿåº¦æˆ–æ”¯æŒ 70B æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Ray å’Œ vLLM åŠ é€Ÿçš„ PPO (Hybrid Engine)

```bash
# åœ¨å®¹å™¨ä¸­å¯åŠ¨ Ray çš„ä¸»èŠ‚ç‚¹
ray start --head --node-ip-address 0.0.0.0 --num-gpus 8

# å¦‚æœè¦åœ¨æ›´å¤šèŠ‚ç‚¹ä¸Šå¯åŠ¨ Rayï¼Œè¯·ä½¿ç”¨
ray start --address {MASTER-NODE-ADDRESS}:6379 --num-gpus 8

ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  --vllm_num_engines 4 \
  --vllm_tensor_parallel_size 2 \
  --colocate_all_models \
  --vllm_gpu_memory_utilization 0.5 \
  --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
  --reward_pretrain OpenRLHF/Llama-3-8b-rm-700k \
  --save_path /openrlhf/examples/test_scripts/final/llama3-8b-rlhf \
  --ckpt_path /openrlhf/examples/test_scripts/ckpt/llama3-8b-rlhf \
  --save_hf_ckpt \
  --micro_train_batch_size 8 \
  --train_batch_size 128 \
  --micro_rollout_batch_size 16 \
  --rollout_batch_size 1024 \
  --n_samples_per_prompt 1 \
  --max_epochs 1 \
  --prompt_max_len 1024 \
  --max_samples 100000 \
  --generate_max_len 1024 \
  --zero_stage 3 \
  --bf16 \
  --actor_learning_rate 5e-7 \
  --critic_learning_rate 9e-6 \
  --init_kl_coef 0.01 \
  --prompt_data OpenRLHF/prompt-collection-v0.1 \
  --input_key context_messages \
  --apply_chat_template \
  --normalize_reward \
  --gradient_checkpointing \
  --packing_samples \
  --vllm_sync_backend nccl \
  --enforce_eager \
  --vllm_enable_sleep \
  --deepspeed_enable_sleep \
  --use_wandb {wandb_token}

# æ”¯æŒ REINFORCE++  | RLOO | REINFORCE++-baseline | GRPO | Dr. GRPO
# --advantage_estimator reinforce | rloo | reinforce_baseline | group_norm | dr_grpo

# è®¾ç½® --init_kl_coef ä¸º 0 å°†ä¸ä¼šå¯åŠ¨å‚è€ƒæ¨¡å‹

# æ”¯æŒè¿œç¨‹å¥–åŠ±æ¨¡å‹ (HTTP)
# --remote_rm_url http://localhost:5000/get_reward

# æ”¯æŒ N ä¸ªæ ·æœ¬
# --n_samples_per_prompt 4
```

> [!NOTE]
> ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ ``setup_commands`` è®© Ray è‡ªåŠ¨éƒ¨ç½²ç¯å¢ƒï¼Œä¾‹å¦‚ `--runtime-env-json='{"setup_commands": ["pip install openrlhf[vllm]"]}'`ã€‚

> [!NOTE]
> OpenRLHF ä¸­çš„ RLOO å’Œ REINFORCE++-baseline æ˜¯åŸºäº REINFORCE++ çš„ä¿®æ”¹ç‰ˆæœ¬ï¼š
> - REINFORCE++ é›†æˆäº† PPO çš„å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚ä¼˜åŠ¿å½’ä¸€åŒ–å’Œ PPO-clip lossï¼‰åˆ° REINFORCEï¼ŒåŒæ—¶æ¶ˆé™¤äº†å¯¹ Critic ç½‘ç»œçš„éœ€æ±‚ã€‚
> - REINFORCE++-baseline ä½¿ç”¨`æ¥è‡ªåŒä¸€ä¸ª prompt çš„å¤šä¸ªæ ·æœ¬çš„å¹³å‡å¥–åŠ±`ä½œä¸ºåŸºçº¿æ¥é‡å¡‘å¥–åŠ±ï¼ˆä½¿ç”¨å…¨å±€æ‰¹æ¬¡å½’ä¸€åŒ– `/std`ï¼‰ã€‚
> - OpenRLHF ä¸­çš„ RLOO é€šè¿‡å¼•å…¥`per token çš„ KL å¥–åŠ±`å¹¶ä½¿ç”¨ `PPO-clip loss` æ¥ä¿®æ”¹åŸå§‹ç‰ˆæœ¬ã€‚
> - Dr. GRPO ç§»é™¤äº† GRPO ä¸­çš„ç»„å½’ä¸€åŒ– `/std`ã€‚


> [!NOTE]
> å¦‚æœé‡åˆ° deepspeed è®¾ç½® GPU è®¾å¤‡æ—¶å‡ºç°ç´¢å¼•è¶Šç•Œé”™è¯¯ï¼Œå¯ä»¥å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py) ä½œä¸ºä¸´æ—¶è§£å†³æ–¹æ¡ˆã€‚
>   ```bash
>   # å¯¹äº NVIDIA GPUï¼š
>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1
>   ```

æ‰€æœ‰æ”¯æŒç®—æ³•çš„å¯åŠ¨è„šæœ¬å’Œæ–‡æ¡£åœ¨ [example/scripts](./examples/scripts/) å’Œ [Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)

### Reinforced Fine-tuning

OpenRLHF æ”¯æŒä¾¿æ·é«˜æ•ˆçš„ Reinforced Fine-tuningã€‚æ‚¨åªéœ€è¦å®ç°ä¸€ä¸ªåŒ…å«è‡ªå®šä¹‰ `reward_func` å‡½æ•°çš„[æ–‡ä»¶](./examples/scripts/reward_func.py)å¹¶å°†å…¶è·¯å¾„ä¼ é€’ç»™ `remote_rm_url` å‚æ•°å³å¯ã€‚ä¾‹å¦‚ï¼š

```python
# reward_func.py
import torch

def reward_func(queries, prompts, labels):
    # queries æ˜¯ prompts + responses
    # labels æ˜¯ answers
    print(queries)
    return torch.randn(len(queries))
```

ç„¶ååªéœ€è®¾ç½®ï¼š

```shell 
ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  ...
  --remote_rm_url /path/to/reward_func.py \
  --label_key answer
```

å…¶ä¸­ `label_key` å‚æ•°ç”¨äºå‘å¥–åŠ±å‡½æ•°ä¼ é€’é¢å¤–çš„æ ·æœ¬ä¿¡æ¯æ¯”å¦‚ç­”æ¡ˆã€‚


### ä½¿ç”¨ LoRA
å¦‚æœæ‚¨ä½¿ç”¨äº† `LoRA (Low-Rank Adaptation)`ï¼Œé»˜è®¤ä¿å­˜ä¸‹æ¥çš„æ–‡ä»¶**å¹¶é**å®Œæ•´æ¨¡å‹æƒé‡ï¼Œè€Œæ˜¯ `LoRA Adapter`ï¼Œè‹¥æƒ³æŒ‰å®Œæ•´æƒé‡çš„æ–¹å¼è¿›è¡Œåç»­ä»»åŠ¡ï¼Œæ‚¨éœ€è¦å°† `Adapter` ä¸è®­ç»ƒå‰çš„æ¨¡å‹æƒé‡è¿›è¡Œåˆå¹¶

```bash
python -m openrlhf.cli.lora_combiner \
    --model_path meta-llama/Meta-Llama-3-8B \
    --lora_path ./checkpoint/llama3-8b-rm \
    --output_path ./checkpoint/llama-3-8b-rm-combined \
    --is_rm \
    --bf16
```

## æ€§èƒ½

æˆ‘ä»¬é€šè¿‡å¯ç”¨ Adam offloadã€å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å’Œå‚è€ƒæ¨¡å‹ï¼ˆRefï¼‰offload ç­‰æŠ€æœ¯ï¼Œæœ€å¤§é™åº¦åœ°ä¼˜åŒ–äº† DSChat çš„æ€§èƒ½ï¼Œä»¥å¢åŠ æ¨ç†é˜¶æ®µçš„å¾®æ‰¹æ¬¡å¤§å°å¹¶é¿å…å†…å­˜ä¸è¶³é—®é¢˜ã€‚æˆ‘ä»¬ç”šè‡³ä¿®å¤äº† DSChat ä¸­çš„ä¸€äº›é”™è¯¯ï¼Œä¸º LLaMA2 å¯ç”¨äº† Hybrid Engineï¼ˆHEï¼‰ã€‚ä½¿ç”¨ä¼˜åŒ–åçš„ DSChat å’Œ OpenRLHF è®­ç»ƒ 1024 ä¸ªæç¤ºçš„ 1 ä¸ª PPO epoch æ‰€éœ€çš„å¹³å‡æ—¶é—´ï¼ˆç§’ï¼‰ï¼š

| **å¤§å°** | **NVIDIA A800-80GB GPUs** | **ä¼˜åŒ–åçš„ DSChat (å¸¦ Hybrid Engine)** | **OpenRLHF** | **åŠ é€Ÿæ¯”** |
| :---: | :---: | :---: | :---: | :---: |
| 7B | 16 | 855.09 | 471.11 | 1.82x |
| 13B | 32 | 1528.93 | 608.93 | 2.5x |
| 34B | 32 | 3634.98 | 1526.4 | 2.4x |
| 70B | 32 | 10407.0 | 4488.53 | 2.3x |

> [!NOTE]
> è¿™äº›æ•°æ®å·²ç»è¿‡æ—¶ï¼Œè¯·å‚è€ƒæ€§èƒ½è°ƒä¼˜éƒ¨åˆ†è¿›è¡Œé‡æ–°æµ‹è¯•ã€‚

### æ€§èƒ½è°ƒä¼˜æŒ‡å—

ä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®æŒ‰ `vLLM:Actor:Critic = 1:1:1` çš„æ¯”ä¾‹åˆ†é…èŠ‚ç‚¹ã€‚

- ä¾‹å¦‚ï¼Œå¯¹äº 70B æ¨¡å‹å’Œ 48 ä¸ª A100 GPUï¼Œå»ºè®®å°† 16 ä¸ª A100 GPU åˆ†é…ç»™ vLLM å¼•æ“ï¼Œ16 ä¸ª GPU åˆ†é…ç»™ Actor æ¨¡å‹ï¼Œå‰©ä½™çš„ 16 ä¸ª GPU åˆ†é…ç»™ Critic æ¨¡å‹ã€‚
- å½“æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜æ—¶ï¼Œä½¿ç”¨ hybrid engine `--colocate_all_models` å’Œ `--vllm_enable_sleep` ä»¥åŠ `--deepspeed_enable_sleep`ï¼Œè€Œä¸æ˜¯åˆ†å¸ƒå¼ RLHFã€‚
- å¯ç”¨ `--colocate_critic_reward`ã€`--colocate_actor_ref` é€‰é¡¹æ¥åˆå¹¶èŠ‚ç‚¹ã€‚
- åº”è¯¥å°½å¯èƒ½å¢åŠ  `rollout_micro_batch_size`ï¼ˆå¹¶æœ€å°åŒ– vLLM å¼•æ“çš„ TP å¤§å°ï¼‰ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œè¾ƒå¤§çš„ `--micro_train_batch_size` æ›´å¥½ï¼Œå¹¶å¯ç”¨ `--packing_samples`ã€‚
- å½“æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜æ—¶ï¼Œè¯·ç¦ç”¨ `--adam_offload` å¹¶å¯ç”¨ `--overlap_comm`ã€‚åŒæ—¶å¯ç”¨ `--deepcompile` æ¥åŠ é€Ÿè®­ç»ƒã€‚
- å¯¹äº vLLMï¼Œè¯·ä½¿ç”¨ `--vllm_sync_backend nccl`
- å½“ `n_samples_per_prompts` > 1 æ—¶ï¼Œåœ¨ vLLM ç”Ÿæˆä¸­å¯ç”¨ [enable_prefix_caching](https://docs.vllm.ai/en/stable/automatic_prefix_caching/apc.html)ã€‚
- å¯¹äºå¤§å‹åŸºç¡€æ¨¡å‹ï¼Œå¦‚æœå‘ç”Ÿ OOMï¼Œä¸è¦ä½¿ç”¨ä»»ä½• `--colocate_xxxx` é€‰é¡¹ã€‚


## ä½¿ç”¨ OpenRLHF çš„å…¬å¸å’Œç»„ç»‡

- Google
- ByteDance
- Tencent
- Alibaba
- Baidu
- China Telecom
- Allen AI
- Vivo
- NexusFlow
- JÃ¼lich Supercomputing Centre (JSC)
- Berkeley Starling Team
- M-A-P
- ...


## åŠ å…¥æˆ‘ä»¬

**å¦‚ä½•åŠ å…¥ï¼Ÿ**

1. é€šè¿‡è”ç³»é‚®ç®± janhu9527@gmail.com æˆ–è€…åŠ å…¥ [GitHub Organization](https://github.com/OpenRLHF)ã€‚è¯·åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
   - æ‚¨çš„å§“å
   - æ‚¨çš„ GitHub ç”¨æˆ·å
   - æ‚¨æ„Ÿå…´è¶£çš„é¢†åŸŸ
   - æ‚¨åœ¨ NLP å’Œ/æˆ– AI ç›¸å…³çš„æŠ€èƒ½å’Œç»éªŒ
2. æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡å®˜æ–¹ GitHub [OpenRLHF â†—](https://github.com/OpenRLHF/OpenRLHF) é¡¹ç›®é¡µé¢åŠ å…¥æˆ‘ä»¬ã€‚åªéœ€åˆ›å»ºä¸€ä¸ªå…³äºæ‚¨æƒ³è¦è´¡çŒ®çš„å…´è¶£çš„ issueï¼Œæˆ‘ä»¬ä¼šä¸æ‚¨è”ç³»ã€‚

**æ‚¨èƒ½åšä»€ä¹ˆï¼Ÿ**

1. åŠ å…¥å›¢é˜Ÿï¼Œå‚ä¸ OpenRLHF é¡¹ç›®çš„å¼€å‘ã€‚
2. é€šè¿‡æäº¤ pull è¯·æ±‚æ¥ä¸ºé¡¹ç›®åšå‡ºè´¡çŒ®ã€‚
3. å¸®åŠ©æ”¹è¿›æ–‡æ¡£ï¼Œä¿®å¤ bugs æˆ–åˆ›å»ºæ–°åŠŸèƒ½ã€‚
4. åˆ†äº«é¡¹ç›®å¹¶å¸®åŠ©æˆ‘ä»¬å‘å±•ç¤¾åŒºã€‚

## èµåŠ©æˆ‘ä»¬

æ‚¨çš„èµåŠ©å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç»´æŠ¤å’Œæ”¹è¿› OpenRLHFã€‚å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰ç”¨ï¼Œè¯·è€ƒè™‘èµåŠ©æˆ‘ä»¬ã€‚æ‚¨å¯ä»¥åœ¨ [Open Collective â†—](https://opencollective.com/OpenRLHF) ä¸ŠèµåŠ©æˆ‘ä»¬ã€‚

## æ˜Ÿå›¾

[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)

## è´¡çŒ®è€…

éå¸¸æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ï¼å¦‚æœæ‚¨æƒ³è´¡çŒ®ï¼Œè¯·éšæ—¶åˆ›å»º pull è¯·æ±‚æˆ–åˆ›å»º issueã€‚

<a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF" />
</a>

## å¼•ç”¨ä¸è‡´è°¢

æˆ‘ä»¬æƒ³è¦å¯¹ä»¥ä¸‹é¡¹ç›®å’Œç»„ç»‡åœ¨ AI å’Œ NLP é¢†åŸŸçš„è´¡çŒ®è¡¨ç¤ºæ„Ÿè°¢ï¼š

- [Hugging Face Transformers â†—](https://github.com/huggingface/transformers)
- [OpenAI GPT â†—](https://github.com/openai/gpt-3)
- [LLaMA â†—](https://llama.meta.com/)
- [DeepSpeed â†—](https://github.com/microsoft/DeepSpeed)
- [Ray â†—](https://github.com/ray-project/ray)

æˆ‘ä»¬çš„é¡¹ç›®è¿˜æƒ³è¦æ„Ÿè°¢ [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalChat) å’Œ [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)ã€‚åœ¨é¡¹ç›®çš„æ—©æœŸé˜¶æ®µï¼Œæˆ‘ä»¬å‚è€ƒäº†ä»–ä»¬çš„ä»£ç è®¾è®¡ã€‚
æˆ‘ä»¬çš„é¡¹ç›®è¿˜æƒ³è¦æ„Ÿè°¢ [Netmind.AI](https://www.netmind.ai/) å¯¹äºring attentionå¼€å‘çš„GPUæ”¯æŒã€‚

(2024/7) æˆ‘ä»¬çš„ GitHub ç»„ç»‡ä» OpenLLMAI è¿ç§»åˆ°äº† OpenRLHF.

## å¼•ç”¨
```
@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}
```


______________________________________________________________________

*OpenRLHF Â© 2025 OpenRLHF. ç‰ˆæƒæ‰€æœ‰ã€‚*
